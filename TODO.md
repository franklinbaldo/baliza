# TODO: Arquitetura Ibis+Kedro - EliminaÃ§Ã£o da Era dbt

**Nova Arquitetura**: Ibis Pipeline + Kedro Orchestration  
**Data**: Julho 2025  
**Status**: ImplementaÃ§Ã£o PÃ³s-Abandono do dbt

## ðŸŽ¯ Objetivos da Nova Arquitetura

### âœ… **ATIVO - Ibis Pipeline**
- TransformaÃ§Ãµes Python nativas (sem SQL/Jinja)
- Type safety com Pydantic
- Domain enrichment automÃ¡tico
- Testes E2E com dados reais

### ðŸš§ **PENDENTE - Kedro Integration** 
- OrquestraÃ§Ã£o de produÃ§Ã£o robusta
- Pipeline modular e testÃ¡vel
- **Bloqueio**: CLI incompatibility (Typer vs Click)

### âŒ **REMOVIDO - dbt**
- Complexidade excessiva eliminada
- SQL/Python duplication removida
- Dependencies e infraestrutura limpa

---

## Fase 1: RefatoraÃ§Ã£o da PersistÃªncia de Dados (CRÃTICO)

### ðŸ”¥ **Eliminar Raw Response Storage**
**Problema**: Armazenar JSON completo das respostas da API causa:
- 90% storage desperdiÃ§ado
- 10x slower parsing
- DuplicaÃ§Ã£o de dados desnecessÃ¡ria

### **F1.1** Nova Arquitetura Direct-to-Structured
- [ ] **Mapear API Endpoints Completamente**
  - [ ] `/contratos` â†’ schema especÃ­fico
  - [ ] `/atas` â†’ schema especÃ­fico  
  - [ ] `/contratacoes` â†’ schema especÃ­fico
  - [ ] `/fontes_orcamentarias` â†’ schema especÃ­fico
  - [ ] `/instrumentos_cobranca` â†’ schema especÃ­fico
  - [ ] `/planos_contratacao` + itens â†’ schemas especÃ­ficos

- [ ] **Implementar Parsing Direto**
  ```python
  # ANTES: API â†’ pncp_content (JSON) â†’ Parsing â†’ Structured Tables
  # DEPOIS: API â†’ Pydantic Validation â†’ Direct Insert to Typed Tables
  ```

- [ ] **Sistema de Error Handling**
  - [ ] Tabela `pncp_parse_errors` apenas para falhas
  - [ ] Campos: `url`, `error_message`, `extracted_at`, `retry_count`
  - [ ] Retry automÃ¡tico para erros transitÃ³rios
  - [ ] Preservar raw response APENAS se parsing falhar

### **F1.2** ValidaÃ§Ã£o Pydantic Robusta
- [ ] **Validators Customizados**:
  - [ ] CNPJ: 14 dÃ­gitos + dÃ­gitos verificadores vÃ¡lidos
  - [ ] CPF: 11 dÃ­gitos + dÃ­gitos verificadores vÃ¡lidos
  - [ ] Datas: nÃ£o futuras, formato consistente
  - [ ] Valores: nÃ£o negativos para preÃ§os/estimativas
  - [ ] UF: apenas cÃ³digos oficiais (27 estados)
  - [ ] CÃ³digos ENUM: validaÃ§Ã£o contra tabelas de domÃ­nio

- [ ] **TransformaÃ§Ã£o AutomÃ¡tica**:
  - [ ] Strings: trim whitespace, normalizaÃ§Ã£o de case
  - [ ] CoerÃ§Ã£o segura: `""` â†’ `None`, `"0"` â†’ `0`
  - [ ] Parsing de datas: mÃºltiplos formatos aceitos
  - [ ] Limpeza de CNPJ/CPF: remover pontuaÃ§Ã£o

### **F1.3** BenefÃ­cios Esperados
- **-90% storage usage**: Sem raw JSON duplicado
- **+5x parsing speed**: Direct structured parsing
- **+10x query performance**: Dados jÃ¡ normalizados
- **100% data quality**: Pydantic validation
- **Type safety**: Garantia de tipos corretos

---

## Fase 2: Kedro Pipeline Structure

### ðŸš§ **F2.1** Resolver CLI Incompatibility
**Problema**: Kedro espera Click CLI, BALIZA usa Typer
**SoluÃ§Ãµes PossÃ­veis**:
- [ ] Adapter pattern: Typer wrapper para Kedro commands
- [ ] Dual CLI: `baliza run` (Typer) + `kedro run` (separado)
- [ ] Refactoring: MigraÃ§Ã£o completa para Click
- [ ] HÃ­brido: Core CLI Typer + Pipeline CLI Kedro

### **F2.2** Estrutura Kedro + Ibis
```
src/baliza/pipelines/
â”œâ”€â”€ data_extraction/          # API â†’ Bronze tables
â”‚   â”œâ”€â”€ nodes.py              # Extraction functions
â”‚   â”œâ”€â”€ pipeline.py           # Extraction pipeline
â”‚   â””â”€â”€ __init__.py
â”œâ”€â”€ data_processing/          # Bronze â†’ Silver (Ibis)
â”‚   â”œâ”€â”€ nodes.py              # Cleaning, enrichment
â”‚   â”œâ”€â”€ pipeline.py           # Processing pipeline
â”‚   â””â”€â”€ __init__.py
â”œâ”€â”€ analytics/                # Silver â†’ Gold (Ibis)
â”‚   â”œâ”€â”€ nodes.py              # Aggregations, metrics
â”‚   â”œâ”€â”€ pipeline.py           # Analytics pipeline
â”‚   â””â”€â”€ __init__.py
â””â”€â”€ domain_enrichment/        # Domain table integration
    â”œâ”€â”€ nodes.py              # ENUM enrichment
    â”œâ”€â”€ pipeline.py           # Domain pipeline
    â””â”€â”€ __init__.py
```

### **F2.3** Kedro + Ibis Integration
- [ ] **Kedro DataCatalog** com Ibis Tables
  ```yaml
  # conf/base/catalog.yml
  bronze_contratos:
    type: ibis.TableDataset
    table: bronze_contratos
    connection: duckdb://data/baliza.duckdb
  ```

- [ ] **Pipeline Nodes** retornam Ibis expressions
  ```python
  def process_contracts(bronze_contracts: ibis.Table) -> ibis.Table:
      return (
          bronze_contracts
          .filter(_.modalidadeId.notnull())
          .mutate(valor_normalizado=_.valorTotalEstimado.cast("decimal(15,4)"))
      )
  ```

- [ ] **Lazy Evaluation**: Ibis expressions atÃ© materializaÃ§Ã£o final

---

## Fase 3: Storage Tiers & Performance

### **F3.1** Hot-Cold Storage Architecture
- [ ] **Hot Tier** (Ãºltimos 90 dias):
  - [ ] Otimizado para writes rÃ¡pidos
  - [ ] Compression leve (ZSTD level 1)
  - [ ] Ãndices para queries frequentes
  
- [ ] **Cold Tier** (90 dias - 2 anos):
  - [ ] Compression agressiva (ZSTD level 9)
  - [ ] Read-only com batch updates
  - [ ] Row-group optimization para Parquet export
  
- [ ] **Archive Tier** (>2 anos):
  - [ ] Export para Parquet + Internet Archive
  - [ ] `read_parquet('https://archive.org/download/baliza-pncp/**/*.parquet')`
  - [ ] Views remotas via httpfs

### **F3.2** Lifecycle Management
- [ ] **Automated Tier Migration**:
  ```python
  def migrate_to_cold_tier():
      # Mover dados de 90+ dias para cold tier
      # Aplicar compression agressiva
      # Update views transparentes
  ```

### **F3.3** Performance Monitoring
- [ ] **Pipeline Metrics**:
  - [ ] Parsing success rate por endpoint
  - [ ] Records processed per second
  - [ ] Storage efficiency por tier
  - [ ] Query performance benchmarks

---

## Fase 4: MigraÃ§Ãµes & Deployment

### **F4.1** MigraÃ§Ã£o do Estado Atual
- [ ] **Backup dos Dados Existentes**
- [ ] **Schema Migration Script**:
  - [ ] `pncp_content` â†’ structured tables
  - [ ] Aplicar Pydantic validation retroativa
  - [ ] Migrar metadata de requests
  
- [ ] **Validation Suite**:
  - [ ] Row count consistency
  - [ ] Data integrity checks
  - [ ] Performance regression tests

### **F4.2** CI/CD Integration
- [ ] **Automated Testing**:
  - [ ] Kedro pipeline tests
  - [ ] Ibis expression validation
  - [ ] E2E tests com dados reais
  - [ ] Performance benchmarks

- [ ] **Deployment Strategy**:
  - [ ] Blue-green deployment
  - [ ] Rollback capability
  - [ ] Zero-downtime migration

---

## CritÃ©rios de Sucesso

### **Performance Targets**
- [ ] âœ… **-90% storage usage** vs schema atual
- [ ] âœ… **+5x ingestion speed** (direct parsing)
- [ ] âœ… **+10x query performance** (structured data)
- [ ] âœ… **<30s** para extraÃ§Ã£o mensal completa

### **Quality Gates**
- [ ] âœ… **100% Pydantic validation** success
- [ ] âœ… **Zero data loss** durante migration
- [ ] âœ… **E2E test coverage** para todos os pipelines
- [ ] âœ… **Type safety** garantida

### **Operational Goals**
- [ ] âœ… **Kedro orchestration** funcionando
- [ ] âœ… **Automated tier management** ativo
- [ ] âœ… **Monitoring dashboards** implementados
- [ ] âœ… **CI/CD automation** completo

---

## Referencias Atualizadas

- [ADR-014: Ibis Pipeline Adoption](docs/adr/014-ibis-pipeline-adoption.md) - **ATIVO**
- [dbt-to-kedro-migration-plan.md](docs/dbt-to-kedro-migration-plan.md) - **ROADMAP**
- [ADR-001: DuckDB Adoption](docs/adr/001-adopt-duckdb.md) - **MANTIDO**
- [ADR-006: Internet Archive](docs/adr/006-internet-archive.md) - **MANTIDO**
- [ADR-013: dbt Integration](docs/adr/013-dbt-integration-for-ddl.md) - **DEPRECIADO**

---

**Status**: ðŸš€ **Ready to Start** - Arquitetura definida, dbt removido  
**PrÃ³ximo**: Implementar Fase 1 (Eliminar Raw Response Storage)  
**Bloqueio**: Kedro CLI integration (Fase 2) precisa de soluÃ§Ã£o de compatibilidade