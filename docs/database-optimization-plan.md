# Plano de Otimiza√ß√£o e Refatora√ß√£o da Base de Dados BALIZA

## Contexto e Objetivos

Este plano visa otimizar radicalmente a arquitetura de dados BALIZA atrav√©s de:

1. **Extra√ß√£o de SQL inline** para arquivos `.sql` organizados
2. **Quebra de compatibilidade controlada** para implementar melhores pr√°ticas
3. **DDL robusta** com compress√£o inteligente e tipos otimizados
4. **Arquivamento em tiers** (quente/frio) para efici√™ncia de armazenamento

## An√°lise da Documenta√ß√£o Oficial PNCP

### Fontes de Verdade para Schema

**Documentos Analisados**:
- `docs/openapi/MANUAL-PNCP-CONSULSTAS-VERSAO-1.md` - Manual oficial com tabelas de dom√≠nio
- `docs/openapi/api-pncp-consulta.json` - Schema OpenAPI oficial com tipos precisos

**Insights Cr√≠ticos Descobertos**:

1. **Tabelas de Dom√≠nio Oficiais** - O manual define 17 tabelas de dom√≠nio com c√≥digos precisos:
   - Modalidade de Contrata√ß√£o: 13 valores (Leil√£o Eletr√¥nico=1, Preg√£o Eletr√¥nico=6, etc)
   - Situa√ß√£o da Contrata√ß√£o: 4 valores (Divulgada=1, Revogada=2, Anulada=3, Suspensa=4)
   - UF: 27 valores oficiais (AC, AL, AP, ..., TO)
   - Natureza Jur√≠dica: 47 c√≥digos espec√≠ficos (1015=Executivo Federal, etc)

2. **Campos Obrigat√≥rios vs Opcionais** - OpenAPI define precisamente:
   - **Obrigat√≥rios**: dataInicial, dataFinal, codigoModalidadeContratacao, pagina
   - **Opcionais**: uf, codigoMunicipioIbge, cnpj, codigoUnidadeAdministrativa, idUsuario

3. **Tipos de Dados Precisos**:
   - Decimais com **at√© 4 casas**: `DECIMAL(15,4)` para valores monet√°rios
   - Textos com limites: objetoCompra `VARCHAR(5120)`, numeroCompra `VARCHAR(50)`
   - C√≥digos CNPJ sempre 14 d√≠gitos, CPF 11 d√≠gitos
   - Datas em formato espec√≠fico: AAAAMMDD para consultas

4. **Estrutura Hier√°rquica Oficial**:
   ```
   √ìrg√£o/Entidade (CNPJ + razaoSocial + poderId + esferaId)
   ‚îî‚îÄ‚îÄ Unidade Administrativa (codigoUnidade + nomeUnidade + municipio + UF)
       ‚îî‚îÄ‚îÄ Contrata√ß√£o (numeroControlePNCP + modalidade + situa√ß√£o)
           ‚îî‚îÄ‚îÄ Contratos/Atas (numeroControlePNCPContrato + valores + datas)
   ```

## Princ√≠pios de Compress√£o: Heur√≠stica Autom√°tica do DuckDB

> **üéØ IMPORTANTE**: DuckDB possui heur√≠stica autom√°tica de compress√£o que escolhe o algoritmo mais eficiente por coluna. For√ßar `USING COMPRESSION` s√≥ √© necess√°rio em casos espec√≠ficos onde a heur√≠stica comprovadamente falha.

**Estrat√©gia Recomendada**:
- ‚úÖ **ENUM j√° √© dicion√°rio** - n√£o usar `USING COMPRESSION dictionary` adicional  
- ‚úÖ **Configura√ß√£o global**: `SET default_compression='zstd'` ao inv√©s de por tabela
- ‚úÖ **Evitar --strict** em hot writes, aplicar ZSTD no CHECKPOINT noturno
- ‚úÖ **Row-group tuning** apenas para Parquet export (tier frio)
- ‚ùå **N√£o for√ßar** FSST, bitpacking, delta - deixar heur√≠stica decidir

## Fase 1: Diagn√≥stico e Mapeamento Orientado pela Documenta√ß√£o Oficial (1-2 dias)

### 1.1 Auditoria da Base Atual vs Schema Oficial

**Objetivo**: Comparar implementa√ß√£o atual com documenta√ß√£o oficial PNCP

**A√ß√µes**:
```bash
# Executar diagn√≥stico orientado pela documenta√ß√£o oficial
baliza database-audit --compare-with-official-schema --output docs/database-audit-report.md
```

**M√©tricas a Coletar**:
- `pragma_storage_info()` para cada tabela
- **Conformidade com tabelas de dom√≠nio oficiais**: Verificar se usamos os c√≥digos corretos
- **Valida√ß√£o de tipos**: Comparar tipos atuais vs OpenAPI schema
- **Campos faltantes**: Identificar campos oficiais n√£o capturados
- **Oportunidades de ENUMs**: Mapear campos categ√≥ricos para ENUMs oficiais

### 1.2 Invent√°rio de SQL Inline

**Arquivo**: `docs/sql-inventory.md`

**Tarefa**: Escanear todo c√≥digo Python e mapear:
- Localiza√ß√£o de cada SQL inline
- Complexidade (simples SELECT vs multi-table JOIN)
- Depend√™ncias entre queries
- Candidatas para stored procedures/views

```python
# Exemplo de mapeamento
SQL_INVENTORY = {
    "src/baliza/pncp_writer.py:123": {
        "type": "INSERT",
        "complexity": "simple", 
        "target_file": "sql/inserts/pncp_content.sql"
    },
    "src/baliza/dashboard.py:273": {
        "type": "SELECT",
        "complexity": "analytics",
        "target_file": "sql/analytics/deduplication_stats.sql"
    }
}
```

## Fase 2: Extra√ß√£o e Reorganiza√ß√£o SQL (2-3 dias)

### 2.1 Estrutura de Diret√≥rios SQL

```
sql/
‚îú‚îÄ‚îÄ ddl/
‚îÇ   ‚îú‚îÄ‚îÄ 001_drop_legacy_tables.sql
‚îÇ   ‚îú‚îÄ‚îÄ 002_create_schemas.sql
‚îÇ   ‚îú‚îÄ‚îÄ 003_create_core_tables.sql
‚îÇ   ‚îú‚îÄ‚îÄ 004_create_indexes.sql
‚îÇ   ‚îî‚îÄ‚îÄ 005_create_views.sql
‚îú‚îÄ‚îÄ dml/
‚îÇ   ‚îú‚îÄ‚îÄ inserts/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ pncp_content.sql
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ pncp_requests.sql
‚îÇ   ‚îú‚îÄ‚îÄ updates/
‚îÇ   ‚îî‚îÄ‚îÄ deletes/
‚îú‚îÄ‚îÄ analytics/
‚îÇ   ‚îú‚îÄ‚îÄ deduplication_stats.sql
‚îÇ   ‚îú‚îÄ‚îÄ endpoint_performance.sql
‚îÇ   ‚îî‚îÄ‚îÄ storage_efficiency.sql
‚îú‚îÄ‚îÄ maintenance/
‚îÇ   ‚îú‚îÄ‚îÄ cleanup_old_data.sql
‚îÇ   ‚îú‚îÄ‚îÄ optimize_compression.sql
‚îÇ   ‚îî‚îÄ‚îÄ export_to_cold_storage.sql
‚îî‚îÄ‚îÄ migrations/
    ‚îú‚îÄ‚îÄ v1_to_v2_breaking_changes.sql
    ‚îî‚îÄ‚îÄ README.md
```

### 2.2 Template SQL Padr√£o

Cada arquivo SQL seguir√° este template:

```sql
-- File: sql/analytics/deduplication_stats.sql
-- Purpose: Calculate storage efficiency from content deduplication
-- Author: BALIZA Database Refactor
-- Created: 2025-01-XX
-- Dependencies: psa.pncp_content table
-- Performance: ~100ms on 1M records

-- Parameters (to be replaced by calling code):
-- ${SCHEMA_NAME} - Target schema (default: psa)
-- ${DATE_FILTER} - Optional date range filter

WITH dedup_analysis AS (
    SELECT
        COUNT(*) as unique_content,
        SUM(reference_count) as total_references,
        COUNT(CASE WHEN reference_count > 1 THEN 1 END) as deduplicated,
        SUM(content_size_bytes) as actual_size,
        SUM(content_size_bytes * reference_count) as theoretical_size
    FROM ${SCHEMA_NAME}.pncp_content
    WHERE 1=1
    ${DATE_FILTER}
)
SELECT 
    unique_content,
    total_references,
    deduplicated,
    ROUND(deduplicated::FLOAT / unique_content * 100, 1) as dedup_rate_pct,
    actual_size,
    theoretical_size,
    (theoretical_size - actual_size) as bytes_saved,
    ROUND((theoretical_size - actual_size)::FLOAT / theoretical_size * 100, 1) as storage_efficiency_pct
FROM dedup_analysis;
```

### 2.3 Utilit√°rio de Carregamento SQL

**Arquivo**: `src/baliza/sql_loader.py`

```python
from pathlib import Path
from typing import Dict, Optional
import string

class SQLLoader:
    """Carrega e parametriza queries SQL de arquivos externos."""
    
    def __init__(self, sql_root: Path = Path("sql")):
        self.sql_root = sql_root
        self._cache: Dict[str, str] = {}
    
    def load(self, query_path: str, **params) -> str:
        """Carrega SQL file e substitui par√¢metros."""
        if query_path not in self._cache:
            full_path = self.sql_root / query_path
            self._cache[query_path] = full_path.read_text(encoding='utf-8')
        
        template = string.Template(self._cache[query_path])
        return template.safe_substitute(**params)
```

## Fase 3: Nova DDL Robusta com Integra√ß√£o dbt (3-4 dias)

### 3.1 Arquitetura Medallion Otimizada

**Princ√≠pios de Design**:
1. **Compatibilidade total com dbt** - DDL gerada via dbt models
2. **Arquitetura Medallion** aprimorada (Bronze ‚Üí Silver ‚Üí Gold)
3. **ENUMs para dados categ√≥ricos** (economia de at√© 90%)
4. **Compress√£o por coluna** espec√≠fica
5. **Tipos compactos** onde apropriado
6. **Separa√ß√£o tier quente/frio** com materializa√ß√µes dbt

### 3.2 Estrutura dbt + DDL Otimizada

**Nova Estrutura de Projeto dbt**:
```
dbt_baliza/
‚îú‚îÄ‚îÄ models/
‚îÇ   ‚îú‚îÄ‚îÄ bronze/           # Raw data staging
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ _bronze_schema.yml
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ bronze_pncp_raw_hot.sql      # Last 90 days
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ bronze_pncp_raw_cold.sql     # Historical data
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ bronze_pncp_content_optimized.sql
‚îÇ   ‚îú‚îÄ‚îÄ silver/           # Business logic
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ _silver_schema.yml
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ silver_contratos_enhanced.sql
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ silver_atas_enhanced.sql
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ silver_contratacoes_enhanced.sql
‚îÇ   ‚îî‚îÄ‚îÄ gold/             # Analytics ready
‚îÇ       ‚îú‚îÄ‚îÄ _gold_schema.yml
‚îÇ       ‚îú‚îÄ‚îÄ gold_procurement_analytics_optimized.sql
‚îÇ       ‚îî‚îÄ‚îÄ gold_agency_performance.sql
‚îú‚îÄ‚îÄ macros/
‚îÇ   ‚îú‚îÄ‚îÄ compression/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ apply_column_compression.sql
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ create_optimized_table.sql
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ setup_enum_types.sql
‚îÇ   ‚îú‚îÄ‚îÄ optimization/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ partition_by_month.sql
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ create_strategic_indexes.sql
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ compress_and_checkpoint.sql
‚îÇ   ‚îî‚îÄ‚îÄ archival/
‚îÇ       ‚îú‚îÄ‚îÄ export_to_cold_storage.sql
‚îÇ       ‚îî‚îÄ‚îÄ tier_migration.sql
‚îî‚îÄ‚îÄ seeds/
    ‚îú‚îÄ‚îÄ enum_definitions.csv
    ‚îî‚îÄ‚îÄ compression_config.csv
```

### 3.3 Bronze Layer: Staging Conforme Schema Oficial PNCP

**Princ√≠pios do Mapeamento**:
1. **Fidelidade ao Schema OpenAPI**: Todos os tipos seguem exatamente a especifica√ß√£o oficial
2. **Tabelas de Dom√≠nio como ENUMs**: Usar c√≥digos oficiais do manual PNCP
3. **Valida√ß√£o de Integridade**: Constraints baseadas nos limites oficiais
4. **Normaliza√ß√£o Hier√°rquica**: Estrutura org√£o ‚Üí unidade ‚Üí contrata√ß√£o ‚Üí contrato

**Arquivo**: `dbt_baliza/models/bronze/bronze_pncp_raw_hot.sql`

```sql
-- Bronze Layer: Hot Tier (Last 90 days)
-- Materialization: table with optimized compression

{{ config(
    materialized='table',
    pre_hook="{{ setup_enum_types() }}",
    post_hook="{{ apply_table_compression('bronze_pncp_raw_hot') }}"
) }}

WITH raw_requests AS (
    SELECT 
        {{ generate_uuid() }} as request_id,
        endpoint_name::endpoint_type as endpoint_name,
        url_path,
        extracted_at,
        response_code::SMALLINT as response_code,
        CASE 
            WHEN response_code = 200 THEN 'success'::response_status
            WHEN response_code = 404 THEN 'not_found'::response_status
            WHEN response_code = 429 THEN 'rate_limit'::response_status
            WHEN response_code >= 500 THEN 'server_error'::response_status
            ELSE 'other'::response_status
        END as response_status,
        total_records,
        content_hash,
        file_size_bytes,
        COALESCE(processing_time_ms, 0)::SMALLINT as processing_time_ms
    FROM {{ source('raw', 'pncp_requests') }}
    WHERE extracted_at >= CURRENT_DATE - INTERVAL '90 days'
)

SELECT * FROM raw_requests
```

**Arquivo**: `dbt_baliza/models/bronze/_bronze_schema.yml`

```yaml
version: 2

sources:
  - name: raw
    description: Raw PNCP data from extractor
    tables:
      - name: pncp_requests
        description: Raw API requests and responses
        columns:
          - name: endpoint_name
            tests:
              - accepted_values:
                  values: ['contratos_publicacao', 'atas_publicacao', 'contratacoes_publicacao']
          - name: content_hash
            tests:
              - unique
              - not_null

models:
  - name: bronze_pncp_raw_hot
    description: "Optimized hot tier storage (last 90 days) with compression"
    config:
      materialized: table
      post_hook: |
        -- DuckDB escolhe compress√£o automaticamente - heur√≠stica √© eficiente
        -- Apenas definir ZSTD global, evitar for√ßar compress√µes espec√≠ficas
        SET default_compression='zstd';
    tests:
      - dbt_utils.expression_is_true:
          expression: "extracted_at >= CURRENT_DATE - INTERVAL '90 days'"
    
  - name: bronze_pncp_content_optimized
    description: "Deduplicated content with structured parsing"
    config:
      materialized: table
      post_hook: |
        {{ apply_column_compression([
          ('content_data', 'fsst'),
          ('content_size_bytes', 'bitpacking'),
          ('reference_count', 'bitpacking'),
          ('contract_id', 'dictionary'),
          ('agency_code', 'dictionary'),
          ('supplier_cnpj', 'dictionary')
        ]) }}
```

### 3.4 Macros dbt para Otimiza√ß√£o

**Arquivo**: `dbt_baliza/macros/compression/setup_enum_types.sql`

```sql
{% macro setup_enum_types() %}
  {% if execute %}
    -- ENUMs baseados na documenta√ß√£o oficial PNCP
    
    -- 5.2 Modalidade de Contrata√ß√£o (Manual PNCP)
    CREATE TYPE IF NOT EXISTS modalidade_contratacao AS ENUM (
        '1',   -- Leil√£o - Eletr√¥nico
        '2',   -- Di√°logo Competitivo  
        '3',   -- Concurso
        '4',   -- Concorr√™ncia - Eletr√¥nica
        '5',   -- Concorr√™ncia - Presencial
        '6',   -- Preg√£o - Eletr√¥nico
        '7',   -- Preg√£o - Presencial
        '8',   -- Dispensa de Licita√ß√£o
        '9',   -- Inexigibilidade
        '10',  -- Manifesta√ß√£o de Interesse
        '11',  -- Pr√©-qualifica√ß√£o
        '12',  -- Credenciamento
        '13'   -- Leil√£o - Presencial
    );

    -- 5.5 Situa√ß√£o da Contrata√ß√£o (Manual PNCP)  
    CREATE TYPE IF NOT EXISTS situacao_contratacao AS ENUM (
        '1',   -- Divulgada no PNCP
        '2',   -- Revogada
        '3',   -- Anulada
        '4'    -- Suspensa
    );

    -- 5.3 Modo de Disputa (Manual PNCP)
    CREATE TYPE IF NOT EXISTS modo_disputa AS ENUM (
        '1',   -- Aberto
        '2',   -- Fechado
        '3',   -- Aberto-Fechado
        '4',   -- Dispensa Com Disputa
        '5',   -- N√£o se aplica
        '6'    -- Fechado-Aberto
    );

    -- UF oficial (27 valores)
    CREATE TYPE IF NOT EXISTS uf_type AS ENUM (
        'AC','AL','AP','AM','BA','CE','DF','ES','GO','MA','MT','MS','MG',
        'PA','PB','PR','PE','PI','RJ','RN','RS','RO','RR','SC','SP','SE','TO'
    );

    -- 5.1 Instrumento Convocat√≥rio (Manual PNCP)
    CREATE TYPE IF NOT EXISTS instrumento_convocatorio AS ENUM (
        '1',   -- Edital
        '2',   -- Aviso de Contrata√ß√£o Direta
        '3'    -- Ato que autoriza a Contrata√ß√£o Direta
    );

    -- Tipo de Pessoa (OpenAPI)
    CREATE TYPE IF NOT EXISTS tipo_pessoa AS ENUM (
        'PJ',  -- Pessoa Jur√≠dica
        'PF',  -- Pessoa F√≠sica  
        'PE'   -- Pessoa Estrangeira
    );

    -- 5.9 Tipo de Contrato (Manual PNCP) - principais
    CREATE TYPE IF NOT EXISTS tipo_contrato AS ENUM (
        '1',   -- Contrato (termo inicial)
        '2',   -- Comodato
        '3',   -- Arrendamento
        '4',   -- Concess√£o
        '5',   -- Termo de Ades√£o
        '6',   -- Conv√™nio
        '7',   -- Empenho
        '8',   -- Outros
        '9',   -- TED
        '10',  -- ACT
        '11',  -- Termo de Compromisso
        '12'   -- Carta Contrato
    );

    -- Status de resposta da API (interno)
    CREATE TYPE IF NOT EXISTS response_status AS ENUM (
        'success',      -- 200
        'not_found',    -- 404  
        'rate_limit',   -- 429
        'server_error', -- 500+
        'timeout',      -- timeout
        'other'         -- outros c√≥digos
    );
  {% endif %}
{% endmacro %}
```

**Arquivo**: `dbt_baliza/macros/compression/apply_column_compression.sql`

```sql
{% macro apply_column_compression(column_specs) %}
  {% if execute %}
    {% for column_name, compression_type in column_specs %}
      -- DuckDB heur√≠stica autom√°tica √© melhor que for√ßar compress√µes espec√≠ficas
    -- Apenas aplicar ZSTD global se necess√°rio
    {% endfor %}
  {% endif %}
{% endmacro %}

{% macro apply_table_compression(table_name, compression='zstd') %}
  {% if execute %}
    -- Usar configura√ß√£o global ao inv√©s de por tabela
    -- SET default_compression='{{ compression }}';
    CHECKPOINT;  -- Force compression to take effect
  {% endif %}
{% endmacro %}
```

**Arquivo**: `dbt_baliza/macros/optimization/create_strategic_indexes.sql`

```sql
{% macro create_strategic_indexes(table_name, index_specs) %}
  {% if execute %}
    {% for index_name, columns in index_specs %}
      CREATE INDEX IF NOT EXISTS {{ index_name }}
      ON {{ table_name }}({{ columns | join(', ') }});
    {% endfor %}
  {% endif %}
{% endmacro %}

{% macro create_performance_indexes() %}
  {{ create_strategic_indexes('bronze_pncp_raw_hot', [
    ('idx_hot_endpoint_date', ['endpoint_name', 'extracted_at DESC']),
    ('idx_hot_hash', ['content_hash']),
    ('idx_hot_status', ['response_status', 'extracted_at DESC'])
  ]) }}
{% endmacro %}
```

### 3.5 Silver Layer: Estrutura Conforme Schema Oficial PNCP

**Arquivo**: `dbt_baliza/models/silver/silver_contratacoes_normalized.sql`

```sql
{{ config(
    materialized='table',
    post_hook=[
      "{{ apply_table_compression(this, 'zstd') }}",
      "{{ create_contratacao_indexes() }}"
    ]
) }}

-- Normaliza√ß√£o conforme estrutura oficial PNCP
WITH contratacoes_parsed AS (
    SELECT
        content_hash,
        
        -- Campos obrigat√≥rios (conforme OpenAPI)
        JSON_EXTRACT_STRING(content_data, '$.numeroControlePNCP') as numero_controle_pncp,
        JSON_EXTRACT_STRING(content_data, '$.numeroCompra') as numero_compra,
        JSON_EXTRACT(content_data, '$.anoCompra')::INTEGER as ano_compra,
        JSON_EXTRACT_STRING(content_data, '$.processo') as processo,
        
        -- Modalidade (refer√™ncia oficial Manual 5.2)
        JSON_EXTRACT(content_data, '$.modalidadeId')::modalidade_contratacao as modalidade_id,
        JSON_EXTRACT_STRING(content_data, '$.modalidadeNome') as modalidade_nome,
        
        -- Situa√ß√£o (refer√™ncia oficial Manual 5.5) 
        JSON_EXTRACT(content_data, '$.situacaoCompraId')::situacao_contratacao as situacao_compra_id,
        JSON_EXTRACT_STRING(content_data, '$.situacaoCompraNome') as situacao_compra_nome,
        
        -- Valores monet√°rios (precis√£o oficial: at√© 4 casas decimais)
        JSON_EXTRACT(content_data, '$.valorTotalEstimado')::DECIMAL(18,4) as valor_total_estimado,
        JSON_EXTRACT(content_data, '$.valorTotalHomologado')::DECIMAL(18,4) as valor_total_homologado,
        
        -- Datas (formato ISO)
        JSON_EXTRACT_STRING(content_data, '$.dataPublicacaoPncp')::TIMESTAMP as data_publicacao_pncp,
        JSON_EXTRACT_STRING(content_data, '$.dataAberturaProposta')::TIMESTAMP as data_abertura_proposta,
        JSON_EXTRACT_STRING(content_data, '$.dataEncerramentoProposta')::TIMESTAMP as data_encerramento_proposta,
        
        -- Textos com limites oficiais
        LEFT(JSON_EXTRACT_STRING(content_data, '$.objetoCompra'), 5120) as objeto_compra,
        LEFT(JSON_EXTRACT_STRING(content_data, '$.informacaoComplementar'), 5120) as informacao_complementar,
        
        -- Flags booleanas
        JSON_EXTRACT(content_data, '$.srp')::BOOLEAN as srp,
        
        -- √ìrg√£o/Entidade (estrutura hier√°rquica oficial)
        JSON_EXTRACT_STRING(content_data, '$.orgaoEntidade.cnpj') as orgao_cnpj,
        LEFT(JSON_EXTRACT_STRING(content_data, '$.orgaoEntidade.razaoSocial'), 255) as orgao_razao_social,
        JSON_EXTRACT_STRING(content_data, '$.orgaoEntidade.poderId') as poder_id,
        JSON_EXTRACT_STRING(content_data, '$.orgaoEntidade.esferaId') as esfera_id,
        
        -- Unidade Administrativa (estrutura hier√°rquica oficial)
        JSON_EXTRACT_STRING(content_data, '$.unidadeOrgao.codigoUnidade') as codigo_unidade,
        LEFT(JSON_EXTRACT_STRING(content_data, '$.unidadeOrgao.nomeUnidade'), 255) as nome_unidade,
        JSON_EXTRACT_STRING(content_data, '$.unidadeOrgao.ufSigla')::uf_type as uf_sigla,
        JSON_EXTRACT_STRING(content_data, '$.unidadeOrgao.municipioNome') as municipio_nome,
        JSON_EXTRACT(content_data, '$.unidadeOrgao.codigoIbge')::INTEGER as codigo_ibge,
        
        -- Amparo Legal (estrutura oficial)
        JSON_EXTRACT(content_data, '$.amparoLegal.codigo')::INTEGER as amparo_legal_codigo,
        JSON_EXTRACT_STRING(content_data, '$.amparoLegal.nome') as amparo_legal_nome,
        
        -- Metadata
        c.first_seen_at,
        c.reference_count
        
    FROM {{ ref('bronze_pncp_content_optimized') }} c
    WHERE JSON_EXTRACT_STRING(content_data, '$.numeroControlePNCP') IS NOT NULL
    AND JSON_VALID(content_data)
),

validated_contratacoes AS (
    SELECT 
        *,
        
        -- Valida√ß√µes conforme documenta√ß√£o oficial
        CASE 
            WHEN LENGTH(orgao_cnpj) = 14 THEN orgao_cnpj 
            ELSE NULL 
        END as cnpj_validado,
        
        -- Categoriza√ß√£o de valores (para analytics)
        CASE 
            WHEN valor_total_estimado >= 1000000.00 THEN 'alto_valor'
            WHEN valor_total_estimado >= 100000.00 THEN 'medio_valor'
            WHEN valor_total_estimado > 0 THEN 'baixo_valor'
            ELSE 'sem_valor'
        END as categoria_valor,
        
        -- Dimens√µes temporais para analytics
        DATE_TRUNC('month', data_publicacao_pncp) as mes_publicacao,
        DATE_TRUNC('year', data_publicacao_pncp) as ano_publicacao,
        
        -- Performance/quality metrics
        CASE WHEN reference_count > 1 THEN TRUE ELSE FALSE END as is_duplicate_content,
        
        -- Dura√ß√£o do per√≠odo de propostas (em horas)
        CASE 
            WHEN data_abertura_proposta IS NOT NULL 
            AND data_encerramento_proposta IS NOT NULL 
            THEN EXTRACT(EPOCH FROM (data_encerramento_proposta - data_abertura_proposta)) / 3600.0
            ELSE NULL 
        END as duracao_propostas_horas
        
    FROM contratacoes_parsed
    WHERE numero_controle_pncp IS NOT NULL
)

SELECT * FROM validated_contratacoes
```

### 3.6 Gold Layer: Analytics com Compress√£o M√°xima

**Arquivo**: `dbt_baliza/models/gold/gold_procurement_analytics_optimized.sql`

```sql
{{ config(
    materialized='table',
    post_hook=[
      "{{ apply_table_compression(this, 'zstd') }}",
      "{{ apply_column_compression([
        ('total_contratos', 'bitpacking'),
        ('valor_total', 'bitpacking'), 
        ('valor_medio', 'bitpacking'),
        ('orgao_razao_social', 'dictionary'),
        ('uf', 'dictionary'),
        ('modalidade_contratacao', 'dictionary')
      ]) }}"
    ]
) }}

WITH monthly_analytics AS (
    SELECT 
        mes_publicacao,
        ano_publicacao,
        uf::uf_type as uf,  -- Use ENUM for compression
        orgao_razao_social,
        modalidade_contratacao,
        valor_tier,
        
        -- Aggregated metrics with optimal types
        COUNT(*)::INTEGER as total_contratos,
        SUM(valor_inicial)::DECIMAL(18,2) as valor_total,
        AVG(valor_inicial)::DECIMAL(15,2) as valor_medio,
        MEDIAN(valor_inicial)::DECIMAL(15,2) as valor_mediano,
        
        -- Efficiency metrics
        COUNT(CASE WHEN is_duplicate_content THEN 1 END)::INTEGER as contratos_duplicados,
        SUM(reference_count)::INTEGER as total_referencias,
        
        -- Data quality score
        (COUNT(CASE WHEN orgao_cnpj IS NOT NULL 
                    AND valor_inicial > 0 
                    AND data_publicacao IS NOT NULL 
                THEN 1 END)::FLOAT / COUNT(*) * 100)::DECIMAL(5,2) as data_quality_score
        
    FROM {{ ref('silver_contratos_enhanced') }}
    WHERE data_publicacao >= '2021-01-01'
    GROUP BY 1,2,3,4,5,6
),

ranked_agencies AS (
    SELECT 
        *,
        ROW_NUMBER() OVER (
            PARTITION BY mes_publicacao, uf 
            ORDER BY valor_total DESC
        ) as ranking_mensal_uf,
        
        -- Moving averages for trend analysis
        AVG(valor_total) OVER (
            PARTITION BY orgao_razao_social, uf
            ORDER BY mes_publicacao
            ROWS BETWEEN 2 PRECEDING AND CURRENT ROW
        )::DECIMAL(18,2) as valor_media_movel_3m
        
    FROM monthly_analytics
)

SELECT * FROM ranked_agencies
WHERE total_contratos >= 5  -- Filter noise
```

### 3.7 Integra√ß√£o com Pipeline Python

**Arquivo**: `src/baliza/dbt_integration.py`

```python
from pathlib import Path
from typing import Dict, List, Optional
import subprocess
import duckdb

class DbtDuckDBManager:
    """Gerencia integra√ß√£o entre pipeline Python e transforma√ß√µes dbt."""
    
    def __init__(self, dbt_project_path: Path, db_path: Path):
        self.dbt_project_path = dbt_project_path
        self.db_path = db_path
    
    def run_optimized_transformations(self, models: Optional[List[str]] = None) -> Dict:
        """Executa transforma√ß√µes dbt com otimiza√ß√µes."""
        cmd = ["dbt", "run"]
        
        if models:
            cmd.extend(["--models"] + models)
        
        # Add performance flags
        cmd.extend([
            "--vars", "{'compression_enabled': true}",
            "--project-dir", str(self.dbt_project_path)
        ])
        
        result = subprocess.run(cmd, capture_output=True, text=True)
        
        if result.returncode == 0:
            self._run_post_transformation_optimization()
            
        return {
            "success": result.returncode == 0,
            "stdout": result.stdout,
            "stderr": result.stderr
        }
    
    def _run_post_transformation_optimization(self):
        """Otimiza√ß√µes p√≥s-dbt para m√°xima compress√£o."""
        with duckdb.connect(str(self.db_path)) as conn:
            # Force checkpoint para aplicar compress√£o
            conn.execute("CHECKPOINT;")
            
            # Collect statistics para query planner
            conn.execute("ANALYZE;")
            
            # Export para cold storage se necess√°rio
            self._export_old_data_to_cold_storage(conn)
    
    def get_compression_report(self) -> Dict:
        """Relat√≥rio de efici√™ncia da compress√£o."""
        with duckdb.connect(str(self.db_path)) as conn:
            tables = [
                'bronze_pncp_raw_hot',
                'silver_contratos_enhanced', 
                'gold_procurement_analytics_optimized'
            ]
            
            report = {}
            for table in tables:
                try:
                    result = conn.execute(f"""
                        SELECT * FROM pragma_storage_info('{table}')
                    """).fetchall()
                    report[table] = result
                except Exception as e:
                    report[table] = f"Error: {e}"
                    
            return report
```

### 3.8 Estrat√©gia de Migra√ß√£o Incremental

**Arquivo**: `dbt_baliza/macros/migration/incremental_migration.sql`

```sql
{% macro migrate_legacy_to_optimized(legacy_table, target_model, batch_size=100000) %}
  
  {% set migration_status_query %}
    SELECT COALESCE(MAX(migrated_batch_id), 0) as last_batch
    FROM migration_log 
    WHERE target_table = '{{ target_model }}'
  {% endset %}
  
  {% if execute %}
    {% set last_batch = run_query(migration_status_query).columns[0].values()[0] %}
    
    INSERT INTO {{ target_model }}
    SELECT * FROM (
      {{ get_legacy_data_batch(legacy_table, last_batch + 1, batch_size) }}
    );
    
    -- Log migration progress
    INSERT INTO migration_log (
      target_table, 
      migrated_batch_id, 
      migrated_at,
      records_migrated
    ) 
    SELECT 
      '{{ target_model }}',
      {{ last_batch + 1 }},
      CURRENT_TIMESTAMP,
      {{ batch_size }};
      
  {% endif %}
{% endmacro %}
```

### 3.9 Indexes Estrat√©gicos via dbt

```sql
-- sql/ddl/004_create_indexes.sql

-- Hot data indexes (performance critical)
CREATE INDEX idx_requests_hot_endpoint_date 
ON psa.pncp_requests_hot(endpoint_name, extracted_at DESC);

CREATE INDEX idx_requests_hot_hash 
ON psa.pncp_requests_hot(content_hash);

-- Analytics indexes
CREATE INDEX idx_content_agency_value 
ON psa.pncp_content_optimized(agency_code, contract_value DESC)
WHERE contract_value IS NOT NULL;

-- Specialized index for deduplication queries
CREATE INDEX idx_content_ref_count 
ON psa.pncp_content_optimized(reference_count DESC, content_size_bytes DESC);
```

## Fase 4: Estrat√©gia de Compress√£o e Arquivamento (2-3 dias)

### 4.1 Pipeline de Otimiza√ß√£o Autom√°tica

**Arquivo**: `src/baliza/optimization_pipeline.py`

```python
class DatabaseOptimizer:
    """Executa otimiza√ß√µes autom√°ticas baseadas em m√©tricas."""
    
    def analyze_compression_opportunities(self) -> Dict[str, Any]:
        """Usa pragma_storage_info para identificar alvos."""
        
    def apply_column_compression(self, table: str, recommendations: Dict):
        """Aplica algoritmos espec√≠ficos por coluna."""
        
    def migrate_to_cold_storage(self, cutoff_date: datetime):
        """Move dados antigos para tier frio em Parquet ZSTD."""
        
    def reclaim_space(self):
        """Executa CHECKPOINT e cleanup."""
```

### 4.2 Rotina de Arquivamento Inteligente

```sql
-- sql/maintenance/export_to_cold_storage.sql

-- Export old requests to cold storage (Parquet v2 + ZSTD)
COPY (
    SELECT * FROM psa.pncp_requests_hot 
    WHERE extracted_at < CURRENT_DATE - INTERVAL '90 days'
) TO 's3://baliza-archive/{year}-{month}/requests.parquet' (
    PARQUET_VERSION v2,
    COMPRESSION zstd,
    ROW_GROUP_SIZE 500000
);

-- Move to cold table and delete from hot
INSERT INTO psa.pncp_requests_cold 
SELECT * FROM psa.pncp_requests_hot 
WHERE extracted_at < CURRENT_DATE - INTERVAL '90 days';

DELETE FROM psa.pncp_requests_hot 
WHERE extracted_at < CURRENT_DATE - INTERVAL '90 days';

-- Force cleanup
CHECKPOINT;
```

## Fase 5: Migra√ß√£o e Testes (2-3 dias)

### 5.1 Script de Migra√ß√£o Segura

```sql
-- sql/migrations/v1_to_v2_breaking_changes.sql

-- BREAKING CHANGE NOTICE:
-- This migration breaks compatibility with BALIZA v1.x
-- Backup your data before proceeding

BEGIN TRANSACTION;

-- Step 1: Create new optimized schema
\i sql/ddl/002_create_schemas.sql
\i sql/ddl/003_create_core_tables.sql

-- Step 2: Migrate existing data with optimizations
INSERT INTO psa.pncp_requests_hot (
    request_id,
    endpoint_name,
    url_path,
    extracted_at,
    response_code,
    response_status,
    total_records,
    content_hash,
    file_size_bytes,
    processing_time_ms
)
SELECT 
    gen_random_uuid(),
    endpoint_name::endpoint_type,
    url_path,
    extracted_at,
    response_code::SMALLINT,
    CASE 
        WHEN response_code = 200 THEN 'success'::response_status
        WHEN response_code = 404 THEN 'not_found'::response_status
        WHEN response_code = 429 THEN 'rate_limit'::response_status
        WHEN response_code >= 500 THEN 'server_error'::response_status
        ELSE 'other'::response_status
    END,
    total_records,
    content_hash,
    file_size_bytes,
    COALESCE(processing_time_ms, 0)::SMALLINT
FROM legacy.pncp_requests
WHERE extracted_at >= CURRENT_DATE - INTERVAL '90 days';

-- Step 3: Archive old data to cold storage immediately
-- (older than 90 days goes straight to cold tier)

COMMIT;
```

### 5.2 Testes de Performance

**Arquivo**: `tests/test_database_optimization.py`

```python
def test_compression_ratios():
    """Verifica que compress√£o est√° funcionando."""
    
def test_query_performance():
    """Benchmark queries antes/depois da otimiza√ß√£o."""
    
def test_storage_efficiency():
    """Valida economia de espa√ßo em disco."""
```

## Cronograma e Entreg√°veis

| Fase | Dura√ß√£o | Entreg√°veis | Crit√©rio de Sucesso |
|------|---------|-------------|-------------------|
| 1 | 1-2 dias | `docs/database-audit-report.md`, `docs/sql-inventory.md` | Diagn√≥stico completo do estado atual |
| 2 | 2-3 dias | Estrutura `sql/`, `SQLLoader` class | Todo SQL extra√≠do e parametrizado |
| 3 | 3-4 dias | Nova DDL com ENUMs e compress√£o | Schema otimizado criado |
| 4 | 2-3 dias | Pipeline de arquivamento | Tier quente/frio funcionando |
| 5 | 2-3 dias | Scripts de migra√ß√£o, testes | Migra√ß√£o segura validada |

**Total: 10-15 dias √∫teis**

## M√©tricas de Sucesso

### Antes vs Depois

| M√©trica | Estado Atual | Meta P√≥s-Otimiza√ß√£o | M√©todo de Medi√ß√£o |
|---------|--------------|-------------------|-------------------|
| Tamanho DB | TBD | -40% a -80% | `pragma_storage_info()` |
| Tempo query analytics | TBD | -50% a -70% | Benchmark suite |
| Consultas por segundo | TBD | +100% a +200% | Load testing |
| Tempo de backup | TBD | -60% a -80% | Backup pipeline |

### Qualidade do C√≥digo

- ‚úÖ Zero SQL inline no c√≥digo Python
- ‚úÖ 100% das queries em arquivos `.sql` versionados
- ‚úÖ Cobertura de testes para todas as queries cr√≠ticas
- ‚úÖ Documenta√ß√£o completa da nova arquitetura

## Riscos e Mitiga√ß√µes

| Risco | Probabilidade | Impacto | Mitiga√ß√£o |
|-------|---------------|---------|-----------|
| Perda de dados na migra√ß√£o | Baixa | Alto | Backup completo + dry-run + rollback plan |
| Performance pior que esperado | M√©dia | M√©dio | Benchmark incremental + rollback |
| Queries quebradas | Alta | Alto | Test suite abrangente + gradual rollout |
| Incompatibilidade com c√≥digo existente | Alta | Alto | Wrapper classes + deprecation warnings |

## Conclus√£o

Este plano implementa uma refatora√ß√£o ambiciosa mas necess√°ria da arquitetura BALIZA. A combina√ß√£o de **SQL extra√≠do**, **DDL otimizada** e **arquivamento inteligente** resultar√° em:

- **Melhor manutenibilidade** (SQL versionado e test√°vel)
- **Performance superior** (compress√£o otimizada + tipos corretos)
- **Efici√™ncia de armazenamento** (tier quente/frio + ENUMs)
- **Escalabilidade futura** (arquitetura preparada para crescimento)

*Sapere aude* - mediremos cada otimiza√ß√£o e s√≥ avan√ßaremos se os ganhos justificarem o esfor√ßo.