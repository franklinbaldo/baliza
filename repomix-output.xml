This file is a merged representation of the entire codebase, combined into a single document by Repomix.

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files (if enabled)
5. Multiple file entries, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Files are sorted by Git change count (files with more changes are at the bottom)
</notes>

</file_summary>

<directory_structure>
.github/
  workflows/
    baliza_daily_run.yml
    code_quality.yml
dbt_baliza/
  macros/
    extract_organization_data.sql
  models/
    dimensions/
      dim_organizacoes.sql
      dim_unidades_orgao.sql
    facts/
      fact_contratacoes.sql
      fact_contratos.sql
    marts/
      mart_procurement_analytics.sql
    staging/
      stg_atas_raw.sql
      stg_contratacoes_raw.sql
      stg_contratos_raw.sql
    sources.yml
  .user.yml
  dbt_project.yml
  profiles.yml
docs/
  openapi/
    api-pncp-consulta.json
src/
  baliza/
    __init__.py
    .gitignore
    pncp_extractor.py
tests/
  conftest.py
  README.md
.gitignore
.pre-commit-config.yaml
LICENSE
pyproject.toml
README.md
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path=".github/workflows/code_quality.yml">
name: Code Quality

on:
  pull_request:
    branches: [main]
  push:
    branches: [main]
  workflow_dispatch:

# Cancel previous runs if a new commit is pushed
concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true

jobs:
  code_quality:
    name: Code Quality Checks
    runs-on: ubuntu-latest
    timeout-minutes: 15
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0  # Full history for better blame info

      - name: Install uv
        run: |
          curl -LsSf https://astral.sh/uv/install.sh | sh
          echo "$HOME/.cargo/bin" >> $GITHUB_PATH

      - name: Set up Python with uv
        run: |
          uv venv --python 3.11 .venv
          echo "VIRTUAL_ENV=.venv" >> $GITHUB_ENV
          echo ".venv/bin" >> $GITHUB_PATH

      - name: Install dependencies
        run: |
          source $VIRTUAL_ENV/bin/activate
          uv sync --frozen-lockfile
          
      - name: Cache pre-commit hooks
        uses: actions/cache@v3
        with:
          path: ~/.cache/pre-commit
          key: pre-commit-${{ runner.os }}-${{ hashFiles('.pre-commit-config.yaml') }}
          restore-keys: |
            pre-commit-${{ runner.os }}-

      - name: Install pre-commit
        run: |
          source $VIRTUAL_ENV/bin/activate
          pre-commit install

      - name: Run Ruff linting
        run: |
          source $VIRTUAL_ENV/bin/activate
          echo "🔍 Running Ruff linter..."
          ruff check . --output-format=github
        continue-on-error: false

      - name: Run Ruff formatting check
        run: |
          source $VIRTUAL_ENV/bin/activate
          echo "🎨 Checking Ruff formatting..."
          ruff format --check --diff .
        continue-on-error: false

      - name: Run MyPy type checking
        run: |
          source $VIRTUAL_ENV/bin/activate
          echo "🏷️ Running MyPy type checking..."
          mypy src/ --show-error-codes --pretty
        continue-on-error: true  # Type errors shouldn't block PRs initially

      - name: Run security checks with Bandit
        run: |
          source $VIRTUAL_ENV/bin/activate
          echo "🔒 Running Bandit security checks..."
          bandit -r src/ -f json -o bandit-report.json || true
          bandit -r src/ -f txt
        continue-on-error: true

      - name: Upload Bandit report
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: bandit-security-report
          path: bandit-report.json
          retention-days: 30

      - name: Run tests with coverage
        run: |
          source $VIRTUAL_ENV/bin/activate
          echo "🧪 Running tests with coverage..."
          pytest tests/ \
            --cov=src \
            --cov-report=term-missing \
            --cov-report=xml \
            --cov-report=html \
            --junitxml=pytest-report.xml \
            -v
        continue-on-error: false

      - name: Upload coverage reports
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: coverage-reports
          path: |
            coverage.xml
            htmlcov/
            pytest-report.xml
          retention-days: 30

      - name: Upload coverage to Codecov
        if: always()
        uses: codecov/codecov-action@v3
        with:
          file: coverage.xml
          flags: unittests
          name: codecov-umbrella
          fail_ci_if_error: false

      - name: Check import sorting
        run: |
          source $VIRTUAL_ENV/bin/activate
          echo "📦 Checking import sorting..."
          ruff check --select I .
        continue-on-error: false

      - name: Check for TODO/FIXME comments
        run: |
          echo "📝 Checking for TODO/FIXME comments..."
          if grep -r "TODO\|FIXME\|XXX\|HACK" src/ tests/ --exclude-dir=.git --exclude-dir=__pycache__ --exclude="*.pyc"; then
            echo "⚠️ Found TODO/FIXME comments. Consider addressing them."
          else
            echo "✅ No TODO/FIXME comments found."
          fi
        continue-on-error: true

      - name: Check file permissions
        run: |
          echo "🔐 Checking file permissions..."
          find src/ tests/ -name "*.py" -executable -type f | while read file; do
            echo "⚠️ Python file has executable permission: $file"
          done
        continue-on-error: true

      - name: Validate YAML files
        run: |
          source $VIRTUAL_ENV/bin/activate
          echo "📋 Validating YAML files..."
          python -c "
          import yaml
          import sys
          import glob
          
          yaml_files = glob.glob('**/*.yml', recursive=True) + glob.glob('**/*.yaml', recursive=True)
          errors = 0
          
          for file in yaml_files:
              try:
                  with open(file, 'r') as f:
                      yaml.safe_load(f)
                  print(f'✅ {file}')
              except yaml.YAMLError as e:
                  print(f'❌ {file}: {e}')
                  errors += 1
          
          if errors > 0:
              print(f'Found {errors} YAML validation errors')
              sys.exit(1)
          else:
              print('All YAML files are valid')
          "

      - name: Check code complexity
        run: |
          source $VIRTUAL_ENV/bin/activate
          echo "🔢 Checking code complexity..."
          python -c "
          import ast
          import glob
          
          def get_complexity(node):
              complexity = 1
              for child in ast.walk(node):
                  if isinstance(child, (ast.If, ast.While, ast.For, ast.comprehension)):
                      complexity += 1
                  elif isinstance(child, ast.BoolOp):
                      complexity += len(child.values) - 1
              return complexity
          
          high_complexity = []
          
          for file in glob.glob('src/**/*.py', recursive=True):
              try:
                  with open(file, 'r') as f:
                      tree = ast.parse(f.read())
                  
                  for node in ast.walk(tree):
                      if isinstance(node, ast.FunctionDef):
                          complexity = get_complexity(node)
                          if complexity > 10:
                              high_complexity.append((file, node.name, complexity))
              except Exception as e:
                  print(f'Error parsing {file}: {e}')
          
          if high_complexity:
              print('⚠️ Functions with high complexity (>10):')
              for file, func, complexity in high_complexity:
                  print(f'  {file}:{func} - {complexity}')
          else:
              print('✅ No functions with high complexity found')
          "
        continue-on-error: true

  dependency_check:
    name: Dependency Security Check
    runs-on: ubuntu-latest
    timeout-minutes: 10
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Install uv
        run: |
          curl -LsSf https://astral.sh/uv/install.sh | sh
          echo "$HOME/.cargo/bin" >> $GITHUB_PATH

      - name: Set up Python with uv
        run: |
          uv venv --python 3.11 .venv
          echo "VIRTUAL_ENV=.venv" >> $GITHUB_ENV
          echo ".venv/bin" >> $GITHUB_PATH

      - name: Install dependencies
        run: |
          source $VIRTUAL_ENV/bin/activate
          uv sync --frozen-lockfile

      - name: Run safety check
        run: |
          source $VIRTUAL_ENV/bin/activate
          # Install safety for dependency vulnerability checking
          uv add safety --dev
          echo "🛡️ Checking dependencies for known vulnerabilities..."
          safety check --json --output safety-report.json || true
          safety check
        continue-on-error: true

      - name: Upload safety report
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: safety-security-report
          path: safety-report.json
          retention-days: 30

  documentation_check:
    name: Documentation Check
    runs-on: ubuntu-latest
    timeout-minutes: 5
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Check README exists and is substantial
        run: |
          echo "📖 Checking documentation..."
          if [ ! -f "README.md" ]; then
            echo "❌ README.md not found"
            exit 1
          fi
          
          lines=$(wc -l < README.md)
          if [ $lines -lt 20 ]; then
            echo "⚠️ README.md seems too short ($lines lines)"
          else
            echo "✅ README.md exists and has substantial content ($lines lines)"
          fi

      - name: Check for proper documentation structure
        run: |
          echo "📋 Checking documentation structure..."
          required_sections=("## " "### " "- " "```")
          missing_sections=()
          
          for section in "${required_sections[@]}"; do
            if ! grep -q "$section" README.md; then
              missing_sections+=("$section")
            fi
          done
          
          if [ ${#missing_sections[@]} -gt 0 ]; then
            echo "⚠️ README.md missing some documentation patterns:"
            printf '%s\n' "${missing_sections[@]}"
          else
            echo "✅ README.md has good documentation structure"
          fi

      - name: Check for broken internal links
        run: |
          echo "🔗 Checking for broken internal links..."
          # Extract markdown links and check if referenced files exist
          grep -oP '\[.*?\]\(\K[^)]+' README.md | grep -v '^http' | while read -r link; do
            if [ ! -f "$link" ] && [ ! -d "$link" ]; then
              echo "⚠️ Broken internal link: $link"
            fi
          done || echo "✅ No broken internal links found"

  quality_gate:
    name: Quality Gate
    needs: [code_quality, dependency_check, documentation_check]
    runs-on: ubuntu-latest
    if: always()
    
    steps:
      - name: Check quality gate status
        run: |
          echo "🚪 Quality Gate Summary"
          echo "======================"
          
          code_quality_result="${{ needs.code_quality.result }}"
          dependency_result="${{ needs.dependency_check.result }}"
          docs_result="${{ needs.documentation_check.result }}"
          
          echo "Code Quality: $code_quality_result"
          echo "Dependency Check: $dependency_result"
          echo "Documentation: $docs_result"
          
          # Quality gate passes if critical checks pass
          if [ "$code_quality_result" = "success" ]; then
            echo "✅ Quality gate PASSED"
            echo "QUALITY_GATE_STATUS=passed" >> $GITHUB_ENV
          else
            echo "❌ Quality gate FAILED"
            echo "QUALITY_GATE_STATUS=failed" >> $GITHUB_ENV
            exit 1
          fi

      - name: Comment on PR
        if: github.event_name == 'pull_request'
        uses: actions/github-script@v7
        with:
          script: |
            const status = process.env.QUALITY_GATE_STATUS;
            const emoji = status === 'passed' ? '✅' : '❌';
            const message = status === 'passed' ? 'PASSED' : 'FAILED';
            
            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: `${emoji} Quality Gate ${message}\n\nCode quality checks have been completed. See the Actions tab for detailed results.`
            });
</file>

<file path="dbt_baliza/macros/extract_organization_data.sql">
{% macro extract_organization_data(json_field, prefix) %}
  {{ json_field }} ->> 'cnpj' AS {{ prefix }}_cnpj,
  {{ json_field }} ->> 'razaoSocial' AS {{ prefix }}_razao_social,
  {{ json_field }} ->> 'poderId' AS {{ prefix }}_poder_id,
  {{ json_field }} ->> 'esferaId' AS {{ prefix }}_esfera_id
{% endmacro %}

{% macro extract_unit_data(json_field, prefix) %}
  {{ json_field }} ->> 'ufNome' AS {{ prefix }}_uf_nome,
  {{ json_field }} ->> 'ufSigla' AS {{ prefix }}_uf_sigla,
  {{ json_field }} ->> 'codigoUnidade' AS {{ prefix }}_codigo_unidade,
  {{ json_field }} ->> 'nomeUnidade' AS {{ prefix }}_nome_unidade,
  {{ json_field }} ->> 'municipioNome' AS {{ prefix }}_municipio_nome,
  {{ json_field }} ->> 'codigoIbge' AS {{ prefix }}_codigo_ibge
{% endmacro %}

{% macro extract_legal_framework_data(json_field, prefix) %}
  CAST({{ json_field }} ->> 'codigo' AS INTEGER) AS {{ prefix }}_codigo,
  {{ json_field }} ->> 'nome' AS {{ prefix }}_nome,
  {{ json_field }} ->> 'descricao' AS {{ prefix }}_descricao
{% endmacro %}

{% macro extract_type_data(json_field, prefix) %}
  CAST({{ json_field }} ->> 'id' AS INTEGER) AS {{ prefix }}_id,
  {{ json_field }} ->> 'nome' AS {{ prefix }}_nome
{% endmacro %}
</file>

<file path="dbt_baliza/models/dimensions/dim_organizacoes.sql">
{{
  config(
    materialized='table',
    description='Dimension table for organizations (órgãos and entidades)'
  )
}}

WITH org_from_contracts AS (
  SELECT DISTINCT
    {{ extract_organization_data('orgao_entidade_json', 'org') }}
  FROM {{ ref('stg_contratos_raw') }}
  WHERE orgao_entidade_json IS NOT NULL
),

org_from_procurements AS (
  SELECT DISTINCT
    {{ extract_organization_data('orgao_entidade_json', 'org') }}
  FROM {{ ref('stg_contratacoes_raw') }}
  WHERE orgao_entidade_json IS NOT NULL
),

subrog_from_contracts AS (
  SELECT DISTINCT
    {{ extract_organization_data('orgao_subrogado_json', 'org') }}
  FROM {{ ref('stg_contratos_raw') }}
  WHERE orgao_subrogado_json IS NOT NULL
),

subrog_from_procurements AS (
  SELECT DISTINCT
    {{ extract_organization_data('orgao_subrogado_json', 'org') }}
  FROM {{ ref('stg_contratacoes_raw') }}
  WHERE orgao_subrogado_json IS NOT NULL
),

all_organizations AS (
  SELECT * FROM org_from_contracts
  UNION ALL
  SELECT * FROM org_from_procurements
  UNION ALL
  SELECT * FROM subrog_from_contracts
  UNION ALL
  SELECT * FROM subrog_from_procurements
),

deduplicated_organizations AS (
  SELECT DISTINCT
    org_cnpj,
    org_razao_social,
    org_poder_id,
    org_esfera_id
  FROM all_organizations
  WHERE org_cnpj IS NOT NULL
)

SELECT
  -- Surrogate key
  MD5(org_cnpj) AS org_key,
  
  -- Natural key
  org_cnpj AS cnpj,
  
  -- Organization details
  org_razao_social AS razao_social,
  org_poder_id AS poder_id,
  org_esfera_id AS esfera_id,
  
  -- Derived attributes
  CASE 
    WHEN org_poder_id = 'E' THEN 'Executivo'
    WHEN org_poder_id = 'L' THEN 'Legislativo'
    WHEN org_poder_id = 'J' THEN 'Judiciário'
    WHEN org_poder_id = 'M' THEN 'Ministério Público'
    ELSE 'Outros'
  END AS poder_nome,
  
  CASE 
    WHEN org_esfera_id = 'F' THEN 'Federal'
    WHEN org_esfera_id = 'E' THEN 'Estadual'
    WHEN org_esfera_id = 'M' THEN 'Municipal'
    ELSE 'Outros'
  END AS esfera_nome,
  
  -- Metadata
  CURRENT_TIMESTAMP AS created_at,
  CURRENT_TIMESTAMP AS updated_at

FROM deduplicated_organizations
ORDER BY org_cnpj
</file>

<file path="dbt_baliza/models/dimensions/dim_unidades_orgao.sql">
{{
  config(
    materialized='table',
    description='Dimension table for organizational units (unidades do órgão)'
  )
}}

WITH units_from_contracts AS (
  SELECT DISTINCT
    {{ extract_organization_data('orgao_entidade_json', 'org') }},
    {{ extract_unit_data('unidade_orgao_json', 'unit') }}
  FROM {{ ref('stg_contratos_raw') }}
  WHERE orgao_entidade_json IS NOT NULL
    AND unidade_orgao_json IS NOT NULL
),

units_from_procurements AS (
  SELECT DISTINCT
    {{ extract_organization_data('orgao_entidade_json', 'org') }},
    {{ extract_unit_data('unidade_orgao_json', 'unit') }}
  FROM {{ ref('stg_contratacoes_raw') }}
  WHERE orgao_entidade_json IS NOT NULL
    AND unidade_orgao_json IS NOT NULL
),

subrog_units_from_contracts AS (
  SELECT DISTINCT
    {{ extract_organization_data('orgao_subrogado_json', 'org') }},
    {{ extract_unit_data('unidade_subrogada_json', 'unit') }}
  FROM {{ ref('stg_contratos_raw') }}
  WHERE orgao_subrogado_json IS NOT NULL
    AND unidade_subrogada_json IS NOT NULL
),

subrog_units_from_procurements AS (
  SELECT DISTINCT
    {{ extract_organization_data('orgao_subrogado_json', 'org') }},
    {{ extract_unit_data('unidade_subrogada_json', 'unit') }}
  FROM {{ ref('stg_contratacoes_raw') }}
  WHERE orgao_subrogado_json IS NOT NULL
    AND unidade_subrogada_json IS NOT NULL
),

all_units AS (
  SELECT * FROM units_from_contracts
  UNION ALL
  SELECT * FROM units_from_procurements
  UNION ALL
  SELECT * FROM subrog_units_from_contracts
  UNION ALL
  SELECT * FROM subrog_units_from_procurements
),

deduplicated_units AS (
  SELECT DISTINCT
    org_cnpj,
    unit_codigo_unidade,
    unit_nome_unidade,
    unit_uf_nome,
    unit_uf_sigla,
    unit_municipio_nome,
    unit_codigo_ibge
  FROM all_units
  WHERE org_cnpj IS NOT NULL
    AND unit_codigo_unidade IS NOT NULL
)

SELECT
  -- Surrogate key
  MD5(org_cnpj || '|' || unit_codigo_unidade) AS unit_key,
  
  -- Natural keys
  org_cnpj AS cnpj_orgao,
  unit_codigo_unidade AS codigo_unidade,
  
  -- Unit details
  unit_nome_unidade AS nome_unidade,
  unit_uf_nome AS uf_nome,
  unit_uf_sigla AS uf_sigla,
  unit_municipio_nome AS municipio_nome,
  unit_codigo_ibge AS codigo_ibge,
  
  -- Derived attributes
  CASE 
    WHEN unit_uf_sigla IN ('AC', 'AL', 'AP', 'AM', 'BA', 'CE', 'DF', 'ES', 'GO', 'MA', 'MT', 'MS', 'MG', 'PA', 'PB', 'PR', 'PE', 'PI', 'RJ', 'RN', 'RS', 'RO', 'RR', 'SC', 'SP', 'SE', 'TO') THEN unit_uf_sigla
    ELSE 'OUTROS'
  END AS uf_sigla_normalizada,
  
  CASE 
    WHEN unit_uf_sigla IN ('AC', 'AM', 'AP', 'PA', 'RO', 'RR', 'TO') THEN 'Norte'
    WHEN unit_uf_sigla IN ('AL', 'BA', 'CE', 'MA', 'PB', 'PE', 'PI', 'RN', 'SE') THEN 'Nordeste'
    WHEN unit_uf_sigla IN ('GO', 'MT', 'MS', 'DF') THEN 'Centro-Oeste'
    WHEN unit_uf_sigla IN ('ES', 'MG', 'RJ', 'SP') THEN 'Sudeste'
    WHEN unit_uf_sigla IN ('PR', 'RS', 'SC') THEN 'Sul'
    ELSE 'Outros'
  END AS regiao,
  
  -- Metadata
  CURRENT_TIMESTAMP AS created_at,
  CURRENT_TIMESTAMP AS updated_at

FROM deduplicated_units
ORDER BY org_cnpj, unit_codigo_unidade
</file>

<file path="dbt_baliza/models/facts/fact_contratacoes.sql">
{{
  config(
    materialized='table',
    description='Fact table for procurements (contratações)',
    indexes=[
      {'columns': ['numero_controle_pncp'], 'unique': True},
      {'columns': ['data_publicacao_pncp']},
      {'columns': ['ano_compra']},
      {'columns': ['modalidade_id']},
      {'columns': ['org_key']},
      {'columns': ['unit_key']}
    ]
  )
}}

WITH procurements_with_keys AS (
  SELECT
    p.*,
    
    -- Organization keys
    org.org_key,
    unit.unit_key,
    
    -- Subrogated organization keys
    subrog_org.org_key AS subrog_org_key,
    subrog_unit.unit_key AS subrog_unit_key
    
  FROM {{ ref('stg_contratacoes_raw') }} p
  
  -- Main organization
  LEFT JOIN {{ ref('dim_organizacoes') }} org
    ON p.orgao_entidade_json ->> 'cnpj' = org.cnpj
  
  -- Main unit
  LEFT JOIN {{ ref('dim_unidades_orgao') }} unit
    ON p.orgao_entidade_json ->> 'cnpj' = unit.cnpj_orgao
    AND p.unidade_orgao_json ->> 'codigoUnidade' = unit.codigo_unidade
  
  -- Subrogated organization
  LEFT JOIN {{ ref('dim_organizacoes') }} subrog_org
    ON p.orgao_subrogado_json ->> 'cnpj' = subrog_org.cnpj
  
  -- Subrogated unit
  LEFT JOIN {{ ref('dim_unidades_orgao') }} subrog_unit
    ON p.orgao_subrogado_json ->> 'cnpj' = subrog_unit.cnpj_orgao
    AND p.unidade_subrogada_json ->> 'codigoUnidade' = subrog_unit.codigo_unidade
)

SELECT
  -- Surrogate key
  MD5(numero_controle_pncp) AS procurement_key,
  
  -- Natural key
  numero_controle_pncp,
  
  -- Procurement identifiers
  numero_compra,
  ano_compra,
  sequencial_compra,
  
  -- Dates
  data_publicacao_pncp,
  data_abertura_proposta,
  data_encerramento_proposta,
  data_inclusao,
  data_atualizacao,
  data_atualizacao_global,
  
  -- Duration calculations
  CASE 
    WHEN data_abertura_proposta IS NOT NULL AND data_encerramento_proposta IS NOT NULL
    THEN DATE_DIFF('day', data_abertura_proposta, data_encerramento_proposta)
    ELSE NULL
  END AS duracao_proposta_dias,
  
  -- Amounts
  valor_total_estimado,
  valor_total_homologado,
  
  -- Procurement details
  objeto_compra,
  informacao_complementar,
  processo,
  link_sistema_origem,
  link_processo_eletronico,
  justificativa_presencial,
  
  -- Procurement method and mode
  modalidade_id,
  modalidade_nome,
  modalidade, -- from API parameter
  modo_disputa_id,
  modo_disputa_nome,
  
  -- Instrument and framework
  tipo_instrumento_convocatorio_codigo,
  tipo_instrumento_convocatorio_nome,
  
  -- Status and flags
  situacao_compra_id,
  situacao_compra_nome,
  srp,
  existe_resultado,
  
  -- User information
  usuario_nome,
  
  -- Foreign keys
  org_key,
  unit_key,
  subrog_org_key,
  subrog_unit_key,
  
  -- Legal framework information (extracted from JSON)
  amparo_legal_json ->> 'codigo' AS amparo_legal_codigo,
  amparo_legal_json ->> 'nome' AS amparo_legal_nome,
  amparo_legal_json ->> 'descricao' AS amparo_legal_descricao,
  
  -- Derived attributes
  CASE 
    WHEN modalidade_id = 1 THEN 'Leilão Eletrônico'
    WHEN modalidade_id = 3 THEN 'Concurso'
    WHEN modalidade_id = 4 THEN 'Concorrência Eletrônica'
    WHEN modalidade_id = 6 THEN 'Pregão Eletrônico'
    WHEN modalidade_id = 8 THEN 'Dispensa'
    WHEN modalidade_id = 9 THEN 'Inexigibilidade'
    WHEN modalidade_id = 10 THEN 'Credenciamento'
    WHEN modalidade_id = 11 THEN 'Seleção'
    WHEN modalidade_id = 12 THEN 'Consulta'
    WHEN modalidade_id = 13 THEN 'Registro de Preço'
    WHEN modalidade_id = 14 THEN 'Outros'
    ELSE 'Não informado'
  END AS modalidade_descricao,
  
  CASE 
    WHEN valor_total_estimado IS NOT NULL AND valor_total_estimado > 0 THEN
      CASE 
        WHEN valor_total_estimado <= 17600 THEN 'Até R$ 17.600'
        WHEN valor_total_estimado <= 88000 THEN 'R$ 17.601 a R$ 88.000'
        WHEN valor_total_estimado <= 176000 THEN 'R$ 88.001 a R$ 176.000'
        WHEN valor_total_estimado <= 1408000 THEN 'R$ 176.001 a R$ 1.408.000'
        WHEN valor_total_estimado <= 3300000 THEN 'R$ 1.408.001 a R$ 3.300.000'
        ELSE 'Acima de R$ 3.300.000'
      END
    ELSE 'Não informado'
  END AS faixa_valor_estimado,
  
  CASE 
    WHEN situacao_compra_id = '1' THEN 'Planejada'
    WHEN situacao_compra_id = '2' THEN 'Publicada'
    WHEN situacao_compra_id = '3' THEN 'Homologada'
    WHEN situacao_compra_id = '4' THEN 'Deserta/Fracassada'
    ELSE 'Não informado'
  END AS situacao_compra_descricao,
  
  CASE 
    WHEN data_abertura_proposta IS NOT NULL AND data_encerramento_proposta IS NOT NULL THEN
      CASE 
        WHEN DATE_DIFF('day', data_abertura_proposta, data_encerramento_proposta) <= 7 THEN 'Até 7 dias'
        WHEN DATE_DIFF('day', data_abertura_proposta, data_encerramento_proposta) <= 15 THEN '8 a 15 dias'
        WHEN DATE_DIFF('day', data_abertura_proposta, data_encerramento_proposta) <= 30 THEN '16 a 30 dias'
        WHEN DATE_DIFF('day', data_abertura_proposta, data_encerramento_proposta) <= 60 THEN '31 a 60 dias'
        ELSE 'Mais de 60 dias'
      END
    ELSE 'Não informado'
  END AS faixa_duracao_proposta,
  
  -- Data quality flags
  CASE 
    WHEN numero_controle_pncp IS NULL THEN 'Número de controle ausente'
    WHEN modalidade_id IS NULL THEN 'Modalidade ausente'
    WHEN valor_total_estimado IS NULL OR valor_total_estimado <= 0 THEN 'Valor estimado inválido'
    WHEN data_publicacao_pncp IS NULL THEN 'Data de publicação ausente'
    WHEN objeto_compra IS NULL THEN 'Objeto da compra ausente'
    ELSE 'OK'
  END AS quality_flag,
  
  -- Metadata
  endpoint_name,
  data_date,
  extracted_at,
  run_id,
  
  -- JSON fallback
  procurement_json,
  fontes_orcamentarias_json,
  
  -- Audit
  CURRENT_TIMESTAMP AS created_at,
  CURRENT_TIMESTAMP AS updated_at

FROM procurements_with_keys
ORDER BY numero_controle_pncp
</file>

<file path="dbt_baliza/models/facts/fact_contratos.sql">
{{
  config(
    materialized='table',
    description='Fact table for contracts (contratos/empenhos)',
    indexes=[
      {'columns': ['numero_controle_pncp'], 'unique': True},
      {'columns': ['data_assinatura']},
      {'columns': ['ano_contrato']},
      {'columns': ['org_key']},
      {'columns': ['unit_key']}
    ]
  )
}}

WITH contracts_with_keys AS (
  SELECT
    c.*,
    
    -- Organization keys
    org.org_key,
    unit.unit_key,
    
    -- Subrogated organization keys
    subrog_org.org_key AS subrog_org_key,
    subrog_unit.unit_key AS subrog_unit_key
    
  FROM {{ ref('stg_contratos_raw') }} c
  
  -- Main organization
  LEFT JOIN {{ ref('dim_organizacoes') }} org
    ON c.orgao_entidade_json ->> 'cnpj' = org.cnpj
  
  -- Main unit
  LEFT JOIN {{ ref('dim_unidades_orgao') }} unit
    ON c.orgao_entidade_json ->> 'cnpj' = unit.cnpj_orgao
    AND CAST(c.unidade_orgao_json ->> 'codigoUnidade' AS VARCHAR) = CAST(unit.codigo_unidade AS VARCHAR)
  
  -- Subrogated organization
  LEFT JOIN {{ ref('dim_organizacoes') }} subrog_org
    ON c.orgao_subrogado_json ->> 'cnpj' = subrog_org.cnpj
  
  -- Subrogated unit
  LEFT JOIN {{ ref('dim_unidades_orgao') }} subrog_unit
    ON c.orgao_subrogado_json ->> 'cnpj' = subrog_unit.cnpj_orgao
    AND CAST(c.unidade_subrogada_json ->> 'codigoUnidade' AS VARCHAR) = CAST(subrog_unit.codigo_unidade AS VARCHAR)
)

SELECT
  -- Surrogate key
  MD5(numero_controle_pncp) AS contract_key,
  
  -- Natural key
  numero_controle_pncp,
  
  -- Contract identifiers
  numero_controle_pncp_compra,
  numero_contrato_empenho,
  ano_contrato,
  sequencial_contrato,
  
  -- Dates
  data_assinatura,
  data_vigencia_inicio,
  data_vigencia_fim,
  data_publicacao_pncp,
  data_atualizacao,
  data_atualizacao_global,
  
  -- Duration calculations
  CASE 
    WHEN data_vigencia_inicio IS NOT NULL AND data_vigencia_fim IS NOT NULL
    THEN data_vigencia_fim - data_vigencia_inicio
    ELSE NULL
  END AS duracao_vigencia_dias,
  
  -- Amounts
  valor_inicial,
  valor_global,
  valor_parcela,
  valor_acumulado,
  
  -- Supplier information
  ni_fornecedor,
  tipo_pessoa,
  nome_razao_social_fornecedor,
  ni_fornecedor_subcontratado,
  nome_fornecedor_subcontratado,
  tipo_pessoa_subcontratada,
  
  -- Contract details
  objeto_contrato,
  informacao_complementar,
  processo,
  numero_parcelas,
  numero_retificacao,
  receita,
  
  -- Additional identifiers
  codigo_pais_fornecedor,
  identificador_cipi,
  url_cipi,
  usuario_nome,
  
  -- Foreign keys
  org_key,
  unit_key,
  subrog_org_key,
  subrog_unit_key,
  
  -- Type information (extracted from JSON)
  tipo_contrato_json ->> 'id' AS tipo_contrato_id,
  tipo_contrato_json ->> 'nome' AS tipo_contrato_nome,
  categoria_processo_json ->> 'id' AS categoria_processo_id,
  categoria_processo_json ->> 'nome' AS categoria_processo_nome,
  
  -- Derived attributes
  CASE 
    WHEN tipo_pessoa = 'PJ' THEN 'Pessoa Jurídica'
    WHEN tipo_pessoa = 'PF' THEN 'Pessoa Física'
    WHEN tipo_pessoa = 'PE' THEN 'Pessoa Estrangeira'
    ELSE 'Outros'
  END AS tipo_pessoa_descricao,
  
  CASE 
    WHEN valor_global IS NOT NULL AND valor_global > 0 THEN
      CASE 
        WHEN valor_global <= 17600 THEN 'Até R$ 17.600'
        WHEN valor_global <= 88000 THEN 'R$ 17.601 a R$ 88.000'
        WHEN valor_global <= 176000 THEN 'R$ 88.001 a R$ 176.000'
        WHEN valor_global <= 1408000 THEN 'R$ 176.001 a R$ 1.408.000'
        WHEN valor_global <= 3300000 THEN 'R$ 1.408.001 a R$ 3.300.000'
        ELSE 'Acima de R$ 3.300.000'
      END
    ELSE 'Não informado'
  END AS faixa_valor_global,
  
  CASE 
    WHEN data_vigencia_inicio IS NOT NULL AND data_vigencia_fim IS NOT NULL THEN
      CASE 
        WHEN data_vigencia_fim - data_vigencia_inicio <= 90 THEN 'Até 90 dias'
        WHEN data_vigencia_fim - data_vigencia_inicio <= 365 THEN '91 a 365 dias'
        WHEN data_vigencia_fim - data_vigencia_inicio <= 730 THEN '1 a 2 anos'
        WHEN data_vigencia_fim - data_vigencia_inicio <= 1825 THEN '2 a 5 anos'
        ELSE 'Mais de 5 anos'
      END
    ELSE 'Não informado'
  END AS faixa_duracao_vigencia,
  
  -- Data quality flags
  CASE 
    WHEN numero_controle_pncp IS NULL THEN 'Número de controle ausente'
    WHEN valor_global IS NULL OR valor_global <= 0 THEN 'Valor global inválido'
    WHEN data_assinatura IS NULL THEN 'Data de assinatura ausente'
    WHEN ni_fornecedor IS NULL THEN 'NI do fornecedor ausente'
    ELSE 'OK'
  END AS quality_flag,
  
  -- Metadata
  endpoint_name,
  data_date,
  extracted_at,
  run_id,
  
  -- JSON fallback
  contract_json,
  
  -- Audit
  CURRENT_TIMESTAMP AS created_at,
  CURRENT_TIMESTAMP AS updated_at

FROM contracts_with_keys
ORDER BY numero_controle_pncp
</file>

<file path="dbt_baliza/models/marts/mart_procurement_analytics.sql">
{{
  config(
    materialized='table',
    description='Analytics mart combining procurement and contract data for business intelligence'
  )
}}

WITH procurement_summary AS (
  SELECT
    p.numero_controle_pncp,
    p.ano_compra,
    p.data_publicacao_pncp,
    p.modalidade_id,
    p.modalidade_nome,
    p.modalidade_descricao,
    p.valor_total_estimado,
    p.valor_total_homologado,
    p.faixa_valor_estimado,
    p.situacao_compra_id,
    p.situacao_compra_descricao,
    p.srp,
    p.existe_resultado,
    p.objeto_compra,
    p.org_key,
    p.unit_key,
    
    -- Organization info
    org.cnpj AS org_cnpj,
    org.razao_social AS org_razao_social,
    org.poder_nome AS org_poder,
    org.esfera_nome AS org_esfera,
    
    -- Unit info
    unit.nome_unidade AS unit_nome,
    unit.uf_sigla AS unit_uf,
    unit.regiao AS unit_regiao,
    unit.municipio_nome AS unit_municipio
    
  FROM {{ ref('fact_contratacoes') }} p
  LEFT JOIN {{ ref('dim_organizacoes') }} org
    ON p.org_key = org.org_key
  LEFT JOIN {{ ref('dim_unidades_orgao') }} unit
    ON p.unit_key = unit.unit_key
),

contract_summary AS (
  SELECT
    c.numero_controle_pncp_compra,
    COUNT(*) AS total_contratos,
    SUM(c.valor_global) AS valor_total_contratos,
    MIN(c.data_assinatura) AS primeira_assinatura,
    MAX(c.data_assinatura) AS ultima_assinatura,
    AVG(c.duracao_vigencia_dias) AS duracao_media_vigencia,
    COUNT(DISTINCT c.ni_fornecedor) AS fornecedores_distintos,
    
    -- Contract status flags
    SUM(CASE WHEN c.receita = true THEN 1 ELSE 0 END) AS contratos_com_receita,
    SUM(CASE WHEN c.numero_retificacao > 0 THEN 1 ELSE 0 END) AS contratos_retificados
    
  FROM {{ ref('fact_contratos') }} c
  WHERE c.numero_controle_pncp_compra IS NOT NULL
  GROUP BY c.numero_controle_pncp_compra
)

SELECT
  -- Procurement identifiers
  p.numero_controle_pncp,
  p.ano_compra,
  p.data_publicacao_pncp,
  
  -- Organization information
  p.org_cnpj,
  p.org_razao_social,
  p.org_poder,
  p.org_esfera,
  p.unit_nome,
  p.unit_uf,
  p.unit_regiao,
  p.unit_municipio,
  
  -- Procurement details
  p.modalidade_id,
  p.modalidade_nome,
  p.modalidade_descricao,
  p.objeto_compra,
  p.situacao_compra_id,
  p.situacao_compra_descricao,
  p.srp,
  p.existe_resultado,
  
  -- Financial information
  p.valor_total_estimado,
  p.valor_total_homologado,
  p.faixa_valor_estimado,
  
  -- Contract information
  COALESCE(c.total_contratos, 0) AS total_contratos,
  COALESCE(c.valor_total_contratos, 0) AS valor_total_contratos,
  c.primeira_assinatura,
  c.ultima_assinatura,
  c.duracao_media_vigencia,
  COALESCE(c.fornecedores_distintos, 0) AS fornecedores_distintos,
  COALESCE(c.contratos_com_receita, 0) AS contratos_com_receita,
  COALESCE(c.contratos_retificados, 0) AS contratos_retificados,
  
  -- Performance metrics
  CASE 
    WHEN p.valor_total_estimado > 0 AND c.valor_total_contratos > 0 THEN
      ROUND((c.valor_total_contratos / p.valor_total_estimado) * 100, 2)
    ELSE NULL
  END AS percentual_execucao_financeira,
  
  CASE 
    WHEN p.valor_total_homologado > 0 AND p.valor_total_estimado > 0 THEN
      ROUND((p.valor_total_homologado / p.valor_total_estimado) * 100, 2)
    ELSE NULL
  END AS percentual_economia_homologacao,
  
  -- Time metrics
  CASE 
    WHEN p.data_publicacao_pncp IS NOT NULL AND c.primeira_assinatura IS NOT NULL THEN
      c.primeira_assinatura - p.data_publicacao_pncp
    ELSE NULL
  END AS dias_publicacao_primeira_assinatura,
  
  -- Categories for analysis
  CASE 
    WHEN p.modalidade_id IN (6, 4) THEN 'Competitiva'
    WHEN p.modalidade_id IN (8, 9) THEN 'Não Competitiva'
    WHEN p.modalidade_id IN (1, 3, 10, 11, 12) THEN 'Outros'
    ELSE 'Não Classificada'
  END AS categoria_modalidade,
  
  CASE 
    WHEN p.org_esfera = 'Federal' THEN 'Federal'
    WHEN p.org_esfera = 'Estadual' THEN 'Estadual'
    WHEN p.org_esfera = 'Municipal' THEN 'Municipal'
    ELSE 'Outros'
  END AS categoria_esfera,
  
  CASE 
    WHEN p.org_poder = 'Executivo' THEN 'Executivo'
    WHEN p.org_poder = 'Legislativo' THEN 'Legislativo'
    WHEN p.org_poder = 'Judiciário' THEN 'Judiciário'
    WHEN p.org_poder = 'Ministério Público' THEN 'Ministério Público'
    ELSE 'Outros'
  END AS categoria_poder,
  
  -- Quality indicators
  CASE 
    WHEN p.existe_resultado = true AND COALESCE(c.total_contratos, 0) = 0 THEN 'Resultado sem contratos'
    WHEN p.existe_resultado = false AND COALESCE(c.total_contratos, 0) > 0 THEN 'Contratos sem resultado'
    WHEN p.valor_total_estimado > 0 AND p.valor_total_homologado > p.valor_total_estimado THEN 'Homologação acima do estimado'
    ELSE 'OK'
  END AS indicador_qualidade,
  
  -- Metadata
  CURRENT_TIMESTAMP AS created_at

FROM procurement_summary p
LEFT JOIN contract_summary c
  ON p.numero_controle_pncp = c.numero_controle_pncp_compra

ORDER BY p.ano_compra DESC, p.data_publicacao_pncp DESC
</file>

<file path="dbt_baliza/models/staging/stg_atas_raw.sql">
{{
  config(
    materialized='view',
    description='Staged raw price registration records (atas) data from PNCP API responses'
  )
}}

WITH raw_responses AS (
  SELECT *
  FROM {{ source('psa', 'pncp_raw_responses') }}
  WHERE endpoint_name IN ('atas_periodo', 'atas_atualizacao')
    AND response_code = 200
    AND response_content IS NOT NULL
    AND response_content != ''
),

parsed_responses AS (
  SELECT
    id,
    extracted_at,
    endpoint_name,
    endpoint_url,
    data_date,
    run_id,
    total_records,
    total_pages,
    current_page,
    -- Parse the JSON response content
    TRY_CAST(response_content AS JSON) AS response_json
  FROM raw_responses
  WHERE TRY_CAST(response_content AS JSON) IS NOT NULL
),

-- Extract individual ata records from the data array
ata_records AS (
  SELECT
    parsed_responses.id AS response_id,
    parsed_responses.extracted_at,
    parsed_responses.endpoint_name,
    parsed_responses.endpoint_url,
    parsed_responses.data_date,
    parsed_responses.run_id,
    parsed_responses.total_records,
    parsed_responses.total_pages,
    parsed_responses.current_page,
    -- Generate a unique key for each ata record
    ROW_NUMBER() OVER (PARTITION BY parsed_responses.id ORDER BY ata_data_table.value) AS record_index,
    -- Extract individual ata data
    ata_data_table.value AS ata_data
  FROM parsed_responses
  CROSS JOIN json_each(json_extract(parsed_responses.response_json, '$.data')) AS ata_data_table
  WHERE json_extract(parsed_responses.response_json, '$.data') IS NOT NULL
)

SELECT
  response_id,
  extracted_at,
  endpoint_name,
  endpoint_url,
  data_date,
  run_id,
  total_records,
  total_pages,
  current_page,
  record_index,
  
  -- Ata identifiers
  ata_data ->> 'numeroControlePNCPAta' AS numero_controle_pncp_ata,
  ata_data ->> 'numeroAtaRegistroPreco' AS numero_ata_registro_preco,
  CAST(ata_data ->> 'anoAta' AS INTEGER) AS ano_ata,
  ata_data ->> 'numeroControlePNCPCompra' AS numero_controle_pncp_compra,
  
  -- Dates
  TRY_CAST(ata_data ->> 'dataAssinatura' AS TIMESTAMP) AS data_assinatura,
  TRY_CAST(ata_data ->> 'vigenciaInicio' AS TIMESTAMP) AS vigencia_inicio,
  TRY_CAST(ata_data ->> 'vigenciaFim' AS TIMESTAMP) AS vigencia_fim,
  TRY_CAST(ata_data ->> 'dataPublicacaoPncp' AS TIMESTAMP) AS data_publicacao_pncp,
  TRY_CAST(ata_data ->> 'dataInclusao' AS TIMESTAMP) AS data_inclusao,
  TRY_CAST(ata_data ->> 'dataAtualizacao' AS TIMESTAMP) AS data_atualizacao,
  TRY_CAST(ata_data ->> 'dataAtualizacaoGlobal' AS TIMESTAMP) AS data_atualizacao_global,
  TRY_CAST(ata_data ->> 'dataCancelamento' AS TIMESTAMP) AS data_cancelamento,
  
  -- Status and flags
  CAST(ata_data ->> 'cancelado' AS BOOLEAN) AS cancelado,
  
  -- Ata details
  ata_data ->> 'objetoContratacao' AS objeto_contratacao,
  ata_data ->> 'usuario' AS usuario,
  
  -- Organization information (main organ)
  ata_data ->> 'cnpjOrgao' AS cnpj_orgao,
  ata_data ->> 'nomeOrgao' AS nome_orgao,
  ata_data ->> 'codigoUnidadeOrgao' AS codigo_unidade_orgao,
  ata_data ->> 'nomeUnidadeOrgao' AS nome_unidade_orgao,
  
  -- Subrogated organization information
  ata_data ->> 'cnpjOrgaoSubrogado' AS cnpj_orgao_subrogado,
  ata_data ->> 'nomeOrgaoSubrogado' AS nome_orgao_subrogado,
  ata_data ->> 'codigoUnidadeOrgaoSubrogado' AS codigo_unidade_orgao_subrogado,
  ata_data ->> 'nomeUnidadeOrgaoSubrogado' AS nome_unidade_orgao_subrogado,
  
  -- Full ata data as JSON for fallback
  ata_data AS ata_json

FROM ata_records
</file>

<file path="dbt_baliza/models/staging/stg_contratacoes_raw.sql">
{{
  config(
    materialized='view',
    description='Staged raw procurements data from PNCP API responses'
  )
}}

WITH raw_responses AS (
  SELECT *
  FROM {{ source('psa', 'pncp_raw_responses') }}
  WHERE endpoint_name IN ('contratacoes_publicacao', 'contratacoes_atualizacao', 'contratacoes_proposta')
    AND response_code = 200
    AND response_content IS NOT NULL
    AND response_content != ''
),

parsed_responses AS (
  SELECT
    id,
    extracted_at,
    endpoint_name,
    endpoint_url,
    data_date,
    run_id,
    modalidade,
    total_records,
    total_pages,
    current_page,
    -- Parse the JSON response content
    TRY_CAST(response_content AS JSON) AS response_json
  FROM raw_responses
  WHERE TRY_CAST(response_content AS JSON) IS NOT NULL
),

-- Extract individual procurement records from the data array
procurement_records AS (
  SELECT
    parsed_responses.id AS response_id,
    parsed_responses.extracted_at,
    parsed_responses.endpoint_name,
    parsed_responses.endpoint_url,
    parsed_responses.data_date,
    parsed_responses.run_id,
    parsed_responses.modalidade,
    parsed_responses.total_records,
    parsed_responses.total_pages,
    parsed_responses.current_page,
    -- Generate a unique key for each procurement record
    ROW_NUMBER() OVER (PARTITION BY parsed_responses.id ORDER BY procurement_data_table.value) AS record_index,
    -- Extract individual procurement data
    procurement_data_table.value AS procurement_data
  FROM parsed_responses
  CROSS JOIN json_each(json_extract(parsed_responses.response_json, '$.data')) AS procurement_data_table
  WHERE json_extract(parsed_responses.response_json, '$.data') IS NOT NULL
)

SELECT
  response_id,
  extracted_at,
  endpoint_name,
  endpoint_url,
  data_date,
  run_id,
  modalidade,
  total_records,
  total_pages,
  current_page,
  record_index,
  
  -- Procurement identifiers
  procurement_data ->> 'numeroControlePNCP' AS numero_controle_pncp,
  procurement_data ->> 'numeroCompra' AS numero_compra,
  CAST(procurement_data ->> 'anoCompra' AS INTEGER) AS ano_compra,
  CAST(procurement_data ->> 'sequencialCompra' AS INTEGER) AS sequencial_compra,
  
  -- Dates
  TRY_CAST(procurement_data ->> 'dataPublicacaoPncp' AS TIMESTAMP) AS data_publicacao_pncp,
  TRY_CAST(procurement_data ->> 'dataAberturaProposta' AS TIMESTAMP) AS data_abertura_proposta,
  TRY_CAST(procurement_data ->> 'dataEncerramentoProposta' AS TIMESTAMP) AS data_encerramento_proposta,
  TRY_CAST(procurement_data ->> 'dataInclusao' AS TIMESTAMP) AS data_inclusao,
  TRY_CAST(procurement_data ->> 'dataAtualizacao' AS TIMESTAMP) AS data_atualizacao,
  TRY_CAST(procurement_data ->> 'dataAtualizacaoGlobal' AS TIMESTAMP) AS data_atualizacao_global,
  
  -- Amounts
  CAST(procurement_data ->> 'valorTotalEstimado' AS DOUBLE) AS valor_total_estimado,
  CAST(procurement_data ->> 'valorTotalHomologado' AS DOUBLE) AS valor_total_homologado,
  
  -- Procurement details
  procurement_data ->> 'objetoCompra' AS objeto_compra,
  procurement_data ->> 'informacaoComplementar' AS informacao_complementar,
  procurement_data ->> 'processo' AS processo,
  procurement_data ->> 'linkSistemaOrigem' AS link_sistema_origem,
  procurement_data ->> 'linkProcessoEletronico' AS link_processo_eletronico,
  procurement_data ->> 'justificativaPresencial' AS justificativa_presencial,
  
  -- Procurement method and mode
  CAST(procurement_data ->> 'modalidadeId' AS INTEGER) AS modalidade_id,
  procurement_data ->> 'modalidadeNome' AS modalidade_nome,
  CAST(procurement_data ->> 'modoDisputaId' AS INTEGER) AS modo_disputa_id,
  procurement_data ->> 'modoDisputaNome' AS modo_disputa_nome,
  
  -- Instrument and framework
  CAST(procurement_data ->> 'tipoInstrumentoConvocatorioCodigo' AS INTEGER) AS tipo_instrumento_convocatorio_codigo,
  procurement_data ->> 'tipoInstrumentoConvocatorioNome' AS tipo_instrumento_convocatorio_nome,
  
  -- Status and flags
  procurement_data ->> 'situacaoCompraId' AS situacao_compra_id,
  procurement_data ->> 'situacaoCompraNome' AS situacao_compra_nome,
  CAST(procurement_data ->> 'srp' AS BOOLEAN) AS srp,
  CAST(procurement_data ->> 'existeResultado' AS BOOLEAN) AS existe_resultado,
  
  -- Organization data (nested JSON)
  procurement_data -> 'orgaoEntidade' AS orgao_entidade_json,
  procurement_data -> 'unidadeOrgao' AS unidade_orgao_json,
  procurement_data -> 'orgaoSubRogado' AS orgao_subrogado_json,
  procurement_data -> 'unidadeSubRogada' AS unidade_subrogada_json,
  procurement_data -> 'amparoLegal' AS amparo_legal_json,
  procurement_data -> 'fontesOrcamentarias' AS fontes_orcamentarias_json,
  
  -- User information
  procurement_data ->> 'usuarioNome' AS usuario_nome,
  
  -- Full procurement data as JSON for fallback
  procurement_data AS procurement_json

FROM procurement_records
</file>

<file path="dbt_baliza/models/staging/stg_contratos_raw.sql">
{{
  config(
    materialized='view',
    description='Staged raw contracts data from PNCP API responses'
  )
}}

WITH raw_responses AS (
  SELECT *
  FROM {{ source('psa', 'pncp_raw_responses') }}
  WHERE endpoint_name IN ('contratos_publicacao', 'contratos_atualizacao')
    AND response_code = 200
    AND response_content IS NOT NULL
    AND response_content != ''
),

parsed_responses AS (
  SELECT
    id,
    extracted_at,
    endpoint_name,
    endpoint_url,
    data_date,
    run_id,
    total_records,
    total_pages,
    current_page,
    -- Parse the JSON response content
    TRY_CAST(response_content AS JSON) AS response_json
  FROM raw_responses
  WHERE TRY_CAST(response_content AS JSON) IS NOT NULL
),

-- Extract individual contract records from the data array
contract_records AS (
  SELECT
    parsed_responses.id AS response_id,
    parsed_responses.extracted_at,
    parsed_responses.endpoint_name,
    parsed_responses.endpoint_url,
    parsed_responses.data_date,
    parsed_responses.run_id,
    parsed_responses.total_records,
    parsed_responses.total_pages,
    parsed_responses.current_page,
    -- Generate a unique key for each contract record
    ROW_NUMBER() OVER (PARTITION BY parsed_responses.id ORDER BY contract_data_table.value) AS record_index,
    -- Extract individual contract data
    contract_data_table.value AS contract_data
  FROM parsed_responses
  CROSS JOIN json_each(json_extract(parsed_responses.response_json, '$.data')) AS contract_data_table
  WHERE json_extract(parsed_responses.response_json, '$.data') IS NOT NULL
)

SELECT
  response_id,
  extracted_at,
  endpoint_name,
  endpoint_url,
  data_date,
  run_id,
  total_records,
  total_pages,
  current_page,
  record_index,
  
  -- Contract identifiers
  contract_data ->> 'numeroControlePNCP' AS numero_controle_pncp,
  contract_data ->> 'numeroControlePncpCompra' AS numero_controle_pncp_compra,
  contract_data ->> 'numeroContratoEmpenho' AS numero_contrato_empenho,
  CAST(contract_data ->> 'anoContrato' AS INTEGER) AS ano_contrato,
  CAST(contract_data ->> 'sequencialContrato' AS INTEGER) AS sequencial_contrato,
  
  -- Dates
  TRY_CAST(contract_data ->> 'dataAssinatura' AS DATE) AS data_assinatura,
  TRY_CAST(contract_data ->> 'dataVigenciaInicio' AS DATE) AS data_vigencia_inicio,
  TRY_CAST(contract_data ->> 'dataVigenciaFim' AS DATE) AS data_vigencia_fim,
  TRY_CAST(contract_data ->> 'dataPublicacaoPncp' AS TIMESTAMP) AS data_publicacao_pncp,
  TRY_CAST(contract_data ->> 'dataAtualizacao' AS TIMESTAMP) AS data_atualizacao,
  TRY_CAST(contract_data ->> 'dataAtualizacaoGlobal' AS TIMESTAMP) AS data_atualizacao_global,
  
  -- Amounts
  CAST(contract_data ->> 'valorInicial' AS DOUBLE) AS valor_inicial,
  CAST(contract_data ->> 'valorGlobal' AS DOUBLE) AS valor_global,
  CAST(contract_data ->> 'valorParcela' AS DOUBLE) AS valor_parcela,
  CAST(contract_data ->> 'valorAcumulado' AS DOUBLE) AS valor_acumulado,
  
  -- Supplier information
  contract_data ->> 'niFornecedor' AS ni_fornecedor,
  contract_data ->> 'tipoPessoa' AS tipo_pessoa,
  contract_data ->> 'nomeRazaoSocialFornecedor' AS nome_razao_social_fornecedor,
  contract_data ->> 'niFornecedorSubContratado' AS ni_fornecedor_subcontratado,
  contract_data ->> 'nomeFornecedorSubContratado' AS nome_fornecedor_subcontratado,
  contract_data ->> 'tipoPessoaSubContratada' AS tipo_pessoa_subcontratada,
  
  -- Contract details
  contract_data ->> 'objetoContrato' AS objeto_contrato,
  contract_data ->> 'informacaoComplementar' AS informacao_complementar,
  contract_data ->> 'processo' AS processo,
  CAST(contract_data ->> 'numeroParcelas' AS INTEGER) AS numero_parcelas,
  CAST(contract_data ->> 'numeroRetificacao' AS INTEGER) AS numero_retificacao,
  CAST(contract_data ->> 'receita' AS BOOLEAN) AS receita,
  
  -- Organization data (nested JSON)
  contract_data -> 'orgaoEntidade' AS orgao_entidade_json,
  contract_data -> 'unidadeOrgao' AS unidade_orgao_json,
  contract_data -> 'orgaoSubRogado' AS orgao_subrogado_json,
  contract_data -> 'unidadeSubRogada' AS unidade_subrogada_json,
  contract_data -> 'tipoContrato' AS tipo_contrato_json,
  contract_data -> 'categoriaProcesso' AS categoria_processo_json,
  
  -- Additional identifiers
  contract_data ->> 'codigoPaisFornecedor' AS codigo_pais_fornecedor,
  contract_data ->> 'identificadorCipi' AS identificador_cipi,
  contract_data ->> 'urlCipi' AS url_cipi,
  contract_data ->> 'usuarioNome' AS usuario_nome,
  
  -- Full contract data as JSON for fallback
  contract_data AS contract_json

FROM contract_records
</file>

<file path="dbt_baliza/models/sources.yml">
version: 2

sources:
  - name: psa
    description: "Persistent Staging Area - Raw PNCP API responses"
    schema: psa
    tables:
      - name: pncp_raw_responses
        description: "All raw PNCP API responses with complete metadata"
        columns:
          - name: id
            description: "Unique identifier for each response"
            data_type: UUID
            tests:
              - not_null
              - unique
          
          - name: extracted_at
            description: "Timestamp when the response was extracted"
            data_type: TIMESTAMP
            tests:
              - not_null
          
          - name: endpoint_url
            description: "Full URL that was called"
            data_type: VARCHAR
            tests:
              - not_null
          
          - name: endpoint_name
            description: "Name of the endpoint (e.g., contratos_publicacao)"
            data_type: VARCHAR
            tests:
              - not_null
              - accepted_values:
                  values: ['contratos_publicacao', 'contratos_atualizacao', 'contratacoes_publicacao', 'contratacoes_atualizacao', 'contratacoes_proposta', 'atas_periodo', 'atas_atualizacao', 'instrumentos_cobranca', 'pca_atualizacao']
          
          - name: http_method
            description: "HTTP method used (typically GET)"
            data_type: VARCHAR
            tests:
              - not_null
              - accepted_values:
                  values: ['GET', 'POST', 'PUT', 'DELETE']
          
          - name: request_parameters
            description: "JSON object containing request parameters"
            data_type: JSON
          
          - name: response_code
            description: "HTTP response status code"
            data_type: INTEGER
            tests:
              - not_null
              - accepted_values:
                  values: [200, 204, 400, 401, 422, 429, 500, 502, 503, 504]
          
          - name: response_content
            description: "Raw response content as text"
            data_type: VARCHAR
            tests:
              - not_null
          
          - name: response_headers
            description: "HTTP response headers as JSON"
            data_type: JSON
          
          - name: data_date
            description: "Date for which the data was extracted"
            data_type: DATE
            tests:
              - not_null
          
          - name: run_id
            description: "Unique identifier for extraction run"
            data_type: VARCHAR
            tests:
              - not_null
          
          - name: endpoint_type
            description: "Type of endpoint (contratos, contratacoes, atas, etc.)"
            data_type: VARCHAR
            tests:
              - not_null
          
          - name: modalidade
            description: "Modalidade code for contratacoes endpoints"
            data_type: INTEGER
          
          - name: total_records
            description: "Total number of records in the response"
            data_type: INTEGER
          
          - name: total_pages
            description: "Total number of pages for the query"
            data_type: INTEGER
          
          - name: current_page
            description: "Current page number"
            data_type: INTEGER
          
          - name: page_size
            description: "Number of records per page"
            data_type: INTEGER

        # Freshness tests
        freshness:
          warn_after: {count: 24, period: hour}
          error_after: {count: 48, period: hour}
          filter: response_code = 200

        # Loaded at tests
        loaded_at_field: extracted_at
</file>

<file path="docs/openapi/api-pncp-consulta.json">
{"openapi":"3.0.1","info":{"title":"API PNCP CONSULTA","description":"API REST de serviços do Portal Nacional de Contratações Públicas (PNCP)","contact":{"name":"Serviço Federal de Processamento de Dados - Serpro","url":"https://www.serpro.gov.br","email":"css.serpro@serpro.gov.br"},"version":"1.0"},"servers":[{"url":"/api/consulta"},{"url":"http://localhost:8080/pncp-consulta"}],"tags":[{"name":"Ata","description":"Consultas de Atas de Registro de Preços"},{"name":"Contratação","description":"Consultas de Contratações"},{"name":"Contrato/Empenho","description":"Consultas de Contratos/Empenhos"},{"name":"Instrumento de Cobrança de Contrato/Empenho","description":"Consultas de Instrumentos de Cobrança de Contratos/Empenhos"},{"name":"Contratação","description":"Manutenção de Contratações"},{"name":"Plano de Contratação","description":"Consultas de Planos de Contratações"}],"paths":{"/v1/pca/usuario":{"get":{"tags":["Plano de Contratação"],"summary":"Consultar Itens de PCA por Ano do PCA, IdUsuario e Código de Classificação Superior","operationId":"consultarItensPorUsuarioAno","parameters":[{"name":"anoPca","in":"query","required":true,"schema":{"type":"integer","format":"int32"}},{"name":"idUsuario","in":"query","required":true,"schema":{"type":"integer","format":"int64"}},{"name":"codigoClassificacaoSuperior","in":"query","required":false,"schema":{"type":"string"}},{"name":"cnpj","in":"query","required":false,"schema":{"type":"string"}},{"name":"pagina","in":"query","required":true,"schema":{"minimum":1,"type":"integer","format":"int32"}},{"name":"tamanhoPagina","in":"query","required":false,"schema":{"maximum":500,"minimum":10,"type":"integer","format":"int32"}}],"responses":{"400":{"description":"Bad Request","content":{"*/*":{"schema":{"oneOf":[{"type":"object","additionalProperties":{"type":"string"}},{"$ref":"#/components/schemas/RespostaErroValidacaoDTO"}]}}}},"422":{"description":"Unprocessable Entity","content":{"*/*":{"schema":{"type":"object","additionalProperties":{"type":"string"}}}}},"204":{"description":"No Content","content":{"*/*":{"schema":{"type":"object","additionalProperties":{"type":"string"}}}}},"500":{"description":"Internal Server Error","content":{"*/*":{"schema":{"type":"string"}}}},"401":{"description":"Unauthorized","content":{"*/*":{"schema":{"type":"string"}}}},"200":{"description":"OK","content":{"*/*":{"schema":{"$ref":"#/components/schemas/PaginaRetornoPlanoContratacaoComItensDoUsuarioDTO"}}}}}}},"/v1/pca/atualizacao":{"get":{"tags":["Plano de Contratação"],"summary":"Consultar PCA por Data de Atualização Global","operationId":"consultarItensPorUsuarioAno_1","parameters":[{"name":"dataInicio","in":"query","required":true,"schema":{"type":"string"}},{"name":"dataFim","in":"query","required":true,"schema":{"type":"string"}},{"name":"cnpj","in":"query","required":false,"schema":{"type":"string"}},{"name":"codigoUnidade","in":"query","required":false,"schema":{"type":"string"}},{"name":"pagina","in":"query","required":true,"schema":{"minimum":1,"type":"integer","format":"int32"}},{"name":"tamanhoPagina","in":"query","required":false,"schema":{"maximum":500,"minimum":10,"type":"integer","format":"int32"}}],"responses":{"400":{"description":"Bad Request","content":{"*/*":{"schema":{"oneOf":[{"type":"object","additionalProperties":{"type":"string"}},{"$ref":"#/components/schemas/RespostaErroValidacaoDTO"}]}}}},"422":{"description":"Unprocessable Entity","content":{"*/*":{"schema":{"type":"object","additionalProperties":{"type":"string"}}}}},"204":{"description":"No Content","content":{"*/*":{"schema":{"type":"object","additionalProperties":{"type":"string"}}}}},"500":{"description":"Internal Server Error","content":{"*/*":{"schema":{"type":"string"}}}},"401":{"description":"Unauthorized","content":{"*/*":{"schema":{"type":"string"}}}},"200":{"description":"OK","content":{"*/*":{"schema":{"$ref":"#/components/schemas/PaginaRetornoPlanoContratacaoComItensDoUsuarioDTO"}}}}}}},"/v1/pca/":{"get":{"tags":["Plano de Contratação"],"summary":"Consultar Itens de PCA por Ano do PCA e Código de Classificação Superior","operationId":"consultarItensPorAno","parameters":[{"name":"anoPca","in":"query","required":true,"schema":{"type":"integer","format":"int32"}},{"name":"codigoClassificacaoSuperior","in":"query","required":true,"schema":{"maxLength":100,"minLength":0,"type":"string"}},{"name":"pagina","in":"query","required":true,"schema":{"minimum":1,"type":"integer","format":"int32"}},{"name":"tamanhoPagina","in":"query","required":false,"schema":{"maximum":500,"minimum":10,"type":"integer","format":"int32"}}],"responses":{"400":{"description":"Bad Request","content":{"*/*":{"schema":{"oneOf":[{"type":"object","additionalProperties":{"type":"string"}},{"$ref":"#/components/schemas/RespostaErroValidacaoDTO"}]}}}},"422":{"description":"Unprocessable Entity","content":{"*/*":{"schema":{"type":"object","additionalProperties":{"type":"string"}}}}},"204":{"description":"No Content","content":{"*/*":{"schema":{"type":"object","additionalProperties":{"type":"string"}}}}},"500":{"description":"Internal Server Error","content":{"*/*":{"schema":{"type":"string"}}}},"401":{"description":"Unauthorized","content":{"*/*":{"schema":{"type":"string"}}}},"200":{"description":"OK","content":{"*/*":{"schema":{"$ref":"#/components/schemas/PaginaRetornoPlanoContratacaoComItensDoUsuarioDTO"}}}}}}},"/v1/orgaos/{cnpj}/compras/{ano}/{sequencial}":{"get":{"tags":["Contratação"],"summary":"Consultar Contratação","operationId":"consultarCompra","parameters":[{"name":"cnpj","in":"path","required":true,"schema":{"type":"string"}},{"name":"ano","in":"path","required":true,"schema":{"type":"integer","format":"int32"}},{"name":"sequencial","in":"path","required":true,"schema":{"minimum":1,"type":"integer","format":"int32"}}],"responses":{"400":{"description":"Bad Request","content":{"*/*":{"schema":{"oneOf":[{"type":"object","additionalProperties":{"type":"string"}},{"$ref":"#/components/schemas/RespostaErroValidacaoDTO"}]}}}},"422":{"description":"Unprocessable Entity","content":{"*/*":{"schema":{"type":"object","additionalProperties":{"type":"string"}}}}},"204":{"description":"No Content","content":{"*/*":{"schema":{"type":"object","additionalProperties":{"type":"string"}}}}},"500":{"description":"Internal Server Error","content":{"*/*":{"schema":{"type":"string"}}}},"401":{"description":"Unauthorized","content":{"*/*":{"schema":{"type":"string"}}}},"200":{"description":"OK","content":{"*/*":{"schema":{"$ref":"#/components/schemas/RecuperarCompraDTO"}}}}}}},"/v1/instrumentoscobranca/inclusao":{"get":{"tags":["Instrumento de Cobrança de Contrato/Empenho"],"summary":"Consultar Instrumentos de Cobrança por Data de Inclusão","operationId":"consultarInstrumentos","parameters":[{"name":"dataInicial","in":"query","required":true,"schema":{"type":"string"}},{"name":"dataFinal","in":"query","required":true,"schema":{"type":"string"}},{"name":"tipoInstrumentoCobranca","in":"query","required":false,"schema":{"type":"integer","format":"int64"}},{"name":"cnpjOrgao","in":"query","required":false,"schema":{"type":"string"}},{"name":"pagina","in":"query","required":true,"schema":{"minimum":1,"type":"integer","format":"int32"}},{"name":"tamanhoPagina","in":"query","required":false,"schema":{"maximum":100,"minimum":10,"type":"integer","format":"int32"}}],"responses":{"400":{"description":"Bad Request","content":{"*/*":{"schema":{"oneOf":[{"type":"object","additionalProperties":{"type":"string"}},{"$ref":"#/components/schemas/RespostaErroValidacaoDTO"}]}}}},"422":{"description":"Unprocessable Entity","content":{"*/*":{"schema":{"type":"object","additionalProperties":{"type":"string"}}}}},"204":{"description":"No Content","content":{"*/*":{"schema":{"type":"object","additionalProperties":{"type":"string"}}}}},"500":{"description":"Internal Server Error","content":{"*/*":{"schema":{"type":"string"}}}},"401":{"description":"Unauthorized","content":{"*/*":{"schema":{"type":"string"}}}},"200":{"description":"OK","content":{"*/*":{"schema":{"$ref":"#/components/schemas/PaginaRetornoConsultarInstrumentoCobrancaDTO"}}}}}}},"/v1/contratos":{"get":{"tags":["Contrato/Empenho"],"summary":"Consultar Contratos por Data de Publicação","operationId":"consultarContratosPorDataPublicacao","parameters":[{"name":"dataInicial","in":"query","required":true,"schema":{"type":"string"}},{"name":"dataFinal","in":"query","required":true,"schema":{"type":"string"}},{"name":"cnpjOrgao","in":"query","required":false,"schema":{"type":"string"}},{"name":"codigoUnidadeAdministrativa","in":"query","required":false,"schema":{"type":"string"}},{"name":"usuarioId","in":"query","required":false,"schema":{"type":"integer","format":"int64"}},{"name":"pagina","in":"query","required":true,"schema":{"minimum":1,"type":"integer","format":"int32"}},{"name":"tamanhoPagina","in":"query","required":false,"schema":{"maximum":500,"minimum":10,"type":"integer","format":"int32"}}],"responses":{"400":{"description":"Bad Request","content":{"*/*":{"schema":{"oneOf":[{"type":"object","additionalProperties":{"type":"string"}},{"$ref":"#/components/schemas/RespostaErroValidacaoDTO"}]}}}},"422":{"description":"Unprocessable Entity","content":{"*/*":{"schema":{"type":"object","additionalProperties":{"type":"string"}}}}},"204":{"description":"No Content","content":{"*/*":{"schema":{"type":"object","additionalProperties":{"type":"string"}}}}},"500":{"description":"Internal Server Error","content":{"*/*":{"schema":{"type":"string"}}}},"401":{"description":"Unauthorized","content":{"*/*":{"schema":{"type":"string"}}}},"200":{"description":"OK","content":{"*/*":{"schema":{"$ref":"#/components/schemas/PaginaRetornoRecuperarContratoDTO"}}}}}}},"/v1/contratos/atualizacao":{"get":{"tags":["Contrato/Empenho"],"summary":"Consultar Contratos/Empenhos por Data de Atualização Global","operationId":"consultarContratosPorDataAtualizacaoGlobal","parameters":[{"name":"dataInicial","in":"query","required":true,"schema":{"type":"string"}},{"name":"dataFinal","in":"query","required":true,"schema":{"type":"string"}},{"name":"cnpjOrgao","in":"query","required":false,"schema":{"type":"string"}},{"name":"codigoUnidadeAdministrativa","in":"query","required":false,"schema":{"type":"string"}},{"name":"usuarioId","in":"query","required":false,"schema":{"type":"integer","format":"int64"}},{"name":"pagina","in":"query","required":true,"schema":{"minimum":1,"type":"integer","format":"int32"}},{"name":"tamanhoPagina","in":"query","required":false,"schema":{"maximum":500,"minimum":10,"type":"integer","format":"int32"}}],"responses":{"400":{"description":"Bad Request","content":{"*/*":{"schema":{"oneOf":[{"type":"object","additionalProperties":{"type":"string"}},{"$ref":"#/components/schemas/RespostaErroValidacaoDTO"}]}}}},"422":{"description":"Unprocessable Entity","content":{"*/*":{"schema":{"type":"object","additionalProperties":{"type":"string"}}}}},"204":{"description":"No Content","content":{"*/*":{"schema":{"type":"object","additionalProperties":{"type":"string"}}}}},"500":{"description":"Internal Server Error","content":{"*/*":{"schema":{"type":"string"}}}},"401":{"description":"Unauthorized","content":{"*/*":{"schema":{"type":"string"}}}},"200":{"description":"OK","content":{"*/*":{"schema":{"$ref":"#/components/schemas/PaginaRetornoRecuperarContratoDTO"}}}}}}},"/v1/contratacoes/publicacao":{"get":{"tags":["Contratação"],"summary":"Consultar Contratações por Data de Publicação","operationId":"consultarContratacaoPorDataDePublicacao","parameters":[{"name":"dataInicial","in":"query","required":true,"schema":{"type":"string"}},{"name":"dataFinal","in":"query","required":true,"schema":{"type":"string"}},{"name":"codigoModalidadeContratacao","in":"query","required":true,"schema":{"type":"integer","format":"int64"}},{"name":"codigoModoDisputa","in":"query","required":false,"schema":{"type":"integer","format":"int32"}},{"name":"uf","in":"query","required":false,"schema":{"type":"string"}},{"name":"codigoMunicipioIbge","in":"query","required":false,"schema":{"type":"string"}},{"name":"cnpj","in":"query","required":false,"schema":{"type":"string"}},{"name":"codigoUnidadeAdministrativa","in":"query","required":false,"schema":{"type":"string"}},{"name":"idUsuario","in":"query","required":false,"schema":{"type":"integer","format":"int64"}},{"name":"pagina","in":"query","required":true,"schema":{"minimum":1,"type":"integer","format":"int32"}},{"name":"tamanhoPagina","in":"query","required":false,"schema":{"maximum":50,"minimum":10,"type":"integer","format":"int32"}}],"responses":{"400":{"description":"Bad Request","content":{"*/*":{"schema":{"oneOf":[{"type":"object","additionalProperties":{"type":"string"}},{"$ref":"#/components/schemas/RespostaErroValidacaoDTO"}]}}}},"422":{"description":"Unprocessable Entity","content":{"*/*":{"schema":{"type":"object","additionalProperties":{"type":"string"}}}}},"204":{"description":"No Content","content":{"*/*":{"schema":{"type":"object","additionalProperties":{"type":"string"}}}}},"500":{"description":"Internal Server Error","content":{"*/*":{"schema":{"type":"string"}}}},"401":{"description":"Unauthorized","content":{"*/*":{"schema":{"type":"string"}}}},"200":{"description":"OK","content":{"*/*":{"schema":{"$ref":"#/components/schemas/PaginaRetornoRecuperarCompraPublicacaoDTO"}}}}}}},"/v1/contratacoes/proposta":{"get":{"tags":["Contratação"],"summary":"Consultar Contratações com Recebimento de Propostas Aberto","operationId":"consultarContratacaoPeriodoRecebimentoPropostas","parameters":[{"name":"dataFinal","in":"query","required":true,"schema":{"type":"string"}},{"name":"codigoModalidadeContratacao","in":"query","required":false,"schema":{"type":"integer","format":"int64"}},{"name":"uf","in":"query","required":false,"schema":{"type":"string"}},{"name":"codigoMunicipioIbge","in":"query","required":false,"schema":{"type":"string"}},{"name":"cnpj","in":"query","required":false,"schema":{"type":"string"}},{"name":"codigoUnidadeAdministrativa","in":"query","required":false,"schema":{"maxLength":30,"minLength":1,"type":"string"}},{"name":"idUsuario","in":"query","required":false,"schema":{"type":"integer","format":"int64"}},{"name":"pagina","in":"query","required":true,"schema":{"minimum":1,"type":"integer","format":"int32"}},{"name":"tamanhoPagina","in":"query","required":false,"schema":{"maximum":50,"minimum":10,"type":"integer","format":"int32"}}],"responses":{"400":{"description":"Bad Request","content":{"*/*":{"schema":{"oneOf":[{"type":"object","additionalProperties":{"type":"string"}},{"$ref":"#/components/schemas/RespostaErroValidacaoDTO"}]}}}},"422":{"description":"Unprocessable Entity","content":{"*/*":{"schema":{"type":"object","additionalProperties":{"type":"string"}}}}},"204":{"description":"No Content","content":{"*/*":{"schema":{"type":"object","additionalProperties":{"type":"string"}}}}},"500":{"description":"Internal Server Error","content":{"*/*":{"schema":{"type":"string"}}}},"401":{"description":"Unauthorized","content":{"*/*":{"schema":{"type":"string"}}}},"200":{"description":"OK","content":{"*/*":{"schema":{"$ref":"#/components/schemas/PaginaRetornoRecuperarCompraPublicacaoDTO"}}}}}}},"/v1/contratacoes/atualizacao":{"get":{"tags":["Contratação"],"summary":"Consultar Contratações por Data de Atualização Global","operationId":"consultarContratacaoPorDataUltimaAtualizacao","parameters":[{"name":"dataInicial","in":"query","required":true,"schema":{"type":"string"}},{"name":"dataFinal","in":"query","required":true,"schema":{"type":"string"}},{"name":"codigoModalidadeContratacao","in":"query","required":true,"schema":{"type":"integer","format":"int64"}},{"name":"codigoModoDisputa","in":"query","required":false,"schema":{"type":"integer","format":"int32"}},{"name":"uf","in":"query","required":false,"schema":{"type":"string"}},{"name":"codigoMunicipioIbge","in":"query","required":false,"schema":{"type":"string"}},{"name":"cnpj","in":"query","required":false,"schema":{"type":"string"}},{"name":"codigoUnidadeAdministrativa","in":"query","required":false,"schema":{"type":"string"}},{"name":"idUsuario","in":"query","required":false,"schema":{"type":"integer","format":"int64"}},{"name":"pagina","in":"query","required":true,"schema":{"minimum":1,"type":"integer","format":"int32"}},{"name":"tamanhoPagina","in":"query","required":false,"schema":{"maximum":50,"minimum":10,"type":"integer","format":"int32"}}],"responses":{"400":{"description":"Bad Request","content":{"*/*":{"schema":{"oneOf":[{"type":"object","additionalProperties":{"type":"string"}},{"$ref":"#/components/schemas/RespostaErroValidacaoDTO"}]}}}},"422":{"description":"Unprocessable Entity","content":{"*/*":{"schema":{"type":"object","additionalProperties":{"type":"string"}}}}},"204":{"description":"No Content","content":{"*/*":{"schema":{"type":"object","additionalProperties":{"type":"string"}}}}},"500":{"description":"Internal Server Error","content":{"*/*":{"schema":{"type":"string"}}}},"401":{"description":"Unauthorized","content":{"*/*":{"schema":{"type":"string"}}}},"200":{"description":"OK","content":{"*/*":{"schema":{"$ref":"#/components/schemas/PaginaRetornoRecuperarCompraPublicacaoDTO"}}}}}}},"/v1/atas":{"get":{"tags":["Ata"],"summary":"Consultar Ata de Registro de Preço por Período de Vigência","operationId":"consultarAtaRegistroPrecoPeriodo","parameters":[{"name":"dataInicial","in":"query","required":true,"schema":{"type":"string"}},{"name":"dataFinal","in":"query","required":true,"schema":{"type":"string"}},{"name":"idUsuario","in":"query","required":false,"schema":{"type":"integer","format":"int64"}},{"name":"cnpj","in":"query","required":false,"schema":{"type":"string"}},{"name":"codigoUnidadeAdministrativa","in":"query","required":false,"schema":{"maxLength":30,"minLength":1,"type":"string"}},{"name":"pagina","in":"query","required":true,"schema":{"minimum":1,"type":"integer","format":"int32"}},{"name":"tamanhoPagina","in":"query","required":false,"schema":{"maximum":500,"minimum":10,"type":"integer","format":"int32"}}],"responses":{"400":{"description":"Bad Request","content":{"*/*":{"schema":{"oneOf":[{"type":"object","additionalProperties":{"type":"string"}},{"$ref":"#/components/schemas/RespostaErroValidacaoDTO"}]}}}},"422":{"description":"Unprocessable Entity","content":{"*/*":{"schema":{"type":"object","additionalProperties":{"type":"string"}}}}},"204":{"description":"No Content","content":{"*/*":{"schema":{"type":"object","additionalProperties":{"type":"string"}}}}},"500":{"description":"Internal Server Error","content":{"*/*":{"schema":{"type":"string"}}}},"401":{"description":"Unauthorized","content":{"*/*":{"schema":{"type":"string"}}}},"200":{"description":"OK","content":{"*/*":{"schema":{"$ref":"#/components/schemas/PaginaRetornoAtaRegistroPrecoPeriodoDTO"}}}}}}},"/v1/atas/atualizacao":{"get":{"tags":["Ata"],"summary":"Consultar Atas de Registro de Preço por Data de Atualização Global","operationId":"consultarAtaRegistroPrecoDataAtualizacao","parameters":[{"name":"dataInicial","in":"query","required":true,"schema":{"type":"string"}},{"name":"dataFinal","in":"query","required":true,"schema":{"type":"string"}},{"name":"idUsuario","in":"query","required":false,"schema":{"type":"integer","format":"int64"}},{"name":"cnpj","in":"query","required":false,"schema":{"type":"string"}},{"name":"codigoUnidadeAdministrativa","in":"query","required":false,"schema":{"maxLength":30,"minLength":1,"type":"string"}},{"name":"pagina","in":"query","required":true,"schema":{"minimum":1,"type":"integer","format":"int32"}},{"name":"tamanhoPagina","in":"query","required":false,"schema":{"maximum":500,"minimum":10,"type":"integer","format":"int32"}}],"responses":{"400":{"description":"Bad Request","content":{"*/*":{"schema":{"oneOf":[{"type":"object","additionalProperties":{"type":"string"}},{"$ref":"#/components/schemas/RespostaErroValidacaoDTO"}]}}}},"422":{"description":"Unprocessable Entity","content":{"*/*":{"schema":{"type":"object","additionalProperties":{"type":"string"}}}}},"204":{"description":"No Content","content":{"*/*":{"schema":{"type":"object","additionalProperties":{"type":"string"}}}}},"500":{"description":"Internal Server Error","content":{"*/*":{"schema":{"type":"string"}}}},"401":{"description":"Unauthorized","content":{"*/*":{"schema":{"type":"string"}}}},"200":{"description":"OK","content":{"*/*":{"schema":{"$ref":"#/components/schemas/PaginaRetornoAtaRegistroPrecoPeriodoDTO"}}}}}}}},"components":{"schemas":{"RespostaErroValidacaoDTO":{"type":"object","properties":{"message":{"type":"string"},"path":{"type":"string"},"timestamp":{"type":"string"},"status":{"type":"string"},"error":{"type":"string"}}},"PaginaRetornoPlanoContratacaoComItensDoUsuarioDTO":{"type":"object","properties":{"data":{"type":"array","items":{"$ref":"#/components/schemas/PlanoContratacaoComItensDoUsuarioDTO"}},"totalRegistros":{"type":"integer","format":"int64"},"totalPaginas":{"type":"integer","format":"int64"},"numeroPagina":{"type":"integer","format":"int64"},"paginasRestantes":{"type":"integer","format":"int64"},"empty":{"type":"boolean"}}},"PlanoContratacaoComItensDoUsuarioDTO":{"type":"object","properties":{"itens":{"type":"array","items":{"$ref":"#/components/schemas/PlanoContratacaoItemDTO"}},"codigoUnidade":{"type":"string"},"nomeUnidade":{"type":"string"},"anoPca":{"type":"integer","format":"int32"},"orgaoEntidadeRazaoSocial":{"type":"string"},"orgaoEntidadeCnpj":{"type":"string"},"dataPublicacaoPNCP":{"type":"string","format":"date-time","example":"2025-07-11T13:08:48"},"dataAtualizacaoGlobalPCA":{"type":"string","format":"date-time","example":"2025-07-11T13:08:48"},"idPcaPncp":{"type":"string"}}},"PlanoContratacaoItemDTO":{"type":"object","properties":{"nomeClassificacaoCatalogo":{"type":"string"},"quantidadeEstimada":{"type":"number"},"descricaoItem":{"type":"string"},"pdmCodigo":{"type":"string"},"dataInclusao":{"type":"string","format":"date-time","example":"2025-07-11T13:08:48"},"numeroItem":{"type":"integer","format":"int32"},"dataAtualizacao":{"type":"string","format":"date-time","example":"2025-07-11T13:08:48"},"valorTotal":{"type":"number"},"pdmDescricao":{"type":"string"},"codigoItem":{"type":"string"},"unidadeRequisitante":{"type":"string"},"grupoContratacaoCodigo":{"type":"string"},"grupoContratacaoNome":{"type":"string"},"classificacaoSuperiorCodigo":{"type":"string"},"classificacaoSuperiorNome":{"type":"string"},"unidadeFornecimento":{"type":"string"},"valorUnitario":{"type":"number"},"valorOrcamentoExercicio":{"type":"number"},"dataDesejada":{"type":"string","format":"date"},"classificacaoCatalogoId":{"type":"integer","format":"int64"},"categoriaItemPcaNome":{"type":"string"}}},"ContratacaoFonteOrcamentariaDTO":{"type":"object","properties":{"codigo":{"type":"integer","format":"int64"},"nome":{"type":"string"},"descricao":{"type":"string"},"dataInclusao":{"type":"string","format":"date-time","example":"2025-07-11T13:08:48"}}},"RecuperarAmparoLegalDTO":{"type":"object","properties":{"descricao":{"type":"string"},"nome":{"type":"string"},"codigo":{"type":"integer","format":"int64"}}},"RecuperarCompraDTO":{"type":"object","properties":{"valorTotalEstimado":{"type":"number"},"valorTotalHomologado":{"type":"number"},"indicadorOrcamentoSigiloso":{"type":"string","writeOnly":true,"enum":["COMPRA_SEM_SIGILO","COMPRA_PARCIALMENTE_SIGILOSA","COMPRA_TOTALMENTE_SIGILOSA"]},"orcamentoSigilosoCodigo":{"type":"integer","format":"int32"},"orcamentoSigilosoDescricao":{"type":"string"},"numeroControlePNCP":{"type":"string"},"linkSistemaOrigem":{"type":"string"},"linkProcessoEletronico":{"type":"string"},"anoCompra":{"type":"integer","format":"int32"},"sequencialCompra":{"type":"integer","format":"int32"},"numeroCompra":{"type":"string"},"processo":{"type":"string"},"orgaoEntidade":{"$ref":"#/components/schemas/RecuperarOrgaoEntidadeDTO"},"unidadeOrgao":{"$ref":"#/components/schemas/RecuperarUnidadeOrgaoDTO"},"orgaoSubRogado":{"$ref":"#/components/schemas/RecuperarOrgaoEntidadeDTO"},"unidadeSubRogada":{"$ref":"#/components/schemas/RecuperarUnidadeOrgaoDTO"},"modalidadeId":{"type":"integer","format":"int64"},"modalidadeNome":{"type":"string"},"justificativaPresencial":{"type":"string"},"modoDisputaId":{"type":"integer","format":"int64"},"modoDisputaNome":{"type":"string"},"tipoInstrumentoConvocatorioCodigo":{"type":"integer","format":"int64"},"tipoInstrumentoConvocatorioNome":{"type":"string"},"amparoLegal":{"$ref":"#/components/schemas/RecuperarAmparoLegalDTO"},"objetoCompra":{"type":"string"},"informacaoComplementar":{"type":"string"},"srp":{"type":"boolean"},"fontesOrcamentarias":{"type":"array","items":{"$ref":"#/components/schemas/ContratacaoFonteOrcamentariaDTO"}},"dataPublicacaoPncp":{"type":"string","format":"date-time","example":"2025-07-11T13:08:48"},"dataAberturaProposta":{"type":"string","format":"date-time","example":"2025-07-11T13:08:48"},"dataEncerramentoProposta":{"type":"string","format":"date-time","example":"2025-07-11T13:08:48"},"situacaoCompraId":{"type":"string","enum":["1","2","3","4"]},"situacaoCompraNome":{"type":"string"},"existeResultado":{"type":"boolean"},"dataInclusao":{"type":"string","format":"date-time","example":"2025-07-11T13:08:48"},"dataAtualizacao":{"type":"string","format":"date-time","example":"2025-07-11T13:08:48"},"dataAtualizacaoGlobal":{"type":"string","format":"date-time","example":"2025-07-11T13:08:48"},"usuarioNome":{"type":"string"}}},"RecuperarOrgaoEntidadeDTO":{"type":"object","properties":{"cnpj":{"type":"string"},"razaoSocial":{"type":"string"},"poderId":{"type":"string"},"esferaId":{"type":"string"}}},"RecuperarUnidadeOrgaoDTO":{"type":"object","properties":{"ufNome":{"type":"string"},"codigoUnidade":{"type":"string"},"nomeUnidade":{"type":"string"},"ufSigla":{"type":"string"},"municipioNome":{"type":"string"},"codigoIbge":{"type":"string"}}},"Categoria":{"type":"object","properties":{"id":{"type":"integer","format":"int64"},"nome":{"type":"string"}}},"ConsultarInstrumentoCobrancaDTO":{"type":"object","properties":{"cnpj":{"type":"string"},"ano":{"type":"integer","format":"int32"},"sequencialContrato":{"type":"integer","format":"int32"},"sequencialInstrumentoCobranca":{"type":"integer","format":"int32"},"tipoInstrumentoCobranca":{"$ref":"#/components/schemas/TipoInstrumentoCobrancaDTO"},"numeroInstrumentoCobranca":{"type":"string"},"dataEmissaoDocumento":{"type":"string","format":"date"},"observacao":{"type":"string"},"chaveNFe":{"type":"string"},"fonteNFe":{"type":"integer","format":"int64"},"dataConsultaNFe":{"type":"string","format":"date-time","example":"2025-07-11T13:08:48"},"statusResponseNFe":{"type":"string"},"jsonResponseNFe":{"type":"string"},"notaFiscalEletronica":{"$ref":"#/components/schemas/NotaFiscalEletronicaConsultaDTO"},"dataInclusao":{"type":"string","format":"date-time","example":"2025-07-11T13:08:48"},"dataAtualizacao":{"type":"string","format":"date-time","example":"2025-07-11T13:08:48"},"recuperarContratoDTO":{"$ref":"#/components/schemas/RecuperarContratoDTO"}}},"EventoNotaFiscalConsultaDTO":{"type":"object","properties":{"dataEvento":{"type":"string"},"tipoEvento":{"type":"string"},"evento":{"type":"string"},"motivoEvento":{"type":"string"}}},"ItemNotaFiscalConsultaDTO":{"type":"object","properties":{"numeroItem":{"type":"string"},"descricaoProdutoServico":{"type":"string"},"codigoNCM":{"type":"string"},"descricaoNCM":{"type":"string"},"cfop":{"type":"string"},"quantidade":{"type":"string"},"unidade":{"type":"string"},"valorUnitario":{"type":"string"},"valorTotal":{"type":"string"}}},"NotaFiscalEletronicaConsultaDTO":{"type":"object","properties":{"instrumentoCobrancaId":{"type":"integer","format":"int64"},"chave":{"type":"string"},"nfTransparenciaID":{"type":"integer","format":"int64"},"numero":{"type":"integer","format":"int64"},"serie":{"type":"integer","format":"int32"},"dataEmissao":{"type":"string"},"niEmitente":{"type":"string"},"nomeEmitente":{"type":"string"},"nomeMunicipioEmitente":{"type":"string"},"codigoOrgaoDestinatario":{"type":"string"},"nomeOrgaoDestinatario":{"type":"string"},"codigoOrgaoSuperiorDestinatario":{"type":"string"},"nomeOrgaoSuperiorDestinatario":{"type":"string"},"valorNotaFiscal":{"type":"string"},"tipoEventoMaisRecente":{"type":"string"},"dataTipoEventoMaisRecente":{"type":"string"},"dataInclusao":{"type":"string","format":"date-time","example":"2025-07-11T13:08:48"},"dataAtualizacao":{"type":"string","format":"date-time","example":"2025-07-11T13:08:48"},"itens":{"type":"array","items":{"$ref":"#/components/schemas/ItemNotaFiscalConsultaDTO"}},"eventos":{"type":"array","items":{"$ref":"#/components/schemas/EventoNotaFiscalConsultaDTO"}}}},"PaginaRetornoConsultarInstrumentoCobrancaDTO":{"type":"object","properties":{"data":{"type":"array","items":{"$ref":"#/components/schemas/ConsultarInstrumentoCobrancaDTO"}},"totalRegistros":{"type":"integer","format":"int64"},"totalPaginas":{"type":"integer","format":"int64"},"numeroPagina":{"type":"integer","format":"int64"},"paginasRestantes":{"type":"integer","format":"int64"},"empty":{"type":"boolean"}}},"RecuperarContratoDTO":{"type":"object","properties":{"numeroControlePncpCompra":{"type":"string"},"codigoPaisFornecedor":{"type":"string"},"anoContrato":{"type":"integer","format":"int32"},"tipoContrato":{"$ref":"#/components/schemas/TipoContrato"},"numeroContratoEmpenho":{"type":"string"},"dataAssinatura":{"type":"string","format":"date"},"dataVigenciaInicio":{"type":"string","format":"date"},"dataVigenciaFim":{"type":"string","format":"date"},"niFornecedor":{"type":"string"},"tipoPessoa":{"type":"string","enum":["PJ","PF","PE"]},"orgaoEntidade":{"$ref":"#/components/schemas/RecuperarOrgaoEntidadeDTO"},"categoriaProcesso":{"$ref":"#/components/schemas/Categoria"},"dataPublicacaoPncp":{"type":"string","format":"date-time","example":"2025-07-11T13:08:48"},"dataAtualizacao":{"type":"string","format":"date-time","example":"2025-07-11T13:08:48"},"sequencialContrato":{"type":"integer","format":"int32"},"unidadeOrgao":{"$ref":"#/components/schemas/RecuperarUnidadeOrgaoDTO"},"informacaoComplementar":{"type":"string"},"processo":{"type":"string"},"unidadeSubRogada":{"$ref":"#/components/schemas/RecuperarUnidadeOrgaoDTO"},"orgaoSubRogado":{"$ref":"#/components/schemas/RecuperarOrgaoEntidadeDTO"},"nomeRazaoSocialFornecedor":{"type":"string"},"niFornecedorSubContratado":{"type":"string"},"nomeFornecedorSubContratado":{"type":"string"},"numeroControlePNCP":{"type":"string"},"receita":{"type":"boolean"},"numeroParcelas":{"type":"integer","format":"int32"},"numeroRetificacao":{"type":"integer","format":"int32"},"tipoPessoaSubContratada":{"type":"string","enum":["PJ","PF","PE"]},"objetoContrato":{"type":"string"},"valorInicial":{"type":"number"},"valorParcela":{"type":"number"},"valorGlobal":{"type":"number"},"valorAcumulado":{"type":"number"},"dataAtualizacaoGlobal":{"type":"string","format":"date-time","example":"2025-07-11T13:08:48"},"identificadorCipi":{"type":"string"},"urlCipi":{"type":"string"},"usuarioNome":{"type":"string"}}},"TipoContrato":{"type":"object","properties":{"id":{"type":"integer","format":"int64"},"nome":{"type":"string"}}},"TipoInstrumentoCobrancaDTO":{"type":"object","properties":{"id":{"type":"integer","format":"int64"},"nome":{"type":"string"},"descricao":{"type":"string"},"dataInclusao":{"type":"string","format":"date-time","example":"2025-07-11T13:08:48"},"dataAtualizacao":{"type":"string","format":"date-time","example":"2025-07-11T13:08:48"},"statusAtivo":{"type":"boolean"}}},"PaginaRetornoRecuperarContratoDTO":{"type":"object","properties":{"data":{"type":"array","items":{"$ref":"#/components/schemas/RecuperarContratoDTO"}},"totalRegistros":{"type":"integer","format":"int64"},"totalPaginas":{"type":"integer","format":"int64"},"numeroPagina":{"type":"integer","format":"int64"},"paginasRestantes":{"type":"integer","format":"int64"},"empty":{"type":"boolean"}}},"PaginaRetornoRecuperarCompraPublicacaoDTO":{"type":"object","properties":{"data":{"type":"array","items":{"$ref":"#/components/schemas/RecuperarCompraPublicacaoDTO"}},"totalRegistros":{"type":"integer","format":"int64"},"totalPaginas":{"type":"integer","format":"int64"},"numeroPagina":{"type":"integer","format":"int64"},"paginasRestantes":{"type":"integer","format":"int64"},"empty":{"type":"boolean"}}},"RecuperarCompraPublicacaoDTO":{"type":"object","properties":{"srp":{"type":"boolean"},"orgaoEntidade":{"$ref":"#/components/schemas/RecuperarOrgaoEntidadeDTO"},"anoCompra":{"type":"integer","format":"int32"},"sequencialCompra":{"type":"integer","format":"int32"},"dataInclusao":{"type":"string","format":"date-time","example":"2025-07-11T13:08:48"},"dataPublicacaoPncp":{"type":"string","format":"date-time","example":"2025-07-11T13:08:48"},"dataAtualizacao":{"type":"string","format":"date-time","example":"2025-07-11T13:08:48"},"numeroCompra":{"type":"string"},"unidadeOrgao":{"$ref":"#/components/schemas/RecuperarUnidadeOrgaoDTO"},"amparoLegal":{"$ref":"#/components/schemas/RecuperarAmparoLegalDTO"},"dataAberturaProposta":{"type":"string","format":"date-time","example":"2025-07-11T13:08:48"},"dataEncerramentoProposta":{"type":"string","format":"date-time","example":"2025-07-11T13:08:48"},"informacaoComplementar":{"type":"string"},"processo":{"type":"string"},"objetoCompra":{"type":"string"},"linkSistemaOrigem":{"type":"string"},"justificativaPresencial":{"type":"string"},"unidadeSubRogada":{"$ref":"#/components/schemas/RecuperarUnidadeOrgaoDTO"},"orgaoSubRogado":{"$ref":"#/components/schemas/RecuperarOrgaoEntidadeDTO"},"valorTotalHomologado":{"type":"number"},"linkProcessoEletronico":{"type":"string"},"numeroControlePNCP":{"type":"string"},"modalidadeId":{"type":"integer","format":"int64"},"dataAtualizacaoGlobal":{"type":"string","format":"date-time","example":"2025-07-11T13:08:48"},"modoDisputaId":{"type":"integer","format":"int64"},"valorTotalEstimado":{"type":"number"},"modalidadeNome":{"type":"string"},"modoDisputaNome":{"type":"string"},"tipoInstrumentoConvocatorioCodigo":{"type":"integer","format":"int64"},"tipoInstrumentoConvocatorioNome":{"type":"string"},"fontesOrcamentarias":{"type":"array","items":{"$ref":"#/components/schemas/ContratacaoFonteOrcamentariaDTO"}},"situacaoCompraId":{"type":"string","enum":["1","2","3","4"]},"situacaoCompraNome":{"type":"string"},"usuarioNome":{"type":"string"}}},"AtaRegistroPrecoPeriodoDTO":{"type":"object","properties":{"numeroControlePNCPAta":{"type":"string"},"numeroAtaRegistroPreco":{"type":"string"},"anoAta":{"type":"integer","format":"int32"},"numeroControlePNCPCompra":{"type":"string"},"cancelado":{"type":"boolean"},"dataCancelamento":{"type":"string","format":"date-time","example":"2025-07-11T13:08:48"},"dataAssinatura":{"type":"string","format":"date-time","example":"2025-07-11T13:08:48"},"vigenciaInicio":{"type":"string","format":"date-time","example":"2025-07-11T13:08:48"},"vigenciaFim":{"type":"string","format":"date-time","example":"2025-07-11T13:08:48"},"dataPublicacaoPncp":{"type":"string","format":"date-time","example":"2025-07-11T13:08:48"},"dataInclusao":{"type":"string","format":"date-time","example":"2025-07-11T13:08:48"},"dataAtualizacao":{"type":"string","format":"date-time","example":"2025-07-11T13:08:48"},"dataAtualizacaoGlobal":{"type":"string","format":"date-time","example":"2025-07-11T13:08:48"},"usuario":{"type":"string"},"objetoContratacao":{"type":"string"},"cnpjOrgao":{"type":"string"},"nomeOrgao":{"type":"string"},"cnpjOrgaoSubrogado":{"type":"string"},"nomeOrgaoSubrogado":{"type":"string"},"codigoUnidadeOrgao":{"type":"string"},"nomeUnidadeOrgao":{"type":"string"},"codigoUnidadeOrgaoSubrogado":{"type":"string"},"nomeUnidadeOrgaoSubrogado":{"type":"string"}}},"PaginaRetornoAtaRegistroPrecoPeriodoDTO":{"type":"object","properties":{"data":{"type":"array","items":{"$ref":"#/components/schemas/AtaRegistroPrecoPeriodoDTO"}},"totalRegistros":{"type":"integer","format":"int64"},"totalPaginas":{"type":"integer","format":"int64"},"numeroPagina":{"type":"integer","format":"int64"},"paginasRestantes":{"type":"integer","format":"int64"},"empty":{"type":"boolean"}}}},"securitySchemes":{"bearerAuth":{"type":"http","scheme":"bearer","bearerFormat":"JWT"}}}}
</file>

<file path="src/baliza/.gitignore">
__pycache__/*
build/
dist/
*.egg-info/
.pytest_cache/
*.pyc
# pyenv
.python-version

# Environments
.env
.venv

# mypy
.mypy_cache/
.dmypy.json
dmypy.json

# JetBrains
.idea/

/coverage.xml
/.coverage
</file>

<file path=".pre-commit-config.yaml">
# Pre-commit configuration for Baliza
# See https://pre-commit.com for more information
repos:
  # Ruff for linting and formatting (replaces black, isort, flake8, etc.)
  - repo: https://github.com/astral-sh/ruff-pre-commit
    rev: v0.12.3
    hooks:
      # Run the linter
      - id: ruff
        args: [--fix]
        types_or: [python, pyi]
      # Run the formatter  
      - id: ruff-format
        types_or: [python, pyi]

  # Built-in pre-commit hooks
  - repo: https://github.com/pre-commit/pre-commit-hooks
    rev: v4.6.0
    hooks:
      # General file checks
      - id: trailing-whitespace
        exclude: '\.md$'
      - id: end-of-file-fixer
        exclude: '\.md$'
      - id: check-yaml
        args: [--allow-multiple-documents]
      - id: check-toml
      - id: check-json
      - id: check-xml
      - id: check-added-large-files
        args: [--maxkb=1000]
      - id: check-case-conflict
      - id: check-merge-conflict
      - id: check-executables-have-shebangs
      - id: check-shebang-scripts-are-executable
      
      # Python-specific checks
      - id: check-ast
      - id: check-builtin-literals
      - id: check-docstring-first
      - id: debug-statements
      - id: name-tests-test
        args: [--pytest-test-first]

  # Security checks with bandit
  - repo: https://github.com/PyCQA/bandit
    rev: 1.7.9
    hooks:
      - id: bandit
        args: ['-c', 'pyproject.toml']
        additional_dependencies: ['bandit[toml]']
        exclude: '^tests/'

  # Type checking with mypy
  - repo: https://github.com/pre-commit/mirrors-mypy
    rev: v1.16.1
    hooks:
      - id: mypy
        additional_dependencies: 
          - types-requests
          - types-PyYAML
        exclude: '^(tests|scripts)/'

  # SQL formatting for DBT files
  - repo: https://github.com/sqlfluff/sqlfluff
    rev: 3.5.0
    hooks:
      - id: sqlfluff-lint
        files: '\.(sql)$'
        additional_dependencies: ['dbt-duckdb', 'sqlfluff-templater-dbt']
      - id: sqlfluff-fix
        files: '\.(sql)$'
        additional_dependencies: ['dbt-duckdb', 'sqlfluff-templater-dbt']

  # Jupyter notebook cleaning
  - repo: https://github.com/nbQA-dev/nbQA
    rev: 1.9.1
    hooks:
      - id: nbqa-ruff
        args: [--fix]
      - id: nbqa-ruff-format

  # YAML formatting
  - repo: https://github.com/pre-commit/mirrors-prettier
    rev: v4.0.0-alpha.8
    hooks:
      - id: prettier
        files: '\.(yaml|yml)$'

  # Dockerfile linting
  - repo: https://github.com/hadolint/hadolint
    rev: v2.12.0
    hooks:
      - id: hadolint-docker
        files: 'Dockerfile*'

  # Shell script linting
  - repo: https://github.com/shellcheck-py/shellcheck-py
    rev: v0.10.0.1
    hooks:
      - id: shellcheck
        files: '\.(sh|bash)$'

# Global configuration
default_language_version:
  python: python3.11

# Fail fast - stop running hooks after first failure
fail_fast: false

# Default stages to run hooks on
default_stages: [commit, push]

# Specific configurations for different stages
repos:
  # Only run expensive checks on push, not every commit
  - repo: local
    hooks:
      - id: pytest-check
        name: pytest-check
        entry: uv run pytest tests/ --maxfail=1 -q
        language: system
        stages: [push]
        pass_filenames: false
        always_run: true
      
      - id: coverage-check
        name: coverage-check
        entry: uv run pytest tests/ --cov=src --cov-report=term-missing --cov-fail-under=80
        language: system
        stages: [manual]
        pass_filenames: false
        always_run: true
</file>

<file path="LICENSE">
MIT License

Copyright (c) 2024 [Your Name or Organization Here]

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.
</file>

<file path="src/baliza/__init__.py">
"""
BALIZA: Backup Aberto de Licitações Zelando pelo Acesso
Simplified PNCP data extractor for Brazilian public procurement data
"""

__version__ = "0.2.0"
</file>

<file path="dbt_baliza/.user.yml">
id: 01c7ddc8-d648-4d68-b614-b6c286024e47
</file>

<file path="dbt_baliza/dbt_project.yml">
name: 'baliza'
version: '1.0.0'
config-version: 2

# This setting configures which "profile" dbt uses for this project.
profile: 'baliza'

# These configurations specify where dbt should look for different types of files.
model-paths: ["models"]
analysis-paths: ["analyses"]
test-paths: ["tests"]
seed-paths: ["seeds"]
macro-paths: ["macros"]
snapshot-paths: ["snapshots"]

clean-targets:
  - "target"
  - "dbt_packages"

# Configuring models
models:
  baliza:
    # Raw/staging models
    staging:
      +materialized: view
      +schema: staging
    
    # Dimension models
    dimensions:
      +materialized: table
      +schema: dimensions
    
    # Fact models
    facts:
      +materialized: table
      +schema: facts
    
    # Marts/analytics models
    marts:
      +materialized: table
      +schema: marts

# Variables
vars:
  # Database configuration
  database_name: 'baliza'
  psa_schema: 'psa'
  
  # Date range for processing (can be overridden at runtime)
  start_date: '2024-01-01'
  end_date: '2024-12-31'
  
  # Endpoint configurations
  endpoints:
    contratos_publicacao: 'contratos_publicacao'
    contratos_atualizacao: 'contratos_atualizacao'
    contratacoes_publicacao: 'contratacoes_publicacao'
    contratacoes_atualizacao: 'contratacoes_atualizacao'
    contratacoes_proposta: 'contratacoes_proposta'
    atas_periodo: 'atas_periodo'
    atas_atualizacao: 'atas_atualizacao'
    instrumentos_cobranca: 'instrumentos_cobranca'
    pca_atualizacao: 'pca_atualizacao'

# Tests
tests:
  +schema: tests

# Snapshots
snapshots:
  +schema: snapshots

# Seeds
seeds:
  +schema: seeds
</file>

<file path="tests/conftest.py">
import os
import sys
import tempfile
from pathlib import Path
import gc

import pytest
import duckdb # Import duckdb

# Add src to Python path for all tests
sys.path.insert(0, str(Path(__file__).parent.parent / "src"))


def pytest_configure(config):
    """Configure pytest with custom markers."""
    config.addinivalue_line(
        "markers", "slow: marks tests as slow (may take several seconds)"
    )
    config.addinivalue_line("markers", "integration: marks tests as integration tests")
    config.addinivalue_line("markers", "performance: marks tests as performance tests")
    config.addinivalue_line(
        "markers", "end_to_end: marks tests as complete end-to-end tests"
    )


def pytest_addoption(parser):
    """Add custom command line options."""
    parser.addoption(
        "--run-slow", action="store_true", default=False, help="run slow tests"
    )
    parser.addoption(
        "--run-integration",
        action="store_true",
        default=False,
        help="run integration tests",
    )
    parser.addoption(
        "--run-performance",
        action="store_true",
        default=False,
        help="run performance tests",
    )


def pytest_collection_modifyitems(config, items):
    """Modify test collection based on command line options."""
    if not config.getoption("--run-slow"):
        skip_slow = pytest.mark.skip(reason="need --run-slow option to run")
        for item in items:
            if "slow" in item.keywords:
                item.add_marker(skip_slow)

    if not config.getoption("--run-integration"):
        skip_integration = pytest.mark.skip(
            reason="need --run-integration option to run"
        )
        for item in items:
            if "integration" in item.keywords:
                item.add_marker(skip_integration)

    if not config.getoption("--run-performance"):
        skip_performance = pytest.mark.skip(
            reason="need --run-performance option to run"
        )
        for item in items:
            if "performance" in item.keywords:
                item.add_marker(skip_performance)


@pytest.fixture(scope="session")
def project_root():
    """Get the project root directory."""
    return Path(__file__).parent.parent


@pytest.fixture
def temp_baliza_workspace():
    """Create a temporary workspace that mimics the Baliza project structure."""
    with tempfile.TemporaryDirectory() as tmpdir:
        # Change to temporary directory
        original_cwd = os.getcwd()
        os.chdir(tmpdir)

        # Create project structure
        dirs_to_create = [
            "src/baliza",
            "state",
            "baliza_data",
            "dbt_baliza/models/coverage",
            "dbt_baliza/models/staging",
            "dbt_baliza/models/sources",
            "notebooks",
            ".github/workflows",
            "tests",
        ]

        for dir_path in dirs_to_create:
            os.makedirs(dir_path, exist_ok=True)

        yield tmpdir

        # Restore original directory
        os.chdir(original_cwd)

        # Ensure all DuckDB connections are closed and garbage collected
        gc.collect()


@pytest.fixture
def duckdb_conn():
    """Provides a DuckDB in-memory connection for testing."""
    conn = duckdb.connect(database=':memory:')
    yield conn
    conn.close()
    gc.collect()


@pytest.fixture
def mock_environment_variables():
    """Provide mock environment variables for testing."""
    return {
        "IA_ACCESS_KEY": "test_access_key",
        "IA_SECRET_KEY": "test_secret_key",
        "BALIZA_DATE": "2024-01-15",
    }


@pytest.fixture
def sample_pncp_data():
    """Provide sample PNCP data for testing."""
    return {
        "data": [
            {
                "numeroControlePncpCompra": "12345-2024-001",
                "anoContrato": 2024,
                "dataAssinatura": "20240115",
                "niFornecedor": "12345678000195",
                "nomeRazaoSocialFornecedor": "Empresa Teste LTDA",
                "objetoContrato": "Prestação de serviços de TI",
                "valorInicial": 50000.00,
                "valorGlobal": 50000.00,
                "orgaoEntidade": {
                    "razaoSocial": "Prefeitura Municipal",
                    "cnpj": "11111111000111",
                    "uf": "RO",
                },
                "tipoContrato": {"codigo": "1", "descricao": "Serviços"},
            },
            {
                "numeroControlePncpCompra": "12345-2024-002",
                "anoContrato": 2024,
                "dataAssinatura": "20240116",
                "niFornecedor": "98765432000123",
                "nomeRazaoSocialFornecedor": "Outra Empresa SA",
                "objetoContrato": "Fornecimento de materiais",
                "valorInicial": 75000.00,
                "valorGlobal": 75000.00,
                "orgaoEntidade": {
                    "razaoSocial": "Governo do Estado",
                    "cnpj": "22222222000222",
                    "uf": "RO",
                },
                "tipoContrato": {"codigo": "2", "descricao": "Materiais"},
            },
        ],
        "totalRegistros": 2,
        "totalPaginas": 1,
        "paginaAtual": 1,
    }


@pytest.fixture
def sample_ia_items():
    """Provide sample Internet Archive items for testing."""
    return [
        {
            "identifier": "pncp-contratos-2024-01-15",
            "parquet_urls": [
                "https://archive.org/download/pncp-contratos-2024-01-15/file.parquet"
            ],
            "data_date": "2024-01-15",
            "metadata": {
                "date": "2024-01-15",
                "title": "PNCP Contratos 2024-01-15",
                "collection": "opensource",
            },
        }
    ]
</file>

<file path="tests/README.md">
# 🧪 Baliza Test Suite

This directory contains tests for the simplified Baliza project.

## 📋 Current Structure

The project has been refactored to use a simplified architecture with a single script. Most tests have been removed as they were for the old complex architecture.

### ✅ **Remaining Files**
- `conftest.py` - Test configuration and fixtures
- `README.md` - This file

## 🚀 Testing the Simplified Script

Since the project now uses a single self-contained script, testing is primarily done through manual verification:

### **Manual Testing**
```bash
# Test basic functionality
uv run baliza stats

# Test data extraction
uv run baliza extract --start-date 2024-07-10 --end-date 2024-07-10

# Test script directly
uv run python src/baliza/pncp_extractor.py stats
```

### **What is Tested**
- ✅ **Script Execution**: Basic command-line interface
- ✅ **Database Operations**: PSA schema creation and data storage
- ✅ **API Connectivity**: PNCP endpoint access
- ✅ **Data Processing**: Response parsing and storage
- ✅ **Error Handling**: HTTP errors and rate limiting

## 🔧 Future Test Improvements

Potential areas for adding tests back:
1. **Unit Tests**: Test individual functions in `pncp_extractor.py`
2. **Integration Tests**: Test database operations
3. **API Tests**: Mock PNCP API responses
4. **Performance Tests**: Test with large datasets

## 📊 Quality Assurance

The simplified architecture relies on:
- **Self-contained script**: Reduces complexity and potential failure points
- **Raw data storage**: Preserves all API responses for future analysis
- **Unified schema**: Consistent data structure across all endpoints
- **Built-in error handling**: Graceful handling of API failures

## 🎯 Key Benefits of Simplified Architecture

1. **Reduced Test Complexity**: Single script is easier to test
2. **Better Maintainability**: Less code means fewer bugs
3. **Increased Reliability**: Fewer dependencies and moving parts
4. **Simplified Deployment**: Single script deployment

---

## 🔍 Manual Verification Checklist

To verify the system works correctly:

- [ ] `baliza stats` shows existing data
- [ ] `baliza extract` can extract new data  
- [ ] Database file is created at `data/baliza.duckdb`
- [ ] Raw responses are stored in `psa.pncp_raw_responses` table
- [ ] API rate limiting works (1 second delay between requests)
- [ ] All HTTP status codes are handled gracefully
- [ ] Progress bars and console output work correctly

For issues or suggestions, please open a GitHub issue.
</file>

<file path=".github/workflows/baliza_daily_run.yml">
name: Baliza Daily Data Fetch

on:
  workflow_dispatch: # Allows manual triggering
  schedule:
    # Runs at 02:15 BRT (America/Sao_Paulo time)
    # BRT is UTC-3. So, 02:15 BRT is 05:15 UTC.
    - cron: '15 5 * * *'

jobs:
  fetch_and_upload_pncp_data:
    runs-on: ubuntu-latest
    permissions:
      contents: read # Needed to checkout the repository

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Install uv
        run: |
          curl -LsSf https://astral.sh/uv/install.sh | sh
          echo "$HOME/.cargo/bin" >> $GITHUB_PATH
        shell: bash

      - name: Verify uv installation
        run: uv --version

      - name: Set up Python using uv
        run: |
          uv venv --python $(cat .python-version || echo "3.11") .venv # Create venv using .python-version or default
          echo "VIRTUAL_ENV=.venv" >> $GITHUB_ENV
          echo ".venv/bin" >> $GITHUB_PATH
        shell: bash

      - name: Install dependencies using uv
        run: |
          source $VIRTUAL_ENV/bin/activate
          uv sync --frozen-lockfile
        shell: bash

      - name: Calculate Yesterday's Date
        id: get_date
        run: |
          # Using Python to ensure correct date calculation, especially across month/year boundaries
          # GitHub Actions runners usually have Python pre-installed.
          yesterday=$(python -c "from datetime import date, timedelta; print(date.today() - timedelta(days=1))")
          echo "YESTERDAY_DATE=$yesterday" >> $GITHUB_OUTPUT
        shell: bash

      - name: Run Baliza script and capture output
        id: baliza_run
        env:
          IA_ACCESS_KEY: ${{ secrets.IA_ACCESS_KEY }}
          IA_SECRET_KEY: ${{ secrets.IA_SECRET_KEY }}
          BALIZA_DATE: ${{ steps.get_date.outputs.YESTERDAY_DATE }} # Using env var for date
          PYTHONIOENCODING: "utf-8" # Ensure UTF-8 for script output
        run: |
          source $VIRTUAL_ENV/bin/activate
          echo "Fetching data for date: $BALIZA_DATE"

          # Create a directory for logs if it doesn't exist
          mkdir -p baliza_run_logs

          # Execute the script and save its output.
          # The script outputs informational logs to stderr (via typer.echo(err=True))
          # and the final JSON summary to stdout.
          # We'll save stdout to a file for the summary.
          # The script's exit code will determine step success/failure.
          python baliza/src/baliza/main.py > baliza_run_logs/run_summary_${{ steps.get_date.outputs.YESTERDAY_DATE }}.json

          # Check if the script failed (non-zero exit code)
          if [ $? -ne 0 ]; then
            echo "Baliza script failed. See logs for details."
            exit 1
          fi
          echo "Baliza script completed."
        shell: bash

      - name: Upload run summary artifact
        if: always() # Run this step even if the previous one failed, to upload partial logs if any
        uses: actions/upload-artifact@v4
        with:
          name: baliza-run-summary-${{ steps.get_date.outputs.YESTERDAY_DATE }}
          path: baliza_run_logs/run_summary_${{ steps.get_date.outputs.YESTERDAY_DATE }}.json
          retention-days: 90 # Keep artifacts for 90 days

  build_coverage_data:
    needs: fetch_and_upload_pncp_data
    runs-on: ubuntu-latest
    permissions:
      contents: read

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python using uv
        run: |
          curl -LsSf https://astral.sh/uv/install.sh | sh
          echo "$HOME/.cargo/bin" >> $GITHUB_PATH
          uv venv --python $(cat .python-version || echo "3.11") .venv
          echo "VIRTUAL_ENV=.venv" >> $GITHUB_ENV
          echo ".venv/bin" >> $GITHUB_PATH
        shell: bash

      - name: Install dependencies
        run: |
          source $VIRTUAL_ENV/bin/activate
          uv sync --frozen-lockfile
          uv add dbt-duckdb dbt-core
        shell: bash

      - name: Generate CNPJ entities reference data
        run: |
          source $VIRTUAL_ENV/bin/activate
          python scripts/ingest_cnpj_entities.py
        shell: bash

      - name: Run DBT coverage analysis
        run: |
          source $VIRTUAL_ENV/bin/activate
          cd dbt_baliza
          dbt deps
          dbt seed --select ref_entidades_publicas
          dbt run --select coverage_temporal coverage_entidades
        shell: bash

      - name: Export coverage data for dashboard
        run: |
          source $VIRTUAL_ENV/bin/activate
          mkdir -p site/data
          
          # Export to JSON for web dashboard
          python -c "
          import duckdb
          import json
          
          conn = duckdb.connect('state/baliza_dbt.duckdb')
          
          # Export entities coverage
          entities = conn.execute('SELECT * FROM main.coverage_entidades').fetchall()
          columns = [desc[0] for desc in conn.description]
          entities_data = [dict(zip(columns, row)) for row in entities]
          
          with open('site/data/coverage_entidades.json', 'w') as f:
              json.dump(entities_data, f, default=str, ensure_ascii=False, indent=2)
          
          # Export temporal coverage (last 90 days for performance)
          temporal = conn.execute('''
              SELECT * FROM main.coverage_temporal 
              WHERE data_referencia >= CURRENT_DATE - INTERVAL '90 days'
              ORDER BY data_referencia DESC
          ''').fetchall()
          columns = [desc[0] for desc in conn.description]
          temporal_data = [dict(zip(columns, row)) for row in temporal]
          
          with open('site/data/coverage_temporal.json', 'w') as f:
              json.dump(temporal_data, f, default=str, ensure_ascii=False, indent=2)
              
          conn.close()
          print('Coverage data exported successfully')
          "
        shell: bash

      - name: Calculate coverage quality gate
        run: |
          source $VIRTUAL_ENV/bin/activate
          python -c "
          import duckdb
          
          conn = duckdb.connect('state/baliza_dbt.duckdb')
          
          # Calculate average temporal coverage (last 30 days)
          result = conn.execute('''
              SELECT 
                  COUNT(*) as total_days,
                  SUM(CASE WHEN tem_dados_baliza THEN 1 ELSE 0 END) as days_with_data,
                  ROUND(100.0 * SUM(CASE WHEN tem_dados_baliza THEN 1 ELSE 0 END) / COUNT(*), 2) as coverage_percent
              FROM main.coverage_temporal 
              WHERE data_referencia >= CURRENT_DATE - INTERVAL '30 days'
                AND dia_publicacao_esperado = true
                AND uf_sigla = 'TOTAL'
          ''').fetchone()
          
          total_days, days_with_data, coverage_percent = result
          
          print(f'Temporal Coverage Quality Gate:')
          print(f'  Days with data: {days_with_data}/{total_days}')
          print(f'  Coverage percentage: {coverage_percent}%')
          
          # Quality gate: require at least 70% coverage
          if coverage_percent < 70:
              print(f'❌ Coverage quality gate FAILED: {coverage_percent}% < 70%')
              exit(1)
          else:
              print(f'✅ Coverage quality gate PASSED: {coverage_percent}% >= 70%')
          
          conn.close()
          "
        shell: bash

      - name: Upload coverage data artifacts
        uses: actions/upload-artifact@v4
        with:
          name: coverage-data
          path: site/data/
          retention-days: 30

  deploy_pages:
    needs: [fetch_and_upload_pncp_data, build_coverage_data]
    runs-on: ubuntu-latest
    permissions:
      pages: write
      id-token: write
    environment:
      name: github-pages
      url: ${{ steps.deployment.outputs.page_url }}

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Download coverage data
        uses: actions/download-artifact@v4
        with:
          name: coverage-data
          path: site/data/

      - name: Set up Python using uv
        run: |
          curl -LsSf https://astral.sh/uv/install.sh | sh
          echo "$HOME/.cargo/bin" >> $GITHUB_PATH
          uv venv --python $(cat .python-version || echo "3.11") .venv
          echo "VIRTUAL_ENV=.venv" >> $GITHUB_ENV
          echo ".venv/bin" >> $GITHUB_PATH
        shell: bash

      - name: Install dependencies for stats generation
        run: |
          source $VIRTUAL_ENV/bin/activate
          uv sync --frozen-lockfile
        shell: bash

      - name: Collect statistics and generate HTML pages
        run: |
          source $VIRTUAL_ENV/bin/activate
          
          # Create docs directory structure
          mkdir -p docs
          
          # Copy coverage dashboard
          cp site/cobertura.html docs/
          cp -r site/data docs/ 2>/dev/null || true
          
          # Generate main stats page (if scripts exist)
          if [ -f "scripts/collect_stats.py" ]; then
            python scripts/collect_stats.py
          fi
          if [ -f "scripts/generate_stats_page.py" ]; then
            python scripts/generate_stats_page.py
          fi
          
          # Create index.html that links to coverage dashboard
          cat > docs/index.html << 'EOF'
          <!DOCTYPE html>
          <html lang="pt-BR">
          <head>
              <meta charset="UTF-8">
              <meta name="viewport" content="width=device-width, initial-scale=1.0">
              <title>BALIZA - Portal de Transparência PNCP</title>
              <style>
                  body { font-family: Arial, sans-serif; max-width: 800px; margin: 50px auto; padding: 20px; }
                  .header { text-align: center; margin-bottom: 40px; }
                  .card { background: #f8f9fa; padding: 20px; border-radius: 10px; margin: 20px 0; }
                  .button { display: inline-block; background: #007bff; color: white; padding: 12px 24px; 
                           text-decoration: none; border-radius: 6px; margin: 10px; }
                  .button:hover { background: #0056b3; }
              </style>
          </head>
          <body>
              <div class="header">
                  <h1>🚀 BALIZA</h1>
                  <p>Portal de Transparência e Análise do PNCP</p>
              </div>
              
              <div class="card">
                  <h2>📊 Dashboard de Cobertura PNCP</h2>
                  <p>Análise de cobertura temporal e entidades do Portal Nacional de Contratações Públicas.</p>
                  <a href="cobertura.html" class="button">Ver Dashboard de Cobertura</a>
              </div>
              
              <div class="card">
                  <h2>🎯 Sobre o BALIZA</h2>
                  <p>O BALIZA é um sistema de coleta, preservação e análise de dados do PNCP, 
                     focado em transparência e monitoramento de contratações públicas.</p>
                  <a href="https://github.com/franklinbaldo/baliza" class="button">Ver no GitHub</a>
              </div>
          </body>
          </html>
          EOF
        shell: bash

      - name: Setup Pages
        uses: actions/configure-pages@v4

      - name: Upload artifact
        uses: actions/upload-pages-artifact@v3
        with:
          path: './docs'

      - name: Deploy to GitHub Pages
        id: deployment
        uses: actions/deploy-pages@v4
</file>

<file path="dbt_baliza/profiles.yml">
baliza:
  target: dev
  outputs:
    dev:
      type: duckdb
      path: '../data/baliza.duckdb'
      threads: 4
      keepalives_idle: 0
      search_path: psa
    
    prod:
      type: duckdb
      path: '../data/baliza.duckdb'
      threads: 8
      keepalives_idle: 0
      search_path: psa
</file>

<file path="src/baliza/pncp_extractor.py">
#!/usr/bin/env python3
"""
Simple PNCP Data Extractor - Major Refactor Version
Simplified script that iterates through all available PNCP endpoints extracting all data
and stores it in a new PSA (Persistent Staging Area) with raw responses.
"""

import asyncio
import json
import uuid
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, Any, Optional, List
import time

import duckdb
import httpx
import typer
from rich.console import Console
from rich.table import Table
from rich.panel import Panel

console = Console()

# Configuration
PNCP_BASE_URL = "https://pncp.gov.br/api/consulta"
REQUEST_TIMEOUT = 30
RATE_LIMIT_DELAY = 1.0  # seconds between requests
USER_AGENT = "BALIZA/2.0 (Backup Aberto de Licitacoes)"

# Data directory
DATA_DIR = Path.cwd() / "data"
BALIZA_DB_PATH = DATA_DIR / "baliza.duckdb"

# All authentication-free endpoints from OpenAPI analysis
# Only keep endpoints that work reliably without 400 errors
PNCP_ENDPOINTS = [
    {
        "name": "contratos_publicacao",
        "path": "/v1/contratos",
        "description": "Contratos por Data de Publicação",
        "date_params": ["dataInicial", "dataFinal"],
        "required_params": ["pagina", "tamanhoPagina"],
        "optional_params": ["cnpjOrgao", "codigoUnidadeAdministrativa", "usuarioId"],
        "max_page_size": 500,
        "default_page_size": 500,
        "supports_date_range": True
    },
    {
        "name": "contratos_atualizacao", 
        "path": "/v1/contratos/atualizacao",
        "description": "Contratos por Data de Atualização Global",
        "date_params": ["dataInicial", "dataFinal"],
        "required_params": ["pagina", "tamanhoPagina"],
        "optional_params": ["cnpjOrgao", "codigoUnidadeAdministrativa", "usuarioId"],
        "max_page_size": 500,
        "default_page_size": 500,
        "supports_date_range": True
    },
    {
        "name": "atas_periodo",
        "path": "/v1/atas",
        "description": "Atas de Registro de Preço por Período de Vigência",
        "date_params": ["dataInicial", "dataFinal"],
        "required_params": ["pagina", "tamanhoPagina"],
        "optional_params": ["idUsuario", "cnpj", "codigoUnidadeAdministrativa"],
        "max_page_size": 500,
        "default_page_size": 500,
        "supports_date_range": True
    },
    {
        "name": "atas_atualizacao",
        "path": "/v1/atas/atualizacao",
        "description": "Atas por Data de Atualização Global",
        "date_params": ["dataInicial", "dataFinal"],
        "required_params": ["pagina", "tamanhoPagina"],
        "optional_params": ["idUsuario", "cnpj", "codigoUnidadeAdministrativa"],
        "max_page_size": 500,
        "default_page_size": 500,
        "supports_date_range": True
    }
]


class SimplePNCPExtractor:
    """Simplified PNCP data extractor that stores all responses as raw data."""
    
    def __init__(self, base_url: str = PNCP_BASE_URL):
        self.base_url = base_url
        self.client = httpx.Client(
            base_url=base_url,
            timeout=REQUEST_TIMEOUT,
            headers={"User-Agent": USER_AGENT}
        )
        # Add async client for concurrent requests
        self.async_client = None
        self.run_id = str(uuid.uuid4())
        
        # Request tracking for progress and metrics
        self.total_requests_made = 0
        self.total_pages_processed = 0
        self.total_pages_expected = 0
        self.extraction_start_time = None
        
        self._init_database()
        
    def _start_extraction_timer(self):
        """Start the extraction timer for RPS calculations."""
        self.extraction_start_time = time.time()
        self.total_requests_made = 0
        self.total_pages_processed = 0
        self.total_pages_expected = 0
        console.print("📊 [dim]Starting page and RPS tracking...[/dim]")
        
    def _get_rps(self) -> float:
        """Calculate current requests per second."""
        if self.extraction_start_time is None or self.total_requests_made == 0:
            return 0.0
        elapsed = time.time() - self.extraction_start_time
        return self.total_requests_made / elapsed if elapsed > 0 else 0.0
        
    def _get_progress_info(self) -> Dict[str, Any]:
        """Get current progress information."""
        return {
            "pages_processed": self.total_pages_processed,
            "pages_expected": self.total_pages_expected,
            "requests_made": self.total_requests_made,
            "rps": self._get_rps(),
            "progress_percent": (self.total_pages_processed / self.total_pages_expected * 100) if self.total_pages_expected > 0 else 0
        }
        
    def _init_database(self):
        """Initialize database with simplified PSA schema."""
        DATA_DIR.mkdir(parents=True, exist_ok=True)
        
        self.conn = duckdb.connect(str(BALIZA_DB_PATH))
        
        # Create PSA schema
        self.conn.execute("CREATE SCHEMA IF NOT EXISTS psa")
        
        # Create simplified raw responses table
        self.conn.execute("""
            CREATE TABLE IF NOT EXISTS psa.pncp_raw_responses (
                -- Response metadata
                id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
                extracted_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                
                -- Request information
                endpoint_url VARCHAR NOT NULL,
                endpoint_name VARCHAR NOT NULL,
                http_method VARCHAR DEFAULT 'GET',
                request_parameters JSON,
                
                -- Response information
                response_code INTEGER NOT NULL,
                response_content TEXT,
                response_headers JSON,
                
                -- Processing metadata
                data_date DATE,
                run_id VARCHAR,
                endpoint_type VARCHAR,
                modalidade INTEGER,
                
                -- Additional metadata
                total_records INTEGER,
                total_pages INTEGER,
                current_page INTEGER,
                page_size INTEGER
            )
        """)
        
        # Create indexes for performance
        self.conn.execute("CREATE INDEX IF NOT EXISTS idx_endpoint_url ON psa.pncp_raw_responses(endpoint_url)")
        self.conn.execute("CREATE INDEX IF NOT EXISTS idx_data_date ON psa.pncp_raw_responses(data_date)")
        self.conn.execute("CREATE INDEX IF NOT EXISTS idx_response_code ON psa.pncp_raw_responses(response_code)")
        self.conn.execute("CREATE INDEX IF NOT EXISTS idx_extracted_at ON psa.pncp_raw_responses(extracted_at)")
        self.conn.execute("CREATE INDEX IF NOT EXISTS idx_endpoint_name ON psa.pncp_raw_responses(endpoint_name)")
        
    def _format_date(self, date_obj: datetime) -> str:
        """Format date for PNCP API (YYYYMMDD)."""
        return date_obj.strftime("%Y%m%d")
    
    def _check_exact_url_exists(self, endpoint: Dict[str, Any], params: Dict[str, Any]) -> bool:
        """Check if an exact URL with these parameters already exists in PSA."""
        # Generate the exact URL that would be called
        endpoint_url = f"{self.base_url}{endpoint['path']}"
        
        # Check if we have this exact URL and parameters combination
        query = """
            SELECT COUNT(*) 
            FROM psa.pncp_raw_responses 
            WHERE endpoint_url = ? 
            AND json_extract(request_parameters, '$.pagina') = ?
            AND json_extract(request_parameters, '$.tamanhoPagina') = ?
        """
        
        query_params = [endpoint_url, str(params.get('pagina', 1)), str(params.get('tamanhoPagina', 500))]
        
        # Add date parameter checks
        for date_param in endpoint.get("date_params", []):
            if date_param in params:
                query += f" AND json_extract(request_parameters, '$.{date_param}') = ?"
                query_params.append(str(params[date_param]))
        
        # Add modalidade check if applicable
        if 'codigoModalidadeContratacao' in params:
            query += " AND json_extract(request_parameters, '$.codigoModalidadeContratacao') = ?"
            query_params.append(str(params['codigoModalidadeContratacao']))
        
        # Only consider successful responses
        query += " AND response_code = 200"
        
        result = self.conn.execute(query, query_params).fetchone()
        return (result[0] if result else 0) > 0

    def _check_existing_extraction_range(self, endpoint: Dict[str, Any], start_date: datetime, end_date: datetime, modalidade: Optional[int] = None) -> Dict[str, Any]:
        """Check if data for this endpoint/date-range/modalidade combination already exists."""
        # For date ranges, we need to check if we have any overlapping extractions
        # For simplicity, we'll only consider it "complete" if we have the exact same date range
        query = """
            SELECT 
                COUNT(*) as total_responses,
                COUNT(CASE WHEN response_code = 200 THEN 1 END) as success_responses,
                SUM(CASE WHEN response_code = 200 THEN total_records ELSE 0 END) as total_records,
                MAX(CASE WHEN response_code = 200 THEN total_pages ELSE 0 END) as max_total_pages,
                COUNT(DISTINCT CASE WHEN response_code = 200 THEN current_page END) as unique_pages_fetched
            FROM psa.pncp_raw_responses 
            WHERE endpoint_name = ? 
            AND request_parameters LIKE ?
        """
        
        # Create a pattern to match the date range in request_parameters JSON
        # This is a simplified approach - in production, you'd want more robust JSON querying
        date_pattern = f'%"dataInicial":"{self._format_date(start_date)}"%"dataFinal":"{self._format_date(end_date)}"%'
        if "dataInicio" in endpoint.get("date_params", []):
            date_pattern = f'%"dataInicio":"{self._format_date(start_date)}"%"dataFim":"{self._format_date(end_date)}"%'
        
        params = [endpoint["name"], date_pattern]
        
        # Add modalidade filter if applicable
        if modalidade is not None:
            query += " AND modalidade = ?"
            params.append(modalidade)
        else:
            query += " AND modalidade IS NULL"
        
        result = self.conn.execute(query, params).fetchone()
        
        return {
            "total_responses": result[0] or 0,
            "success_responses": result[1] or 0,
            "total_records": result[2] or 0,
            "max_total_pages": result[3] or 0,
            "unique_pages_fetched": result[4] or 0,
            "is_complete": False  # Will be determined by caller
        }

    def _check_existing_extraction(self, endpoint: Dict[str, Any], data_date: datetime, modalidade: Optional[int] = None) -> Dict[str, Any]:
        """Check if data for this endpoint/date/modalidade combination already exists."""
        # Build the query to check existing data
        query = """
            SELECT 
                COUNT(*) as total_responses,
                COUNT(CASE WHEN response_code = 200 THEN 1 END) as success_responses,
                SUM(CASE WHEN response_code = 200 THEN total_records ELSE 0 END) as total_records,
                MAX(CASE WHEN response_code = 200 THEN total_pages ELSE 0 END) as max_total_pages,
                COUNT(DISTINCT CASE WHEN response_code = 200 THEN current_page END) as unique_pages_fetched
            FROM psa.pncp_raw_responses 
            WHERE endpoint_name = ? 
            AND data_date = ?
        """
        params = [endpoint["name"], data_date.date()]
        
        # Add modalidade filter if applicable
        if modalidade is not None:
            query += " AND modalidade = ?"
            params.append(modalidade)
        else:
            query += " AND modalidade IS NULL"
        
        result = self.conn.execute(query, params).fetchone()
        
        return {
            "total_responses": result[0] or 0,
            "success_responses": result[1] or 0,
            "total_records": result[2] or 0,
            "max_total_pages": result[3] or 0,
            "unique_pages_fetched": result[4] or 0,
            "is_complete": False  # Will be determined by caller
        }
        
    async def _init_async_client(self):
        """Initialize async HTTP client."""
        if self.async_client is None:
            self.async_client = httpx.AsyncClient(
                base_url=self.base_url,
                timeout=REQUEST_TIMEOUT,
                headers={"User-Agent": USER_AGENT},
                limits=httpx.Limits(max_connections=10, max_keepalive_connections=5)
            )

    async def _make_request_async(self, endpoint: Dict[str, Any], params: Dict[str, Any]) -> Dict[str, Any]:
        """Make async HTTP request to PNCP API endpoint."""
        try:
            # Rate limiting with async sleep
            await asyncio.sleep(RATE_LIMIT_DELAY)
            
            if self.async_client is None:
                await self._init_async_client()
            
            # Track request
            self.total_requests_made += 1
            
            response = await self.async_client.get(endpoint["path"], params=params)
            
            # Track page processed if successful
            if response.status_code == 200:
                self.total_pages_processed += 1
            
            # Log error responses with full request details for debugging
            if response.status_code >= 400:
                full_url = f"{self.base_url}{endpoint['path']}"
                param_str = "&".join([f"{k}={v}" for k, v in params.items()])
                console.print(f"    ❌ [red]HTTP {response.status_code} - {endpoint['name']}[/red]")
                console.print(f"    🌐 [dim red]Full URL: {full_url}?{param_str}[/dim red]")
                console.print(f"    📋 [dim red]Parameters: {params}[/dim red]")
                if response.text and len(response.text) < 500:
                    console.print(f"    📄 [dim red]Response: {response.text[:200]}[/dim red]")
            
            # Parse response content if possible
            response_content = response.text
            total_records = 0
            total_pages = 0
            
            if response.status_code == 200:
                try:
                    json_data = response.json()
                    total_records = json_data.get("totalRegistros", 0)
                    total_pages = json_data.get("totalPaginas", 0)
                except Exception:
                    pass
            
            return {
                "status_code": response.status_code,
                "content": response_content,
                "headers": dict(response.headers),
                "total_records": total_records,
                "total_pages": total_pages
            }
            
        except Exception as e:
            full_url = f"{self.base_url}{endpoint['path']}"
            param_str = "&".join([f"{k}={v}" for k, v in params.items()])
            console.print(f"    💥 [red]Request Exception - {endpoint['name']}[/red]")
            console.print(f"    🌐 [dim red]Full URL: {full_url}?{param_str}[/dim red]")
            console.print(f"    📋 [dim red]Parameters: {params}[/dim red]")
            console.print(f"    ⚠️ [dim red]Error: {str(e)}[/dim red]")
            
            return {
                "status_code": 0,
                "content": f"Error: {str(e)}",
                "headers": {},
                "total_records": 0,
                "total_pages": 0
            }

    def _make_request(self, endpoint: Dict[str, Any], params: Dict[str, Any]) -> Dict[str, Any]:
        """Make HTTP request to PNCP API endpoint."""
        try:
            time.sleep(RATE_LIMIT_DELAY)  # Rate limiting
            
            # Track request
            self.total_requests_made += 1
            
            response = self.client.get(endpoint["path"], params=params)
            
            # Track page processed if successful
            if response.status_code == 200:
                self.total_pages_processed += 1
            
            # Log error responses with full request details for debugging
            if response.status_code >= 400:
                full_url = f"{self.base_url}{endpoint['path']}"
                param_str = "&".join([f"{k}={v}" for k, v in params.items()])
                console.print(f"    ❌ [red]HTTP {response.status_code} - {endpoint['name']}[/red]")
                console.print(f"    🌐 [dim red]Full URL: {full_url}?{param_str}[/dim red]")
                console.print(f"    📋 [dim red]Parameters: {params}[/dim red]")
                if response.text and len(response.text) < 500:
                    console.print(f"    📄 [dim red]Response: {response.text[:200]}[/dim red]")
            
            # Parse response content if possible
            response_content = response.text
            total_records = 0
            total_pages = 0
            
            if response.status_code == 200:
                try:
                    json_data = response.json()
                    total_records = json_data.get("totalRegistros", 0)
                    total_pages = json_data.get("totalPaginas", 0)
                except Exception:
                    pass
            
            return {
                "status_code": response.status_code,
                "content": response_content,
                "headers": dict(response.headers),
                "total_records": total_records,
                "total_pages": total_pages
            }
            
        except Exception as e:
            full_url = f"{self.base_url}{endpoint['path']}"
            param_str = "&".join([f"{k}={v}" for k, v in params.items()])
            console.print(f"    💥 [red]Request Exception - {endpoint['name']}[/red]")
            console.print(f"    🌐 [dim red]Full URL: {full_url}?{param_str}[/dim red]")
            console.print(f"    📋 [dim red]Parameters: {params}[/dim red]")
            console.print(f"    ⚠️ [dim red]Error: {str(e)}[/dim red]")
            
            return {
                "status_code": 0,
                "content": f"Error: {str(e)}",
                "headers": {},
                "total_records": 0,
                "total_pages": 0
            }
    
    def _store_response_range(self, endpoint: Dict[str, Any], params: Dict[str, Any], 
                            response_data: Dict[str, Any], start_date: datetime, end_date: datetime, modalidade: Optional[int] = None):  # noqa: ARG002
        """Store raw response in PSA with date range metadata."""
        endpoint_url = f"{self.base_url}{endpoint['path']}"
        
        # For date ranges, we'll use the start_date as the primary data_date 
        # but store the full range in request_parameters
        self.conn.execute("""
            INSERT INTO psa.pncp_raw_responses (
                endpoint_url, endpoint_name, http_method, request_parameters,
                response_code, response_content, response_headers,
                data_date, run_id, endpoint_type, modalidade,
                total_records, total_pages, current_page, page_size
            ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
        """, [
            endpoint_url,
            endpoint["name"],
            "GET",
            json.dumps(params),
            response_data["status_code"],
            response_data["content"],
            json.dumps(response_data["headers"]),
            start_date.date(),  # Use start_date as primary date
            self.run_id,
            endpoint["name"].split("_")[0],  # Extract type: contratos, contratacoes, atas, etc.
            modalidade,
            response_data["total_records"],
            response_data["total_pages"],
            params.get("pagina", 1),
            params.get("tamanhoPagina", endpoint["default_page_size"])
        ])

    def _store_response(self, endpoint: Dict[str, Any], params: Dict[str, Any], 
                       response_data: Dict[str, Any], data_date: datetime, modalidade: Optional[int] = None):
        """Store raw response in PSA."""
        endpoint_url = f"{self.base_url}{endpoint['path']}"
        
        self.conn.execute("""
            INSERT INTO psa.pncp_raw_responses (
                endpoint_url, endpoint_name, http_method, request_parameters,
                response_code, response_content, response_headers,
                data_date, run_id, endpoint_type, modalidade,
                total_records, total_pages, current_page, page_size
            ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
        """, [
            endpoint_url,
            endpoint["name"],
            "GET",
            json.dumps(params),
            response_data["status_code"],
            response_data["content"],
            json.dumps(response_data["headers"]),
            data_date.date(),
            self.run_id,
            endpoint["name"].split("_")[0],  # Extract type: contratos, contratacoes, atas, etc.
            modalidade,
            response_data["total_records"],
            response_data["total_pages"],
            params.get("pagina", 1),
            params.get("tamanhoPagina", endpoint["default_page_size"])
        ])
        
    async def extract_endpoint_date_range(self, endpoint: Dict[str, Any], start_date: datetime, end_date: datetime) -> Dict[str, Any]:
        """Extract all data from a single endpoint for a date range with concurrent modalidade processing."""
        results = {
            "endpoint": endpoint["name"],
            "start_date": start_date.date(),
            "end_date": end_date.date(),
            "total_requests": 0,
            "total_records": 0,
            "success_requests": 0,
            "error_requests": 0,
            "modalidades_processed": [],
            "skipped_count": 0,
            "resumed_count": 0
        }
        
        # Handle endpoints based on modalidade strategy
        if "modalidades" in endpoint:
            modalidade_strategy = endpoint.get("modalidade_strategy", "required_individual")
            
            if modalidade_strategy == "optional_unrestricted":
                # Try unrestricted extraction first (no modalidade parameter)
                console.print(f"    🔍 [blue]Trying unrestricted extraction for {endpoint['name']}[/blue]")
                unrestricted_results = await self._extract_endpoint_modalidade_range_async(endpoint, start_date, end_date, None)
                
                if unrestricted_results["success_requests"] > 0 and unrestricted_results["total_records"] > 0:
                    # Unrestricted extraction succeeded and has data
                    console.print(f"    ✅ [green]Unrestricted extraction successful: {unrestricted_results['total_records']:,} records[/green]")
                    results["total_requests"] += unrestricted_results["total_requests"]
                    results["total_records"] += unrestricted_results["total_records"]
                    results["success_requests"] += unrestricted_results["success_requests"]
                    results["error_requests"] += unrestricted_results["error_requests"]
                    
                    if unrestricted_results.get("skipped"):
                        results["skipped_count"] += 1
                    if unrestricted_results.get("resumed"):
                        results["resumed_count"] += 1
                        
                    results["modalidades_processed"].append({
                        "modalidade": "unrestricted",
                        "records": unrestricted_results["total_records"],
                        "skipped": unrestricted_results.get("skipped", False),
                        "resumed": unrestricted_results.get("resumed", False)
                    })
                else:
                    # Unrestricted failed, fallback to individual modalidades
                    console.print(f"    ⚠️ [yellow]Unrestricted extraction failed (success: {unrestricted_results['success_requests']}, records: {unrestricted_results['total_records']}, errors: {unrestricted_results['error_requests']}), falling back to individual modalidades[/yellow]")
                    await self._process_individual_modalidades(endpoint, start_date, end_date, results)
            else:
                # Default: process individual modalidades
                await self._process_individual_modalidades(endpoint, start_date, end_date, results)
        else:
            # Regular endpoint without modalidade
            modalidade_results = await self._extract_endpoint_modalidade_range_async(endpoint, start_date, end_date, None)
            results["total_requests"] += modalidade_results["total_requests"]
            results["total_records"] += modalidade_results["total_records"]
            results["success_requests"] += modalidade_results["success_requests"]
            results["error_requests"] += modalidade_results["error_requests"]
            
            if modalidade_results.get("skipped"):
                results["skipped_count"] += 1
            if modalidade_results.get("resumed"):
                results["resumed_count"] += 1
            
        return results

    async def _process_individual_modalidades(self, endpoint: Dict[str, Any], start_date: datetime, end_date: datetime, results: Dict[str, Any]):
        """Process modalidades individually with concurrent execution."""
        # Process modalidades concurrently
        modalidade_tasks = []
        for modalidade in endpoint["modalidades"]:
            task = self._extract_endpoint_modalidade_range_async(endpoint, start_date, end_date, modalidade)
            modalidade_tasks.append(task)
        
        # Wait for all modalidades to complete
        modalidade_results_list = await asyncio.gather(*modalidade_tasks, return_exceptions=True)
        
        # Process results
        for i, modalidade_results in enumerate(modalidade_results_list):
            modalidade = endpoint["modalidades"][i]
            
            # Handle exceptions
            if isinstance(modalidade_results, Exception):
                console.print(f"❌ Error processing modalidade {modalidade}: {modalidade_results}")
                results["error_requests"] += 1
                continue
            
            results["total_requests"] += modalidade_results["total_requests"]
            results["total_records"] += modalidade_results["total_records"]
            results["success_requests"] += modalidade_results["success_requests"]
            results["error_requests"] += modalidade_results["error_requests"]
            
            if modalidade_results.get("skipped"):
                results["skipped_count"] += 1
            if modalidade_results.get("resumed"):
                results["resumed_count"] += 1
                
            results["modalidades_processed"].append({
                "modalidade": modalidade,
                "records": modalidade_results["total_records"],
                "skipped": modalidade_results.get("skipped", False),
                "resumed": modalidade_results.get("resumed", False)
            })

    def extract_endpoint_data(self, endpoint: Dict[str, Any], data_date: datetime) -> Dict[str, Any]:
        """Extract all data from a single endpoint for a specific date."""
        results = {
            "endpoint": endpoint["name"],
            "date": data_date.date(),
            "total_requests": 0,
            "total_records": 0,
            "success_requests": 0,
            "error_requests": 0,
            "modalidades_processed": [],
            "skipped_count": 0,
            "resumed_count": 0
        }
        
        # Handle endpoints that require modalidade iteration
        if "modalidades" in endpoint:
            for modalidade in endpoint["modalidades"]:
                modalidade_results = self._extract_endpoint_modalidade(endpoint, data_date, modalidade)
                results["total_requests"] += modalidade_results["total_requests"]
                results["total_records"] += modalidade_results["total_records"]
                results["success_requests"] += modalidade_results["success_requests"]
                results["error_requests"] += modalidade_results["error_requests"]
                
                if modalidade_results.get("skipped"):
                    results["skipped_count"] += 1
                if modalidade_results.get("resumed"):
                    results["resumed_count"] += 1
                    
                results["modalidades_processed"].append({
                    "modalidade": modalidade,
                    "records": modalidade_results["total_records"],
                    "skipped": modalidade_results.get("skipped", False),
                    "resumed": modalidade_results.get("resumed", False)
                })
        else:
            # Regular endpoint without modalidade
            modalidade_results = self._extract_endpoint_modalidade(endpoint, data_date, None)
            results["total_requests"] += modalidade_results["total_requests"]
            results["total_records"] += modalidade_results["total_records"]
            results["success_requests"] += modalidade_results["success_requests"]
            results["error_requests"] += modalidade_results["error_requests"]
            
            if modalidade_results.get("skipped"):
                results["skipped_count"] += 1
            if modalidade_results.get("resumed"):
                results["resumed_count"] += 1
            
        return results

    def _extract_endpoint_modalidade_range(self, endpoint: Dict[str, Any], start_date: datetime, end_date: datetime, modalidade: Optional[int], force: bool = False) -> Dict[str, Any]:
        """Extract data from endpoint for a specific modalidade (or no modalidade) using date range."""
        results = {
            "total_requests": 0,
            "total_records": 0,
            "success_requests": 0,
            "error_requests": 0,
            "skipped": False,
            "resumed": False
        }
        
        # Check if we already have complete data for this combination
        existing = self._check_existing_extraction_range(endpoint, start_date, end_date, modalidade)
        
        # Build base parameters for date range
        params = {
            "pagina": 1,
            "tamanhoPagina": endpoint["default_page_size"]
        }
        
        # Add date parameters for the full range
        if endpoint["supports_date_range"]:
            if "dataInicio" in endpoint["date_params"]:
                params["dataInicio"] = self._format_date(start_date)
                params["dataFim"] = self._format_date(end_date)
            else:
                params["dataInicial"] = self._format_date(start_date)
                params["dataFinal"] = self._format_date(end_date)
        else:
            # For endpoints like proposta that only use dataFinal, use end_date
            params["dataFinal"] = self._format_date(end_date)
        
        # Add modalidade if required
        if modalidade is not None:
            params["codigoModalidadeContratacao"] = modalidade
        
        # Check if this exact date range extraction is complete (unless force=True)
        if not force and existing["success_responses"] > 0:
            max_pages = existing["max_total_pages"]
            pages_fetched = existing["unique_pages_fetched"]
            
            if max_pages > 0 and pages_fetched >= max_pages:
                console.print(f"    ✅ [cyan]Skipping {endpoint['name']} {start_date.date()}-{end_date.date()}" + 
                            (f" modalidade={modalidade}" if modalidade else "") + 
                            f" - already complete ({pages_fetched}/{max_pages} pages)[/cyan]")
                results["skipped"] = True
                results["total_records"] = existing["total_records"]
                results["success_requests"] = existing["success_responses"]
                return results
        
        # If we have no existing data or incomplete data, extract fresh
        # Make first request to get total pages
        response_data = self._make_request(endpoint, params)
        results["total_requests"] += 1
        
        if response_data["status_code"] == 200:
            results["success_requests"] += 1
            total_pages = response_data["total_pages"]
            results["total_records"] = response_data["total_records"]
            
            # Track expected pages for progress display
            self.total_pages_expected += total_pages
            
            # Show progress for date range with URL pattern
            endpoint_url = f"{PNCP_BASE_URL}{endpoint['path']}"
            sample_params = {k: v for k, v in params.items() if k != 'pagina'}
            sample_params['pagina'] = '[1-N]'
            url_pattern = f"{endpoint_url}?{'&'.join([f'{k}={v}' for k, v in sample_params.items()])}"
            
            console.print(f"    🔄 [green]Extracting {endpoint['name']} {start_date.date()}-{end_date.date()}" + 
                        (f" modalidade={modalidade}" if modalidade else "") + 
                        f" - {total_pages} pages, {results['total_records']:,} records[/green]")
            console.print(f"    🌐 [dim cyan]URL: {url_pattern}[/dim cyan]")
        else:
            results["error_requests"] += 1
            total_pages = 1  # Still store the error response
            
        # Store first response with date range metadata
        self._store_response_range(endpoint, params, response_data, start_date, end_date, modalidade)
        
        # Get remaining pages if there are more - process concurrently
        if total_pages > 1:
            # Process pages concurrently for better performance
            remaining_pages = list(range(2, total_pages + 1))
            concurrent_results = asyncio.run(self._fetch_pages_concurrently(
                endpoint, params, remaining_pages, start_date, end_date, modalidade
            ))
            
            # Aggregate results from concurrent processing
            results["total_requests"] += concurrent_results["total_requests"]
            results["success_requests"] += concurrent_results["success_requests"]
            results["error_requests"] += concurrent_results["error_requests"]
                
        return results

    async def _extract_endpoint_modalidade_range_async(self, endpoint: Dict[str, Any], start_date: datetime, end_date: datetime, modalidade: Optional[int], force: bool = False) -> Dict[str, Any]:
        """Async version of _extract_endpoint_modalidade_range for concurrent modalidade processing."""
        results = {
            "total_requests": 0,
            "total_records": 0,
            "success_requests": 0,
            "error_requests": 0,
            "skipped": False,
            "resumed": False
        }
        
        # Check if we already have complete data for this combination
        existing = self._check_existing_extraction_range(endpoint, start_date, end_date, modalidade)
        
        # Build base parameters for date range
        params = {
            "pagina": 1,
            "tamanhoPagina": endpoint["default_page_size"]
        }
        
        # Add date parameters for the full range
        if endpoint["supports_date_range"]:
            if "dataInicio" in endpoint["date_params"]:
                params["dataInicio"] = self._format_date(start_date)
                params["dataFim"] = self._format_date(end_date)
            else:
                params["dataInicial"] = self._format_date(start_date)
                params["dataFinal"] = self._format_date(end_date)
        else:
            # For endpoints like proposta that only use dataFinal, use end_date
            params["dataFinal"] = self._format_date(end_date)
        
        # Add modalidade if required
        if modalidade is not None:
            params["codigoModalidadeContratacao"] = modalidade
        
        # Check if this exact date range extraction is complete (unless force=True)
        if not force and existing["success_responses"] > 0:
            max_pages = existing["max_total_pages"]
            pages_fetched = existing["unique_pages_fetched"]
            
            if max_pages > 0 and pages_fetched >= max_pages:
                console.print(f"    ✅ [cyan]Skipping {endpoint['name']} {start_date.date()}-{end_date.date()}" + 
                            (f" modalidade={modalidade}" if modalidade else "") + 
                            f" - already complete ({pages_fetched}/{max_pages} pages)[/cyan]")
                results["skipped"] = True
                results["total_records"] = existing["total_records"]
                results["success_requests"] = existing["success_responses"]
                return results
        
        # If force=True, show that we're re-extracting
        if force and existing["success_responses"] > 0:
            console.print(f"    🔄 [yellow]Force re-extracting {endpoint['name']} {start_date.date()}-{end_date.date()}" + 
                        (f" modalidade={modalidade}" if modalidade else "") + 
                        " - overriding existing data[/yellow]")
        
        # If we have no existing data or incomplete data, extract fresh
        # Make first request to get total pages (async version)
        response_data = await self._make_request_async(endpoint, params)
        results["total_requests"] += 1
        
        if response_data["status_code"] == 200:
            results["success_requests"] += 1
            total_pages = response_data["total_pages"]
            results["total_records"] = response_data["total_records"]
            
            # Track expected pages for progress display
            self.total_pages_expected += total_pages
            
            # Show progress for date range with URL pattern
            endpoint_url = f"{PNCP_BASE_URL}{endpoint['path']}"
            sample_params = {k: v for k, v in params.items() if k != 'pagina'}
            sample_params['pagina'] = '[1-N]'
            url_pattern = f"{endpoint_url}?{'&'.join([f'{k}={v}' for k, v in sample_params.items()])}"
            
            console.print(f"    🔄 [green]Extracting {endpoint['name']} {start_date.date()}-{end_date.date()}" + 
                        (f" modalidade={modalidade}" if modalidade else "") + 
                        f" - {total_pages} pages, {results['total_records']:,} records[/green]")
            console.print(f"    🌐 [dim cyan]URL: {url_pattern}[/dim cyan]")
        else:
            results["error_requests"] += 1
            total_pages = 1  # Still store the error response
            
        # Store first response with date range metadata
        self._store_response_range(endpoint, params, response_data, start_date, end_date, modalidade)
        
        # Get remaining pages if there are more - process concurrently
        if total_pages > 1:
            # Process pages concurrently for better performance
            remaining_pages = list(range(2, total_pages + 1))
            concurrent_results = await self._fetch_pages_concurrently(
                endpoint, params, remaining_pages, start_date, end_date, modalidade
            )
            
            # Aggregate results from concurrent processing
            results["total_requests"] += concurrent_results["total_requests"]
            results["success_requests"] += concurrent_results["success_requests"]
            results["error_requests"] += concurrent_results["error_requests"]
                
        return results

    async def _fetch_pages_concurrently(self, endpoint: Dict[str, Any], base_params: Dict[str, Any], 
                                       page_numbers: List[int], start_date: datetime, end_date: datetime, 
                                       modalidade: Optional[int] = None, max_concurrent: int = 5) -> Dict[str, Any]:
        """Fetch multiple pages concurrently with controlled concurrency."""
        results = {
            "total_requests": 0,
            "success_requests": 0,
            "error_requests": 0
        }
        
        # Create semaphore to limit concurrent requests
        semaphore = asyncio.Semaphore(max_concurrent)
        
        async def fetch_single_page(page_num: int) -> Dict[str, Any]:
            async with semaphore:
                # Create new params dict for this page
                params = base_params.copy()
                params["pagina"] = page_num
                
                # Make async request
                response_data = await self._make_request_async(endpoint, params)
                
                # Store response (note: this is still synchronous database write)
                self._store_response_range(endpoint, params, response_data, start_date, end_date, modalidade)
                
                return {
                    "status_code": response_data["status_code"],
                    "page": page_num
                }
        
        # Execute all page requests concurrently
        tasks = [fetch_single_page(page) for page in page_numbers]
        page_results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # Aggregate results
        for result in page_results:
            results["total_requests"] += 1
            if isinstance(result, dict) and result.get("status_code") == 200:
                results["success_requests"] += 1
            else:
                results["error_requests"] += 1
        
        return results
    
    def _extract_endpoint_modalidade(self, endpoint: Dict[str, Any], data_date: datetime, modalidade: Optional[int], force: bool = False) -> Dict[str, Any]:
        """Extract data from endpoint for a specific modalidade (or no modalidade)."""
        results = {
            "total_requests": 0,
            "total_records": 0,
            "success_requests": 0,
            "error_requests": 0,
            "skipped": False,
            "resumed": False
        }
        
        # Check if we already have complete data for this combination
        existing = self._check_existing_extraction(endpoint, data_date, modalidade)
        
        # Build base parameters
        params = {
            "pagina": 1,
            "tamanhoPagina": endpoint["default_page_size"]
        }
        
        # Add date parameters
        if endpoint["supports_date_range"]:
            if "dataInicio" in endpoint["date_params"]:
                params["dataInicio"] = self._format_date(data_date)
                params["dataFim"] = self._format_date(data_date)
            else:
                params["dataInicial"] = self._format_date(data_date)
                params["dataFinal"] = self._format_date(data_date)
        else:
            # For endpoints like proposta that only use dataFinal
            params["dataFinal"] = self._format_date(data_date)
        
        # Add modalidade if required
        if modalidade is not None:
            params["codigoModalidadeContratacao"] = modalidade
        
        # If force=True, show that we're re-extracting
        if force and existing["success_responses"] > 0:
            console.print(f"    🔄 [yellow]Force re-extracting {endpoint['name']} {data_date.date()}" + 
                        (f" modalidade={modalidade}" if modalidade else "") + 
                        " - overriding existing data[/yellow]")
        
        # If we have no existing data, or only error responses, start fresh (or if force=True)
        if existing["success_responses"] == 0 or force:
            # Make first request to get total pages
            response_data = self._make_request(endpoint, params)
            results["total_requests"] += 1
            
            if response_data["status_code"] == 200:
                results["success_requests"] += 1
                total_pages = response_data["total_pages"]
                results["total_records"] = response_data["total_records"]
            else:
                results["error_requests"] += 1
                total_pages = 1  # Still store the error response
                
            # Store first response
            self._store_response(endpoint, params, response_data, data_date, modalidade)
            
            start_page = 2
        else:
            # We have some existing data - check if it's complete
            max_pages = existing["max_total_pages"]
            pages_fetched = existing["unique_pages_fetched"]
            
            # If we have all pages, skip this extraction entirely
            if max_pages > 0 and pages_fetched >= max_pages:
                console.print(f"    ✅ [cyan]Skipping {endpoint['name']} {data_date.date()}" + 
                            (f" modalidade={modalidade}" if modalidade else "") + 
                            f" - already complete ({pages_fetched}/{max_pages} pages)[/cyan]")
                results["skipped"] = True
                results["total_records"] = existing["total_records"]
                results["success_requests"] = existing["success_responses"]
                return results
            
            # Incomplete extraction - resume from where we left off
            total_pages = max_pages
            results["total_records"] = existing["total_records"]
            results["success_requests"] = existing["success_responses"]
            results["resumed"] = True
            
            # Find which pages we still need
            existing_pages_query = """
                SELECT DISTINCT current_page 
                FROM psa.pncp_raw_responses 
                WHERE endpoint_name = ? AND data_date = ? AND response_code = 200
            """
            existing_params = [endpoint["name"], data_date.date()]
            if modalidade is not None:
                existing_pages_query += " AND modalidade = ?"
                existing_params.append(modalidade)
            else:
                existing_pages_query += " AND modalidade IS NULL"
            
            existing_pages = {row[0] for row in self.conn.execute(existing_pages_query, existing_params).fetchall()}
            missing_pages = [p for p in range(1, total_pages + 1) if p not in existing_pages]
            
            if not missing_pages:
                console.print(f"    ✅ [cyan]Skipping {endpoint['name']} {data_date.date()}" + 
                            (f" modalidade={modalidade}" if modalidade else "") + 
                            f" - already complete ({pages_fetched}/{max_pages} pages)[/cyan]")
                results["skipped"] = True
                return results
            
            console.print(f"    🔄 [yellow]Resuming {endpoint['name']} {data_date.date()}" + 
                        (f" modalidade={modalidade}" if modalidade else "") + 
                        f" - fetching {len(missing_pages)} missing pages[/yellow]")
            
            start_page = min(missing_pages)
            
            # If we need page 1, make the first request
            if 1 in missing_pages:
                response_data = self._make_request(endpoint, params)
                results["total_requests"] += 1
                
                if response_data["status_code"] == 200:
                    results["success_requests"] += 1
                    # Update total_pages from fresh response
                    total_pages = response_data["total_pages"]
                else:
                    results["error_requests"] += 1
                    
                # Store first response
                self._store_response(endpoint, params, response_data, data_date, modalidade)
                
                missing_pages.remove(1)
            
            # Update missing pages list if total_pages changed
            if total_pages != max_pages:
                missing_pages = [p for p in range(start_page, total_pages + 1) if p not in existing_pages]
        
        # Get remaining pages
        remaining_pages = list(range(start_page, total_pages + 1)) if not results["resumed"] else missing_pages
        
        for page in remaining_pages:
            if page == 1:  # Already handled above
                continue
                
            params["pagina"] = page
            response_data = self._make_request(endpoint, params)
            results["total_requests"] += 1
            
            if response_data["status_code"] == 200:
                results["success_requests"] += 1
            else:
                results["error_requests"] += 1
                
            # Store response
            self._store_response(endpoint, params, response_data, data_date, modalidade)
                
        return results
    
    def _extract_endpoint_modalidade_single(self, endpoint: Dict[str, Any], data_date: datetime, modalidade: Optional[int], force: bool = False) -> Dict[str, Any]:
        """Extract data from endpoint for a single date."""
        return self._extract_endpoint_modalidade(endpoint, data_date, modalidade, force)
    
    async def extract_all_data(self, start_date: datetime, end_date: datetime, force: bool = False) -> Dict[str, Any]:
        """Extract data from all endpoints for a date range."""
        console.print(Panel("🔄 Starting PNCP Data Extraction", style="bold blue"))
        console.print(f"📅 Date Range: {start_date.date()} to {end_date.date()}")
        console.print(f"🆔 Run ID: {self.run_id}")
        console.print(f"📊 Endpoints: {len(PNCP_ENDPOINTS)}")
        if force:
            console.print("⚠️ [yellow]Force mode enabled - will re-extract existing data[/yellow]")
        
        # Start extraction timer for RPS tracking
        self._start_extraction_timer()
        
        total_results = {
            "run_id": self.run_id,
            "start_date": start_date.date(),
            "end_date": end_date.date(),
            "total_requests": 0,
            "total_records": 0,
            "success_requests": 0,
            "error_requests": 0,
            "endpoints_processed": [],
            "dates_processed": [],
            "skipped_extractions": 0,
            "resumed_extractions": 0
        }
        
        # Calculate total date range chunks for progress bar
        total_chunks = 0
        temp_start = start_date
        while temp_start <= end_date:
            temp_end = min(temp_start + timedelta(days=364), end_date)
            total_chunks += 1
            temp_start = temp_end + timedelta(days=1)
        
        # Process in date range chunks (up to 365 days per chunk) with progress bar
        from rich.progress import Progress, SpinnerColumn, TextColumn, BarColumn, TimeElapsedColumn, MofNCompleteColumn
        
        with Progress(
            SpinnerColumn(),
            TextColumn("[progress.description]{task.description}"),
            BarColumn(),
            MofNCompleteColumn(),
            TimeElapsedColumn(),
            console=console,
            transient=False
        ) as progress:
            
            # Main date range progress bar
            date_range_task = progress.add_task(
                "📅 Processing Date Ranges", 
                total=total_chunks
            )
            
            current_start = start_date
            chunk_number = 1
            
            while current_start <= end_date:
                # Calculate chunk end date (max 365 days or end_date, whichever is smaller)
                chunk_end = min(current_start + timedelta(days=364), end_date)  # 364 + 1 = 365 days
                days_in_chunk = (chunk_end - current_start).days + 1
                
                progress.update(date_range_task, description=f"📅 Processing Chunk {chunk_number}: {current_start.date()} to {chunk_end.date()} ({days_in_chunk} days)")
                
                chunk_results = {
                    "chunk_number": chunk_number,
                    "start_date": current_start.date(),
                    "end_date": chunk_end.date(),
                    "days_in_chunk": days_in_chunk,
                    "total_requests": 0,
                    "total_records": 0,
                    "success_requests": 0,
                    "error_requests": 0,
                    "endpoints": []
                }
                
                # Process each endpoint for this date range chunk with individual progress bars
                await self._extract_chunk_with_progress_bars(chunk_results, total_results, current_start, chunk_end, chunk_number, days_in_chunk, force)
                
                total_results["total_requests"] += chunk_results["total_requests"]
                total_results["total_records"] += chunk_results["total_records"]
                total_results["success_requests"] += chunk_results["success_requests"]
                total_results["error_requests"] += chunk_results["error_requests"]
                total_results["dates_processed"].append(chunk_results)
                
                # Update progress
                progress.update(date_range_task, advance=1)
                
                # Move to next chunk
                current_start = chunk_end + timedelta(days=1)
                chunk_number += 1
        
        self._print_summary(total_results)
        return total_results

    async def _extract_chunk_with_progress_bars(self, chunk_results: Dict[str, Any], total_results: Dict[str, Any], 
                                               start_date: datetime, end_date: datetime, chunk_number: int, days_in_chunk: int, force: bool = False):
        """Extract data with individual progress bars for each URL pattern."""
        
        # Step 1: Discover all URL patterns and create progress bars
        url_patterns = []
        
        for endpoint in PNCP_ENDPOINTS:
            if "modalidades" in endpoint and endpoint.get("modalidade_strategy") != "optional_unrestricted":
                # Create a pattern for each modalidade
                for modalidade in endpoint["modalidades"]:
                    pattern_id = f"{endpoint['name']}_mod_{modalidade}"
                    pattern_name = f"{endpoint['name']} (modalidade {modalidade})"
                    url_patterns.append({
                        "id": pattern_id,
                        "name": pattern_name,
                        "endpoint": endpoint,
                        "modalidade": modalidade
                    })
            else:
                # Single pattern for endpoint without modalidades
                pattern_id = endpoint['name']
                pattern_name = endpoint['name']
                url_patterns.append({
                    "id": pattern_id,
                    "name": pattern_name,
                    "endpoint": endpoint,
                    "modalidade": None
                })
        
        # Step 2: Process patterns sequentially for now (to avoid nested progress bar conflicts)
        console.print(f"\n📅 Processing Date Range Chunk {chunk_number}: {start_date.date()} to {end_date.date()} ({days_in_chunk} days)")
        
        # Process each pattern individually
        for pattern in url_patterns:
            endpoint = pattern["endpoint"]
            modalidade = pattern["modalidade"]
            
            # Extract this pattern using the existing method
            try:
                if endpoint.get("supports_date_range", True):
                    # Use date range extraction (async version)
                    result = await self._extract_endpoint_modalidade_range_async(endpoint, start_date, end_date, modalidade, force)
                else:
                    # Use single date extraction for endpoints that don't support ranges
                    result = self._extract_endpoint_modalidade(endpoint, end_date, modalidade, force)
                
                # Aggregate results
                chunk_results["total_requests"] += result.get("total_requests", 0)
                chunk_results["total_records"] += result.get("total_records", 0)
                chunk_results["success_requests"] += result.get("success_requests", 0)
                chunk_results["error_requests"] += result.get("error_requests", 0)
                
                # Track specific endpoint results
                endpoint_result = {
                    "endpoint_name": endpoint["name"],
                    "modalidade": modalidade,
                    "total_requests": result.get("total_requests", 0),
                    "total_records": result.get("total_records", 0),
                    "success_requests": result.get("success_requests", 0),
                    "error_requests": result.get("error_requests", 0),
                    "skipped": result.get("skipped", False),
                    "resumed": result.get("resumed", False)
                }
                chunk_results["endpoints"].append(endpoint_result)
                
                # Update global counters
                if result.get("skipped"):
                    total_results["skipped_extractions"] += 1
                if result.get("resumed"):
                    total_results["resumed_extractions"] += 1
                    
            except Exception as e:
                console.print(f"❌ [red]Error processing {pattern['name']}: {e}[/red]")
                chunk_results["error_requests"] += 1

    async def _discover_pattern_total(self, pattern: Dict[str, Any], start_date: datetime, end_date: datetime, progress, task_id) -> Dict[str, Any]:
        """Make first request to discover total pages for a URL pattern."""
        endpoint = pattern["endpoint"]
        modalidade = pattern["modalidade"]
        
        # Build parameters
        params = {
            "pagina": 1,
            "tamanhoPagina": endpoint["default_page_size"]
        }
        
        # Add date parameters
        if endpoint["supports_date_range"]:
            if "dataInicio" in endpoint["date_params"]:
                params["dataInicio"] = self._format_date(start_date)
                params["dataFim"] = self._format_date(end_date)
            else:
                params["dataInicial"] = self._format_date(start_date)
                params["dataFinal"] = self._format_date(end_date)
        else:
            params["dataFinal"] = self._format_date(end_date)
        
        # Add modalidade if required
        if modalidade is not None:
            params["codigoModalidadeContratacao"] = modalidade
        
        # Make discovery request
        response_data = await self._make_request_async(endpoint, params)
        
        if response_data["status_code"] == 200:
            total_pages = response_data["total_pages"]
            total_records = response_data["total_records"]
            
            # Update progress bar with discovered total
            progress.update(task_id, total=total_pages, completed=1)
            progress.start_task(task_id)
            
            # Show URL and stats
            endpoint_url = f"{PNCP_BASE_URL}{endpoint['path']}"
            sample_params = {k: v for k, v in params.items() if k != 'pagina'}
            sample_params['pagina'] = '[1-N]'
            url_pattern = f"{endpoint_url}?{'&'.join([f'{k}={v}' for k, v in sample_params.items()])}"
            console.print(f"🌐 [dim cyan]{pattern['name']}: {total_pages} pages, {total_records:,} records[/dim cyan]")
            console.print(f"   [dim]{url_pattern}[/dim]")
            
            # Store first response
            self._store_response_range(endpoint, params, response_data, start_date, end_date, modalidade)
            
            return {
                "total_pages": total_pages,
                "total_records": total_records,
                "params": params,
                "success": True
            }
        else:
            progress.update(task_id, total=1, completed=1)
            progress.start_task(task_id)
            console.print(f"❌ [red]{pattern['name']}: Failed with status {response_data['status_code']}[/red]")
            return {
                "total_pages": 1,
                "total_records": 0,
                "params": params,
                "success": False
            }

    async def _extract_pattern_pages(self, pattern: Dict[str, Any], start_date: datetime, end_date: datetime, 
                                   discovery_result: Dict[str, Any], progress, task_id) -> Dict[str, Any]:
        """Extract remaining pages for a URL pattern with progress updates."""
        endpoint = pattern["endpoint"]
        modalidade = pattern["modalidade"]
        total_pages = discovery_result["total_pages"]
        base_params = discovery_result["params"]
        
        results = {
            "total_requests": 1,  # Include discovery request
            "total_records": discovery_result["total_records"],
            "success_requests": 1 if discovery_result["success"] else 0,
            "error_requests": 0 if discovery_result["success"] else 1
        }
        
        if total_pages > 1:
            # Extract remaining pages concurrently
            remaining_pages = list(range(2, total_pages + 1))
            
            # Use semaphore to limit concurrent requests per pattern
            semaphore = asyncio.Semaphore(3)  # 3 concurrent requests per pattern
            
            async def fetch_single_page(page_num: int) -> Dict[str, Any]:
                async with semaphore:
                    params = base_params.copy()
                    params["pagina"] = page_num
                    
                    response_data = await self._make_request_async(endpoint, params)
                    self._store_response_range(endpoint, params, response_data, start_date, end_date, modalidade)
                    
                    # Update progress bar
                    progress.advance(task_id)
                    
                    return {
                        "status_code": response_data["status_code"],
                        "page": page_num
                    }
            
            # Execute all page requests concurrently
            tasks = [fetch_single_page(page) for page in remaining_pages]
            page_results = await asyncio.gather(*tasks, return_exceptions=True)
            
            # Aggregate results
            for result in page_results:
                results["total_requests"] += 1
                if isinstance(result, dict) and result.get("status_code") == 200:
                    results["success_requests"] += 1
                else:
                    results["error_requests"] += 1
        
        return results
    
    def _print_summary(self, results: Dict[str, Any]):
        """Print extraction summary with performance metrics."""
        final_progress = self._get_progress_info()
        total_time = time.time() - self.extraction_start_time if self.extraction_start_time else 0
        
        table = Table(title="🔍 PNCP Data Extraction Summary")
        
        table.add_column("Metric", style="cyan")
        table.add_column("Value", style="green", justify="right")
        
        table.add_row("Run ID", results["run_id"])
        table.add_row("Date Range", f"{results['start_date']} to {results['end_date']}")
        table.add_row("Total Requests", f"{results['total_requests']:,}")
        table.add_row("Total Pages", f"{final_progress['pages_processed']:,}")
        table.add_row("Total Records", f"{results['total_records']:,}")
        table.add_row("Success Requests", f"{results['success_requests']:,}")
        table.add_row("Error Requests", f"{results['error_requests']:,}")
        table.add_row("Avg RPS", f"{final_progress['rps']:.2f}")
        table.add_row("Total Time", f"{total_time:.1f}s")
        
        # Add resume/skip statistics
        if results.get("skipped_extractions", 0) > 0:
            table.add_row("Skipped (Complete)", f"{results['skipped_extractions']:,}", style="cyan")
        if results.get("resumed_extractions", 0) > 0:
            table.add_row("Resumed (Partial)", f"{results['resumed_extractions']:,}", style="yellow")
        
        success_rate = (results['success_requests'] / results['total_requests'] * 100) if results['total_requests'] > 0 else 0
        table.add_row("Success Rate", f"{success_rate:.1f}%")
        
        console.print(table)
        
        # Database summary
        record_count = self.conn.execute("SELECT COUNT(*) FROM psa.pncp_raw_responses WHERE run_id = ?", [results["run_id"]]).fetchone()[0]
        console.print(Panel(
            f"💾 Database: {record_count:,} responses stored\\n"
            f"📊 Database Path: {BALIZA_DB_PATH}",
            title="Storage Summary",
            style="bold green"
        ))
    
    def get_extraction_stats(self) -> Dict[str, Any]:
        """Get statistics about stored data."""
        stats = {}
        
        # Overall stats
        stats["total_responses"] = self.conn.execute("SELECT COUNT(*) FROM psa.pncp_raw_responses").fetchone()[0]
        stats["unique_endpoints"] = self.conn.execute("SELECT COUNT(DISTINCT endpoint_name) FROM psa.pncp_raw_responses").fetchone()[0]
        stats["date_range"] = self.conn.execute("SELECT MIN(data_date), MAX(data_date) FROM psa.pncp_raw_responses").fetchone()
        
        # Success rate
        success_responses = self.conn.execute("SELECT COUNT(*) FROM psa.pncp_raw_responses WHERE response_code = 200").fetchone()[0]
        stats["success_rate"] = (success_responses / stats["total_responses"] * 100) if stats["total_responses"] > 0 else 0
        
        # Records by endpoint
        endpoint_stats = self.conn.execute("""
            SELECT endpoint_name, COUNT(*) as responses, SUM(total_records) as total_records
            FROM psa.pncp_raw_responses 
            WHERE response_code = 200
            GROUP BY endpoint_name
            ORDER BY total_records DESC
        """).fetchall()
        
        stats["endpoints"] = [
            {"name": row[0], "responses": row[1], "total_records": row[2]}
            for row in endpoint_stats
        ]
        
        return stats
    
    def __del__(self):
        """Cleanup."""
        if hasattr(self, 'client'):
            self.client.close()
        if hasattr(self, 'async_client') and self.async_client:
            # Close async client if it exists
            try:
                asyncio.run(self.async_client.aclose())
            except Exception:
                pass  # May fail if event loop is already closed
        if hasattr(self, 'conn'):
            self.conn.close()


# CLI interface
app = typer.Typer()

@app.command()
def extract(
    start_date: str = typer.Option(
        "2021-01-01",
        help="Start date (YYYY-MM-DD) - defaults to 2021-01-01 for full historical extraction"
    ),
    end_date: str = typer.Option(
        datetime.now().strftime("%Y-%m-%d"),
        help="End date (YYYY-MM-DD) - defaults to today"
    ),
    force: bool = typer.Option(
        False,
        "--force",
        help="Force re-extraction even if data already exists in PSA"
    )
):
    """Extract data from all PNCP endpoints for a date range."""
    try:
        start_dt = datetime.strptime(start_date, "%Y-%m-%d")
        end_dt = datetime.strptime(end_date, "%Y-%m-%d")
    except ValueError:
        console.print("❌ Invalid date format. Use YYYY-MM-DD", style="bold red")
        raise typer.Exit(1)
    
    if start_dt > end_dt:
        console.print("❌ Start date must be before end date", style="bold red")
        raise typer.Exit(1)
    
    extractor = SimplePNCPExtractor()
    results = asyncio.run(extractor.extract_all_data(start_dt, end_dt, force=force))
    
    # Save results to file
    results_file = DATA_DIR / f"extraction_results_{results['run_id']}.json"
    with open(results_file, 'w') as f:
        json.dump(results, f, indent=2, default=str)
    
    console.print(f"📄 Results saved to: {results_file}")

@app.command()
def stats():
    """Show extraction statistics."""
    extractor = SimplePNCPExtractor()
    stats = extractor.get_extraction_stats()
    
    console.print(Panel("📊 PNCP Extraction Statistics", style="bold blue"))
    
    table = Table()
    table.add_column("Metric", style="cyan")
    table.add_column("Value", style="green", justify="right")
    
    table.add_row("Total Responses", f"{stats['total_responses']:,}")
    table.add_row("Unique Endpoints", f"{stats['unique_endpoints']:,}")
    table.add_row("Success Rate", f"{stats['success_rate']:.1f}%")
    
    if stats['date_range'][0]:
        table.add_row("Date Range", f"{stats['date_range'][0]} to {stats['date_range'][1]}")
    
    console.print(table)
    
    # Endpoint breakdown
    if stats['endpoints']:
        endpoint_table = Table(title="Records by Endpoint")
        endpoint_table.add_column("Endpoint", style="cyan")
        endpoint_table.add_column("Responses", style="yellow", justify="right")
        endpoint_table.add_column("Total Records", style="green", justify="right")
        
        for endpoint in stats['endpoints']:
            endpoint_table.add_row(
                endpoint['name'],
                f"{endpoint['responses']:,}",
                f"{endpoint['total_records']:,}"
            )
        
        console.print(endpoint_table)

if __name__ == "__main__":
    app()
</file>

<file path=".gitignore">
# Data directories (new structure)
data/
.cache/
.config/

# Legacy data directories (to be cleaned up)
baliza_data/
state/
src/baliza_data/
src/state/

# Database files
*.duckdb
*.duckdb.wal

# Python
__pycache__/
*.pyc
*.pyo
*.pyd
.Python
build/
develop-eggs/
dist/
downloads/
eggs/
.eggs/
lib/
lib64/
parts/
sdist/
var/
wheels/
*.egg-info/
**/*.egg-info/
.installed.cfg
*.egg

# Virtual environments
.venv/
venv/
ENV/
env/

# IDE
.vscode/
.idea/
*.swp
*.swo
*~

# OS
.DS_Store
.DS_Store?
._*
.Spotlight-V100
.Trashes
ehthumbs.db
Thumbs.db

# Logs
*.log
logs/

# Temporary files
*.tmp
*.temp

# Package files
uv.lock
</file>

<file path="README.md">
# BALIZA

**Acrônimo oficial**
**B**ackup **A**berto de **LI**citações **Z**elando pelo **A**cesso

---

### Pitch de uma linha

> “BALIZA espelha diariamente o PNCP no Internet Archive para garantir histórico completo e análises retroativas de compras públicas.”

### README.md — estrutura mínima (Atualizado)

````markdown
# BALIZA

**Backup Aberto de Licitações Zelando pelo Acesso** — um bot que baixa o delta diário do PNCP e envia para o Internet Archive em JSONL compactado.

## Por quê
- O PNCP só mantém dados acessíveis via API e sem versionamento.
- Sem séries históricas não há auditoria séria nem detecção de fraude por padrão.

## Como funciona
1. **Coleta Automática**: Por padrão, BALIZA processa TODO o histórico disponível (2020-2025+), identificando automaticamente datas faltantes.
2. **Busca de Dados**: O CLI `baliza` chama os endpoints da API do PNCP (`/v1/contratacoes/publicacao`, `/v1/contratos/publicacao`, `/v1/pca`) com paginação inteligente.
3. **Processamento e Compactação**: Os dados são organizados em Parquet e DuckDB para análise local, além de JSONL compactado com Zstandard para arquivamento.
4. **Upload para o Internet Archive**: Arquivos são enviados para o Internet Archive com metadados estruturados e checksums SHA256.
5. **Federação de Dados**: Sistema de federação permite consultas unificadas entre dados locais e do Internet Archive usando DuckDB.

## 🚀 Como Usar

### 📊 **Para Análise de Dados (Recomendado)**

**🎯 Análise Instantânea no Google Colab:**

[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/franklinbaldo/baliza/blob/main/notebooks/analise_pncp_colab.ipynb)

- ✅ **Um clique** e você está analisando milhões de contratos públicos
- ✅ **Sem configuração** - funciona 100% no navegador  
- ✅ **Dados atualizados** diretamente do Internet Archive
- ✅ **Análises pré-configuradas** com visualizações interativas
- ✅ **Detecção de fraudes** e padrões suspeitos automatizada

### 🔧 **Para Coleta de Dados**

#### Pré-requisitos
- Python 3.11+
- `uv` (gerenciador de pacotes Python rápido). Se não tiver: `curl -LsSf https://astral.sh/uv/install.sh | sh`
- Credenciais do Internet Archive (`IA_KEY` e `IA_SECRET`).

#### 🚀 **Instalação e Uso (Simples)**

1. **Clone e configure o projeto:**
   ```bash
   git clone https://github.com/franklinbaldo/baliza.git
   cd baliza
   
   # Instalar dependências
   uv sync
   
   # Instalar BALIZA como CLI tool
   uv pip install -e .
   ```

2. **Configure credenciais do Internet Archive:**
   ```bash
   export IA_KEY="SUA_CHAVE_IA"
   export IA_SECRET="SEU_SEGREDO_IA"
   ```

3. **Execute a coleta:**

   **🏛️ MODO AUTOMÁTICO (Recomendado) - Baixa TODO o histórico:**
   ```bash
   uv run baliza --auto
   ```
   
   **📅 MODO DATA ESPECÍFICA:**
   ```bash
   uv run baliza --date 2024-07-10
   ```
   
   **⚡ MODO ÚLTIMOS N DIAS:**
   ```bash
   uv run baliza --auto --days-back 30
   ```

#### 📁 **Estrutura de Dados**

BALIZA usa estrutura de diretórios XDG-compliant:

**🏠 Desenvolvimento (padrão):**
```
baliza/
├── data/           # Bancos de dados principais
├── .cache/         # Cache de downloads do IA
└── .config/        # Configurações
```

**🌐 Produção (BALIZA_PRODUCTION=1):**
```
~/.local/share/baliza/  # Dados do usuário
~/.cache/baliza/        # Cache do usuário  
~/.config/baliza/       # Config do usuário
```

#### 🔗 **Federação com Internet Archive**
```bash
# Atualizar federação (incluído automaticamente no --auto)
uv run python src/baliza/ia_federation.py federate

# Executar análises com DBT
cd dbt_baliza
dbt run --select coverage_temporal coverage_entidades
```

## Automação com GitHub Actions
- O projeto inclui um workflow em `.github/workflows/baliza_daily_run.yml`.
- Este workflow executa o script diariamente às 02:15 BRT (05:15 UTC) em modo automático completo.
- O workflow captura um sumário da execução em formato JSON e o armazena como um artefato do GitHub Actions para referência e depuração.
- **Importante**: Para que o upload automático funcione, você **DEVE** configurar `IA_KEY` e `IA_SECRET` como "Secrets" nas configurações do seu repositório GitHub (Settings > Secrets and variables > Actions).

## Roadmap

### Fase 1: Implementação Central (Concluída)
* [x] Script de coleta para `/v1/contratacoes/publicacao` com CLI (`Typer`).
* [x] Compressão dos dados para `.jsonl.zst`.
* [x] Upload dos arquivos para o Internet Archive.
* [x] Agendamento da execução diária via GitHub Actions.
* [x] Captura de sumário estruturado da execução (JSON) como artefato no GitHub Actions.

### Fase 2: Página de Estatísticas e Compartilhamento de Torrents (Planejado)
* [ ] **Coleta de Estatísticas**: Desenvolver script para agregar dados das execuções diárias (e.g., itens coletados, status, links IA).
* [ ] **Manifesto de Torrents**: Gerar e manter uma lista atualizada dos links `.torrent` para os itens arquivados no Internet Archive.
* [ ] **Página Web de Estatísticas**:
    * [ ] Criar template HTML para a página de status.
    * [ ] Desenvolver script para gerar a página HTML estática a partir dos dados de estatísticas e torrents.
    * [ ] Configurar GitHub Pages para hospedar a página.
* [ ] **Atualização do Workflow**: Incrementar o GitHub Actions para executar os scripts de coleta de estatísticas, geração de manifesto de torrents e da página web, e fazer commit dos artefatos atualizados.

### Fase 3: Expansão de Endpoints (Implementado)
* [x] **Implementar coleta para novos endpoints**:
    * [x] `/v1/pca/usuario` - Consultar Itens de PCA por Ano do PCA, IdUsuario e Código de Classificação Superior.
    * [x] `/v1/pca/` - Consultar Itens de PCA por Ano do PCA e Código de Classificação Superior (endpoint geral).
    * [x] `/v1/contratacoes/proposta` - Consultar Contratações com Recebimento de Propostas Aberto.

### Funcionalidades Adicionais (Consideradas para o Futuro - Pós Fase 3)
* [ ] Implementar persistência robusta de checksums e estado de processamento (e.g., `state/processed.csv`) para evitar reprocessamento e duplicatas de forma mais granular.
* [ ] Avaliar a criação de um dump automático para ClickHouse a partir dos dados no Internet Archive.
* [ ] Desenvolver um painel analítico (Superset/Metabase) com KPIs (e.g., sobrepreço) utilizando os dados coletados.
* [ ] Configurar alertas de anomalia (e.g., via Webhook) para falhas na coleta ou problemas nos dados.
* [ ] Implementar configuração dinâmica de parâmetros para os novos endpoints PCA.

## Próximos Passos (Comunidade e Testes)
1. **Teste Manual Extensivo**: Execute o script com `--date` para diferentes dias passados para garantir a robustez do hash, da coleta e do upload.
2. **Feedback e Contribuições**: Abra issues para bugs, sugestões ou melhorias. Contribuições via Pull Requests são bem-vindas!
3. **Anunciar e Engajar**: Após estabilização, considere anunciar no fórum Dados Abertos BR e convidar a comunidade para auditar os dados e o processo.

```

### Projeto em 5 min — visão geral

1. **Use os próprios endpoints de consulta do PNCP** (`/v1/contratacoes/publicacao`, `/v1/contratos/publicacao`, `/v1/pca`, etc.), que já aceitam filtros por intervalo de datas, paginação (`pagina`, `tamanhoPagina ≤ 500`) e devolvem JSON padronizado.&#x20;
2. **Rode um *crawler* diário** (cron ou GitHub Actions) que baixa só o delta do dia anterior. Não invente “varredura completa” — é lento, caro e sujeito a time-out.
3. **Empacote cada lote em `JSONL` comprimido** (`.jsonl.zst` é ótimo) e gere um manifesto SHA-256 para deduplicar depois.
4. **Suba o arquivo para o Internet Archive** usando a API S3-like (`ias3`) com nome estável, ex.:
   `pncp-contratacoes-2025-07-03.jsonl.zst`
   Metadados mínimos: `title`, `creator=“PNCP Mirror Bot”`, `subject=“public procurement Brazil”`. ([archive.org][1], [archive.org][2])
5. **Repita.** Em poucos meses você terá um *data lake* público, versionado e historicamente completo para qualquer análise contábil, *benchmarking* de preços, *red-flag analytics*, etc.

---

### Arquitetura Adotada

| Camada                 | Tech-stack                                     | Por quê                                                      |
| ---------------------- | ---------------------------------------------- | ------------------------------------------------------------ |
| **Coleta**             | `python` + `requests` + `tenacity` (retry)     | Leve, controlado, fácil de debugar                           |
| **Gerenciamento de Dep.** | `uv` (Astral)                                  | Rápido, moderno, compatível com `pyproject.toml`             |
| **Agendamento**        | GitHub Actions (cron)                          | Integrado ao repositório, gratuito para projetos open source   |
| **Processamento**      | Python `json` (para JSONL)                     | Simples e direto para conversão em JSONL                     |
| **Compressão**         | `zstandard` (via lib Python)                   | Excelente taxa de compressão e velocidade                    |
| **Upload**             | `internetarchive` CLI/lib Python               | Biblioteca oficial para interagir com o Internet Archive     |
| **Catálogo (Planejado)** | Manifest (`manifest.yml`) + checksums em CSV   | Garante integridade, evita duplicatas (ainda não implementado) |

---

### Fluxo Incremental (Detalhado)

1. **Cálculo da Data**: O script (ou o workflow do GitHub Actions) determina a data "ontem" (fuso horário de Brasília, UTC-3).
2. **Iteração por Endpoints**: Para cada endpoint configurado (inicialmente, apenas `contratacoes`):
   ```text
   GET https://pncp.gov.br/api/consulta/v1/contratacoes/publicacao?dataInicial=YYYY-MM-DD&dataFinal=YYYY-MM-DD&pagina=1&tamanhoPagina=500
   ```
3. **Paginação**: O script itera sobre as páginas de resultados até que `paginaAtual` seja maior ou igual a `totalPaginas` retornado pela API. Cada página pode conter até 500 registros.
4. **Armazenamento Temporário**: Cada registro JSON é anexado a um arquivo `.jsonl` local.
5. **Compressão**: Após coletar todos os dados do dia para um endpoint, o arquivo `.jsonl` é comprimido usando Zstandard, resultando em um arquivo `.jsonl.zst`.
6. **Cálculo de Checksum**: Um hash SHA256 é calculado para o arquivo `.jsonl.zst`.
7. **Upload para Internet Archive**: O arquivo comprimido é enviado para o Internet Archive, e o checksum SHA256 é incluído nos metadados.
8. **Limpeza (Local)**: O arquivo `.jsonl` original é removido após a compressão e tentativa de upload. O arquivo `.jsonl.zst` permanece localmente no diretório `baliza_data/`.
9. **Registro de Estado (Planejado)**: Futuramente, o hash do arquivo e o status do upload serão gravados para evitar reprocessamento e permitir o rastreamento.

---

### Pontos críticos (opinião sem rodeios)

* **Rate-limit e disponibilidade**: Se o *crawler* falhar, não re-tente infinito — o PNCP derruba conexões longas.
* **Token de acesso**: Hoje a consulta é pública, mas o SERPRO pode exigir API-key amanhã; prepare var env.
* **Qualidade dos dados**: Campos financeiros vêm como texto, vírgula decimal e zeros mágicos (0 = sigilo). Não confie neles sem *post-processing*.&#x20;
* **Internet Archive não é banco OLTP**: ele armazena blob; para consultas SQL use BigQuery, ClickHouse ou DuckDB apontando para seus `JSONL`.
* **Legalidade**: dados já são públicos; o espelho é mera redundância. Mas inclua aviso de responsabilidade (“*dados brutos, sem garantias*”).

---

### Próximos passos (Pós-Implementação Inicial)

1. **Repositório e Licença**: O repositório no GitHub está configurado com Licença MIT e este README atualizado. (Feito!)
2. **Automação**: GitHub Actions está habilitado com `cron: '15 5 * * *'` (02:15 BRT / 05:15 UTC) para execução diária. (Feito!)
3. **Dashboard (Futuro)**: Criar um *dashboard* (ex: Superset, Metabase) que consuma os dados dos arquivos `.jsonl.zst` diretamente do Internet Archive (possivelmente via HTTPFS ou similar).
4. **Engajamento Comunitário (Futuro)**: Quando o sistema estiver estável e com um volume razoável de dados arquivados, anunciar no fórum **Dados Abertos BR** para atrair colaboradores, auditores e usuários.

Com a base implementada, o projeto Baliza está pronto para começar a arquivar os dados e evoluir com as funcionalidades planejadas no Roadmap.

[1]: https://archive.org/developers/ias3.html?utm_source=chatgpt.com "ias3 Internet archive S3-like API"
[2]: https://archive.org/developers/index-apis.html?utm_source=chatgpt.com "Tools and APIs — Internet Archive Developer Portal"
</file>

<file path="pyproject.toml">
[project]
name = "baliza"
version = "0.1.0"
description = "BALIZA: Backup Aberto de Licitações Zelando pelo Acesso - Historical archive of Brazilian public procurement data"
readme = "README.md"
requires-python = ">=3.11"
dependencies = [
    "typer",
    "duckdb", # Native Parquet support with built-in compression
    "httpx", # HTTP client for API requests
]

[project.scripts]
baliza = "baliza.pncp_extractor:app"

[project.optional-dependencies]
dev = [
    "pytest",
    "pytest-mock",
]
analytics = [
    "dbt-core>=1.7.0",
    "dbt-duckdb>=1.7.0",
]

[dependency-groups]
dev = [
    "mypy>=1.16.1",
    "pre-commit>=4.2.0",
    "pytest>=8.4.1",
    "pytest-mock>=3.14.1",
    "ruff>=0.12.3",
]

# Ruff configuration for linting and formatting
[tool.ruff]
target-version = "py311"
line-length = 88

# Exclude specific files/directories
exclude = [
    ".bzr",
    ".direnv",
    ".eggs",
    ".git",
    ".git-rewrite",
    ".hg",
    ".mypy_cache",
    ".nox",
    ".pants.d",
    ".pytype",
    ".ruff_cache",
    ".svn",
    ".tox",
    ".venv",
    "__pypackages__",
    "_build",
    "buck-out",
    "build",
    "dist",
    "node_modules",
    "venv",
    "migrations",
    "*.pyi",
]

# Output configuration
output-format = "grouped"

[tool.ruff.lint]
select = [
    # Error
    "E",
    # Warning  
    "W",
    # Pyflakes
    "F",
    # pycodestyle
    "E", "W",
    # mccabe
    "C90",
    # isort
    "I",
    # pep8-naming
    "N",
    # pyupgrade
    "UP",
    # flake8-bugbear
    "B",
    # flake8-bandit
    "S",
    # flake8-blind-except
    "BLE",
    # flake8-comprehensions
    "C4",
    # flake8-debugger
    "T10",
    # flake8-simplify
    "SIM",
    # flake8-unused-arguments
    "ARG",
    # flake8-use-pathlib
    "PTH",
    # pandas-vet
    "PD",
    # tryceratops
    "TRY",
    # Ruff-specific rules
    "RUF",
]

ignore = [
    # Too aggressive
    "S101",  # Use of assert
    "S603",  # subprocess call - check for execution of untrusted input
    "S607",  # Starting a process with a partial executable path
    "TRY003", # Avoid specifying long messages outside the exception class
    "B008",  # Do not perform function calls in argument defaults
    "S608",  # Possible SQL injection vector (we use DuckDB safely)
    "BLE001", # Do not catch blind exception (sometimes needed)
    # Conflicts with formatter
    "E501",  # Line too long
    "W191",  # Indentation contains tabs
    "E111",  # Indentation is not a multiple of 4
    "E114",  # Indentation is not a multiple of 4 (comment)
    "E117",  # Over-indented
    "D206",  # Docstring should be indented with spaces
    "D300",  # Use """triple double quotes"""
    "Q000",  # Single quotes found but double quotes preferred
    "Q001",  # Single quote multiline found but triple quotes preferred
    "Q002",  # Single quote docstring found but triple quotes preferred
    "Q003",  # Change outer quotes to avoid escaping inner quotes
    "COM812", # Missing trailing comma
    "COM819", # Prohibited trailing comma
    "ISC001", # Implicitly concatenated string literals on one line
    "ISC002", # Implicitly concatenated string literals over continuation lines
]

[tool.ruff.lint.mccabe]
# Maximum cyclomatic complexity
max-complexity = 10

[tool.ruff.lint.isort]
known-first-party = ["baliza"]
known-third-party = [
    "duckdb",
    "httpx",
    "typer",
    "pytest",
    "rich",
]
section-order = [
    "future",
    "standard-library", 
    "third-party",
    "first-party",
    "local-folder"
]

[tool.ruff.lint.pep8-naming]
# Allow Pydantic's `@validator` decorator to trigger class method treatment.
classmethod-decorators = ["classmethod", "pydantic.validator"]

[tool.ruff.lint.flake8-bandit]
# S101: Use of assert
check-typed-exception = true

[tool.ruff.lint.pyupgrade]
# Preserve types, even if a file imports `from __future__ import annotations`.
keep-runtime-typing = true

# Mypy configuration for type checking
[tool.mypy]
python_version = "3.11"
warn_return_any = true
warn_unused_configs = true
disallow_untyped_defs = true
disallow_incomplete_defs = true
check_untyped_defs = true
disallow_untyped_decorators = true
no_implicit_optional = true
warn_redundant_casts = true
warn_unused_ignores = true
warn_no_return = true
warn_unreachable = true
strict_equality = true
show_error_codes = true

# Be less strict for some patterns
[[tool.mypy.overrides]]
module = [
    "tests.*",
    "scripts.*"
]
disallow_untyped_defs = false
disallow_incomplete_defs = false
disallow_untyped_decorators = false

# External library stubs
[[tool.mypy.overrides]]
module = [
    "duckdb.*",
    "httpx.*",
    "typer.*",
    "rich.*"
]
ignore_missing_imports = true

# Coverage configuration for pytest-cov
[tool.coverage.run]
source = ["src"]
omit = [
    "*/tests/*",
    "*/test_*",
    "*/__pycache__/*",
    "*/migrations/*",
    "*/venv/*",
    "*/.venv/*"
]

[tool.coverage.report]
exclude_lines = [
    "pragma: no cover",
    "def __repr__",
    "if self.debug:",
    "if settings.DEBUG",
    "raise AssertionError",
    "raise NotImplementedError",
    "if 0:",
    "if __name__ == .__main__.:",
    "class .*\\bProtocol\\):",
    "@(abc\\.)?abstractmethod",
]

[tool.coverage.html]
directory = "htmlcov"

# Build system configuration
[build-system]
requires = ["setuptools>=61.0", "wheel"]
build-backend = "setuptools.build_meta"

# Package discovery
[tool.setuptools.packages.find]
where = ["src"]
include = ["baliza*"]
exclude = ["tests*"]
</file>

</files>
