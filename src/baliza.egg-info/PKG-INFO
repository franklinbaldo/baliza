Metadata-Version: 2.4
Name: baliza
Version: 0.1.0
Summary: BALIZA: Backup Aberto de Licita√ß√µes Zelando pelo Acesso - Historical archive of Brazilian public procurement data
Requires-Python: >=3.11
Description-Content-Type: text/markdown
License-File: LICENSE
Requires-Dist: requests
Requires-Dist: tenacity
Requires-Dist: internetarchive
Requires-Dist: typer[all]
Requires-Dist: duckdb
Requires-Dist: httpx
Requires-Dist: attrs
Provides-Extra: analytics
Requires-Dist: dbt-duckdb; extra == "analytics"
Requires-Dist: dbt-core; extra == "analytics"
Provides-Extra: dev
Requires-Dist: pytest; extra == "dev"
Requires-Dist: pytest-mock; extra == "dev"
Dynamic: license-file

# BALIZA

**Acr√¥nimo oficial**
**B**ackup **A**berto de **LI**cita√ß√µes **Z**elando pelo **A**cesso

---

### Pitch de uma linha

> ‚ÄúBALIZA espelha diariamente o PNCP no Internet Archive para garantir hist√≥rico completo e an√°lises retroativas de compras p√∫blicas.‚Äù

### README.md ‚Äî estrutura m√≠nima (Atualizado)

````markdown
# BALIZA

**Backup Aberto de Licita√ß√µes Zelando pelo Acesso** ‚Äî um bot que baixa o delta di√°rio do PNCP e envia para o Internet Archive em JSONL compactado.

## Por qu√™
- O PNCP s√≥ mant√©m dados acess√≠veis via API e sem versionamento.
- Sem s√©ries hist√≥ricas n√£o h√° auditoria s√©ria nem detec√ß√£o de fraude por padr√£o.

## Como funciona
1. **Coleta Autom√°tica**: Por padr√£o, BALIZA processa TODO o hist√≥rico dispon√≠vel (2020-2025+), identificando automaticamente datas faltantes.
2. **Busca de Dados**: O CLI `baliza` chama os endpoints da API do PNCP (`/v1/contratacoes/publicacao`, `/v1/contratos/publicacao`, `/v1/pca`) com pagina√ß√£o inteligente.
3. **Processamento e Compacta√ß√£o**: Os dados s√£o organizados em Parquet e DuckDB para an√°lise local, al√©m de JSONL compactado com Zstandard para arquivamento.
4. **Upload para o Internet Archive**: Arquivos s√£o enviados para o Internet Archive com metadados estruturados e checksums SHA256.
5. **Federa√ß√£o de Dados**: Sistema de federa√ß√£o permite consultas unificadas entre dados locais e do Internet Archive usando DuckDB.

## üöÄ Como Usar

### üìä **Para An√°lise de Dados (Recomendado)**

**üéØ An√°lise Instant√¢nea no Google Colab:**

[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/franklinbaldo/baliza/blob/main/notebooks/analise_pncp_colab.ipynb)

- ‚úÖ **Um clique** e voc√™ est√° analisando milh√µes de contratos p√∫blicos
- ‚úÖ **Sem configura√ß√£o** - funciona 100% no navegador  
- ‚úÖ **Dados atualizados** diretamente do Internet Archive
- ‚úÖ **An√°lises pr√©-configuradas** com visualiza√ß√µes interativas
- ‚úÖ **Detec√ß√£o de fraudes** e padr√µes suspeitos automatizada

### üîß **Para Coleta de Dados**

#### Pr√©-requisitos
- Python 3.11+
- `uv` (gerenciador de pacotes Python r√°pido). Se n√£o tiver: `curl -LsSf https://astral.sh/uv/install.sh | sh`
- Credenciais do Internet Archive (`IA_KEY` e `IA_SECRET`).

#### üöÄ **Instala√ß√£o e Uso (Simples)**

1. **Clone e configure o projeto:**
   ```bash
   git clone https://github.com/franklinbaldo/baliza.git
   cd baliza
   
   # Instalar depend√™ncias
   uv sync
   
   # Instalar BALIZA como CLI tool
   uv pip install -e .
   ```

2. **Configure credenciais do Internet Archive:**
   ```bash
   export IA_KEY="SUA_CHAVE_IA"
   export IA_SECRET="SEU_SEGREDO_IA"
   ```

3. **Execute a coleta:**

   **üèõÔ∏è MODO AUTOM√ÅTICO (Recomendado) - Baixa TODO o hist√≥rico:**
   ```bash
   uv run baliza --auto
   ```
   
   **üìÖ MODO DATA ESPEC√çFICA:**
   ```bash
   uv run baliza --date 2024-07-10
   ```
   
   **‚ö° MODO √öLTIMOS N DIAS:**
   ```bash
   uv run baliza --auto --days-back 30
   ```

#### üìÅ **Estrutura de Dados**

BALIZA usa estrutura de diret√≥rios XDG-compliant:

**üè† Desenvolvimento (padr√£o):**
```
baliza/
‚îú‚îÄ‚îÄ data/           # Bancos de dados principais
‚îú‚îÄ‚îÄ .cache/         # Cache de downloads do IA
‚îî‚îÄ‚îÄ .config/        # Configura√ß√µes
```

**üåê Produ√ß√£o (BALIZA_PRODUCTION=1):**
```
~/.local/share/baliza/  # Dados do usu√°rio
~/.cache/baliza/        # Cache do usu√°rio  
~/.config/baliza/       # Config do usu√°rio
```

#### üîó **Federa√ß√£o com Internet Archive**
```bash
# Atualizar federa√ß√£o (inclu√≠do automaticamente no --auto)
uv run python src/baliza/ia_federation.py federate

# Executar an√°lises com DBT
cd dbt_baliza
dbt run --select coverage_temporal coverage_entidades
```

## Automa√ß√£o com GitHub Actions
- O projeto inclui um workflow em `.github/workflows/baliza_daily_run.yml`.
- Este workflow executa o script diariamente √†s 02:15 BRT (05:15 UTC) em modo autom√°tico completo.
- O workflow captura um sum√°rio da execu√ß√£o em formato JSON e o armazena como um artefato do GitHub Actions para refer√™ncia e depura√ß√£o.
- **Importante**: Para que o upload autom√°tico funcione, voc√™ **DEVE** configurar `IA_KEY` e `IA_SECRET` como "Secrets" nas configura√ß√µes do seu reposit√≥rio GitHub (Settings > Secrets and variables > Actions).

## Roadmap

### Fase 1: Implementa√ß√£o Central (Conclu√≠da)
* [x] Script de coleta para `/v1/contratacoes/publicacao` com CLI (`Typer`).
* [x] Compress√£o dos dados para `.jsonl.zst`.
* [x] Upload dos arquivos para o Internet Archive.
* [x] Agendamento da execu√ß√£o di√°ria via GitHub Actions.
* [x] Captura de sum√°rio estruturado da execu√ß√£o (JSON) como artefato no GitHub Actions.

### Fase 2: P√°gina de Estat√≠sticas e Compartilhamento de Torrents (Planejado)
* [ ] **Coleta de Estat√≠sticas**: Desenvolver script para agregar dados das execu√ß√µes di√°rias (e.g., itens coletados, status, links IA).
* [ ] **Manifesto de Torrents**: Gerar e manter uma lista atualizada dos links `.torrent` para os itens arquivados no Internet Archive.
* [ ] **P√°gina Web de Estat√≠sticas**:
    * [ ] Criar template HTML para a p√°gina de status.
    * [ ] Desenvolver script para gerar a p√°gina HTML est√°tica a partir dos dados de estat√≠sticas e torrents.
    * [ ] Configurar GitHub Pages para hospedar a p√°gina.
* [ ] **Atualiza√ß√£o do Workflow**: Incrementar o GitHub Actions para executar os scripts de coleta de estat√≠sticas, gera√ß√£o de manifesto de torrents e da p√°gina web, e fazer commit dos artefatos atualizados.

### Fase 3: Expans√£o de Endpoints (Implementado)
* [x] **Implementar coleta para novos endpoints**:
    * [x] `/v1/pca/usuario` - Consultar Itens de PCA por Ano do PCA, IdUsuario e C√≥digo de Classifica√ß√£o Superior.
    * [x] `/v1/pca/` - Consultar Itens de PCA por Ano do PCA e C√≥digo de Classifica√ß√£o Superior (endpoint geral).
    * [x] `/v1/contratacoes/proposta` - Consultar Contrata√ß√µes com Recebimento de Propostas Aberto.

### Funcionalidades Adicionais (Consideradas para o Futuro - P√≥s Fase 3)
* [ ] Implementar persist√™ncia robusta de checksums e estado de processamento (e.g., `state/processed.csv`) para evitar reprocessamento e duplicatas de forma mais granular.
* [ ] Avaliar a cria√ß√£o de um dump autom√°tico para ClickHouse a partir dos dados no Internet Archive.
* [ ] Desenvolver um painel anal√≠tico (Superset/Metabase) com KPIs (e.g., sobrepre√ßo) utilizando os dados coletados.
* [ ] Configurar alertas de anomalia (e.g., via Webhook) para falhas na coleta ou problemas nos dados.
* [ ] Implementar configura√ß√£o din√¢mica de par√¢metros para os novos endpoints PCA.

## Pr√≥ximos Passos (Comunidade e Testes)
1. **Teste Manual Extensivo**: Execute o script com `--date` para diferentes dias passados para garantir a robustez do hash, da coleta e do upload.
2. **Feedback e Contribui√ß√µes**: Abra issues para bugs, sugest√µes ou melhorias. Contribui√ß√µes via Pull Requests s√£o bem-vindas!
3. **Anunciar e Engajar**: Ap√≥s estabiliza√ß√£o, considere anunciar no f√≥rum Dados Abertos BR e convidar a comunidade para auditar os dados e o processo.

```

### Projeto em 5 min ‚Äî vis√£o geral

1. **Use os pr√≥prios endpoints de consulta do PNCP** (`/v1/contratacoes/publicacao`, `/v1/contratos/publicacao`, `/v1/pca`, etc.), que j√° aceitam filtros por intervalo de datas, pagina√ß√£o (`pagina`, `tamanhoPagina ‚â§ 500`) e devolvem JSON padronizado.&#x20;
2. **Rode um *crawler* di√°rio** (cron ou GitHub Actions) que baixa s√≥ o delta do dia anterior. N√£o invente ‚Äúvarredura completa‚Äù ‚Äî √© lento, caro e sujeito a time-out.
3. **Empacote cada lote em `JSONL` comprimido** (`.jsonl.zst` √© √≥timo) e gere um manifesto SHA-256 para deduplicar depois.
4. **Suba o arquivo para o Internet Archive** usando a API S3-like (`ias3`) com nome est√°vel, ex.:
   `pncp-contratacoes-2025-07-03.jsonl.zst`
   Metadados m√≠nimos: `title`, `creator=‚ÄúPNCP Mirror Bot‚Äù`, `subject=‚Äúpublic procurement Brazil‚Äù`. ([archive.org][1], [archive.org][2])
5. **Repita.** Em poucos meses voc√™ ter√° um *data lake* p√∫blico, versionado e historicamente completo para qualquer an√°lise cont√°bil, *benchmarking* de pre√ßos, *red-flag analytics*, etc.

---

### Arquitetura Adotada

| Camada                 | Tech-stack                                     | Por qu√™                                                      |
| ---------------------- | ---------------------------------------------- | ------------------------------------------------------------ |
| **Coleta**             | `python` + `requests` + `tenacity` (retry)     | Leve, controlado, f√°cil de debugar                           |
| **Gerenciamento de Dep.** | `uv` (Astral)                                  | R√°pido, moderno, compat√≠vel com `pyproject.toml`             |
| **Agendamento**        | GitHub Actions (cron)                          | Integrado ao reposit√≥rio, gratuito para projetos open source   |
| **Processamento**      | Python `json` (para JSONL)                     | Simples e direto para convers√£o em JSONL                     |
| **Compress√£o**         | `zstandard` (via lib Python)                   | Excelente taxa de compress√£o e velocidade                    |
| **Upload**             | `internetarchive` CLI/lib Python               | Biblioteca oficial para interagir com o Internet Archive     |
| **Cat√°logo (Planejado)** | Manifest (`manifest.yml`) + checksums em CSV   | Garante integridade, evita duplicatas (ainda n√£o implementado) |

---

### Fluxo Incremental (Detalhado)

1. **C√°lculo da Data**: O script (ou o workflow do GitHub Actions) determina a data "ontem" (fuso hor√°rio de Bras√≠lia, UTC-3).
2. **Itera√ß√£o por Endpoints**: Para cada endpoint configurado (inicialmente, apenas `contratacoes`):
   ```text
   GET https://pncp.gov.br/api/consulta/v1/contratacoes/publicacao?dataInicial=YYYY-MM-DD&dataFinal=YYYY-MM-DD&pagina=1&tamanhoPagina=500
   ```
3. **Pagina√ß√£o**: O script itera sobre as p√°ginas de resultados at√© que `paginaAtual` seja maior ou igual a `totalPaginas` retornado pela API. Cada p√°gina pode conter at√© 500 registros.
4. **Armazenamento Tempor√°rio**: Cada registro JSON √© anexado a um arquivo `.jsonl` local.
5. **Compress√£o**: Ap√≥s coletar todos os dados do dia para um endpoint, o arquivo `.jsonl` √© comprimido usando Zstandard, resultando em um arquivo `.jsonl.zst`.
6. **C√°lculo de Checksum**: Um hash SHA256 √© calculado para o arquivo `.jsonl.zst`.
7. **Upload para Internet Archive**: O arquivo comprimido √© enviado para o Internet Archive, e o checksum SHA256 √© inclu√≠do nos metadados.
8. **Limpeza (Local)**: O arquivo `.jsonl` original √© removido ap√≥s a compress√£o e tentativa de upload. O arquivo `.jsonl.zst` permanece localmente no diret√≥rio `baliza_data/`.
9. **Registro de Estado (Planejado)**: Futuramente, o hash do arquivo e o status do upload ser√£o gravados para evitar reprocessamento e permitir o rastreamento.

---

### Pontos cr√≠ticos (opini√£o sem rodeios)

* **Rate-limit e disponibilidade**: Se o *crawler* falhar, n√£o re-tente infinito ‚Äî o PNCP derruba conex√µes longas.
* **Token de acesso**: Hoje a consulta √© p√∫blica, mas o SERPRO pode exigir API-key amanh√£; prepare var env.
* **Qualidade dos dados**: Campos financeiros v√™m como texto, v√≠rgula decimal e zeros m√°gicos (0 = sigilo). N√£o confie neles sem *post-processing*.&#x20;
* **Internet Archive n√£o √© banco OLTP**: ele armazena blob; para consultas SQL use BigQuery, ClickHouse ou DuckDB apontando para seus `JSONL`.
* **Legalidade**: dados j√° s√£o p√∫blicos; o espelho √© mera redund√¢ncia. Mas inclua aviso de responsabilidade (‚Äú*dados brutos, sem garantias*‚Äù).

---

### Pr√≥ximos passos (P√≥s-Implementa√ß√£o Inicial)

1. **Reposit√≥rio e Licen√ßa**: O reposit√≥rio no GitHub est√° configurado com Licen√ßa MIT e este README atualizado. (Feito!)
2. **Automa√ß√£o**: GitHub Actions est√° habilitado com `cron: '15 5 * * *'` (02:15 BRT / 05:15 UTC) para execu√ß√£o di√°ria. (Feito!)
3. **Dashboard (Futuro)**: Criar um *dashboard* (ex: Superset, Metabase) que consuma os dados dos arquivos `.jsonl.zst` diretamente do Internet Archive (possivelmente via HTTPFS ou similar).
4. **Engajamento Comunit√°rio (Futuro)**: Quando o sistema estiver est√°vel e com um volume razo√°vel de dados arquivados, anunciar no f√≥rum **Dados Abertos BR** para atrair colaboradores, auditores e usu√°rios.

Com a base implementada, o projeto Baliza est√° pronto para come√ßar a arquivar os dados e evoluir com as funcionalidades planejadas no Roadmap.

[1]: https://archive.org/developers/ias3.html?utm_source=chatgpt.com "ias3 Internet archive S3-like API"
[2]: https://archive.org/developers/index-apis.html?utm_source=chatgpt.com "Tools and APIs ‚Äî Internet Archive Developer Portal"
